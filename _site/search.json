[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "HPC best practices",
    "section": "",
    "text": "High Performance Computing (HPC) plays a crucial role for researchers by offering the computational speed and power needed to manage large and complex data sets, perform simulations, and address intricate problems that would be impractical or too time-consuming with standard computing methods.\nKnowing which HPC resources are accessible and how to use them efficiently is essential for researchers. Making the most of these resources can significantly expedite research and drive innovation. By becoming proficient with the available tools and technologies, researchers can address complex challenges, analyze extensive data sets, and execute advanced simulations with increased speed and accuracy. This module provides essential knowledge on HPC resources and best practices for their utilization.\nThis module offers content for three distinct courses:\n\nHPC Launch: Foundations on HPC and essential knowledge on national HPC resources\nHPC Pipes: Best practices for using workflow management systems and computational environments with HPC\nHPC ML (Machine Learning): Insights into applying HPC for machine learning tasks, including model training, data analysis, and optimization techniques.\n\nBy the end of all the modules, you will gain practical skills in promoting reproducibility through comprehensive training in HPC resource management, workflow pipelines, and computing environments.\n\n\n\n\n\n\nCourse Goals\n\n\n\nBy the end of this workshop, you should be able to apply the following concepts in the context of Next Generation Sequencing data:\n\nUnderstand the Importance of Research Data Management (RDM)\nMake your data analysis and workflows reproducible and FAIR\nMake FAIR environment using conda or Docker"
  },
  {
    "objectID": "index.html#hpc-pipes",
    "href": "index.html#hpc-pipes",
    "title": "HPC best practices",
    "section": "HPC pipes",
    "text": "HPC pipes\nThe course ‚ÄúHPC pipes‚Äù is designed to provide participants with foundational knowledge and practical skills in writing reproducible pipelines. As part of effective data management, it is crucial that researchers create reproducible analyses that enable others to validate and build upon their work. We will explore essential elements of reproducibility and efficiency in computational research, highlighting techniques and tools for creating robust and transparent coding and workflows. By prioritizing reproducibility and replicability, researchers can enhance the credibility and impact of their findings while fostering collaboration and knowledge dissemination within the scientific community. This approach guarantees efficient research management. Explore our content on practical RDM for more details.\n\n\n\n\n\n\nHPC best practices\n\n\n\nWe offer in-person workshops, keep an eye on the upcoming events on the Sandbox website.\n\n\n\nAcknowledgements"
  },
  {
    "objectID": "develop/hpc_intro.html",
    "href": "develop/hpc_intro.html",
    "title": "HPC launch",
    "section": "",
    "text": "Course Overview\n\n\n\n\n‚è∞ Total Time Estimation: X hours\n\nüìÅ Supporting Materials:\n\nüë®‚Äçüíª Target Audience: Ph.D., MSc, anyone interested in HPC systems.\nüë©‚Äçüéì Level: Advanced.\nüîí License: Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license.\n\nüí∞ Funding: This project was funded by the Novo Nordisk Fonden (NNF20OC0063268).",
    "crumbs": [
      "HPC launch"
    ]
  },
  {
    "objectID": "develop/hpc_intro.html#title",
    "href": "develop/hpc_intro.html#title",
    "title": "HPC launch",
    "section": "Title",
    "text": "Title\nIntroduction to the material and HPCs‚Ä¶",
    "crumbs": [
      "HPC launch"
    ]
  },
  {
    "objectID": "develop/hpc_intro.html#sources",
    "href": "develop/hpc_intro.html#sources",
    "title": "HPC launch",
    "section": "Sources",
    "text": "Sources\nUseful links\n\nAcknowledgements",
    "crumbs": [
      "HPC launch"
    ]
  },
  {
    "objectID": "develop/smk.html",
    "href": "develop/smk.html",
    "title": "Snakemake",
    "section": "",
    "text": "Snakemake is a text-based tool using python-based language plus domain specific syntax. The workflow is decompose into rules that are define to obtain output files from input files. It infers dependencies and the execution order. 1. Semantics: define rules 2. Generalise the rule: creating wildcards You can refer by index or by name\n\nDependencies are determined top-down\n\nFor a given target, a rule that can be applied to create it, is determined (a job) For the input files of the rule, go on recursively, If no target is specified, snakemake, tries to apply the first rule\n\nRule all: target rule that collects results",
    "crumbs": [
      "HPC pipes",
      "Pipeline languages",
      "Snakemake"
    ]
  },
  {
    "objectID": "develop/smk.html#sources",
    "href": "develop/smk.html#sources",
    "title": "Snakemake",
    "section": "Sources",
    "text": "Sources\n\nSnakemake tutorial\nSnakemake turorial slides by Johannes Koster\nK√∂ster, Johannes and Rahmann, Sven. ‚ÄúSnakemake - A scalable bioinformatics workflow engine‚Äù. Bioinformatics 2012.\nK√∂ster, Johannes. ‚ÄúParallelization, Scalability, and Reproducibility in Next-Generation Sequencing Analysis‚Äù, PhD thesis, TU Dortmund 2014.\nfaircookbook worflows",
    "crumbs": [
      "HPC pipes",
      "Pipeline languages",
      "Snakemake"
    ]
  },
  {
    "objectID": "develop/data_management.html",
    "href": "develop/data_management.html",
    "title": "HPC Lab",
    "section": "",
    "text": "Overview rsync and scp\nBasic commands\nUse cases\n\nWhen transferring files between servers, it‚Äôs important to ensure that the files are consistent. Differences in file size can occur due to variations in filesystems, but large differences might indicate an issue. Manually checking file sizes in a terminal (ls -lh or du -sh) to determine if a transfer was successful might not be ideal, as it doesn‚Äôt guarantee file integrity. Using checksums provides a more reliable verification method.\nYou can use md5sum to verify that the file contents are identical on both servers. Run the following command on each server:\nmd5sum /path/to/file\nUsing rsync for Efficient File Transfers rsync is a powerful alternative to scp for transferring files. It only sends data if the file has changed, making it more efficient.\n# Transferring files between local machine-server\nrsync -avz local/path/to/file user@server:/remote/path/to/file\n# Transferring files between servers\nrsync -azv server1:/path/to/my_folder server2:/path/to/destination_folder\n\n\n\n\n\n\nNote\n\n\n\nTo transfer files directly between two servers from your local workstation, ensure your SSH setup (configuration, keys, etc.) allows access to both servers. Check [LINK] if you need help setting up your keys (generating, configuring and managing).\n\n\n\n\nrsync is an efficient protocol to compare and copy files between directories or server. It can resume interrupted transfers and compress files on the fly.\n\nChecksum Verification: rsync checks the hashsums of files and only transfers data if the hashes differ. This ensures that only the changed parts of the files are sent (so you can rsync a whole folder, and only the changes files will be send).\nTimestamp Preservation: Using the -a flag with rsync preserves the modified timestamps of files, which is particularly useful for tools like Snakemake.\n\n\n\n\n\n-a: Archive mode, preserves file attributes like timestamps and permissions (important if you are using snakemake).\n-v: Verbose mode, provides detailed information during transfer.\n-z: Compresses data during transfer, reducing the amount of data sent over the network.\n-c: Enables checksum checking, ensuring that files are identical by comparing their contents rather than just their size and modification time.\n\n\n\n\nFor users who prefer a graphical interface, tools like Cyberduck and FileZilla can also be used for transferring files between servers.\n\n\n\n\n\n\n\n\nregular expression\ncookiecutter",
    "crumbs": [
      "HPC launch",
      "Data management on HPC systems"
    ]
  },
  {
    "objectID": "develop/data_management.html#data-management",
    "href": "develop/data_management.html#data-management",
    "title": "HPC Lab",
    "section": "",
    "text": "Overview rsync and scp\nBasic commands\nUse cases\n\nWhen transferring files between servers, it‚Äôs important to ensure that the files are consistent. Differences in file size can occur due to variations in filesystems, but large differences might indicate an issue. Manually checking file sizes in a terminal (ls -lh or du -sh) to determine if a transfer was successful might not be ideal, as it doesn‚Äôt guarantee file integrity. Using checksums provides a more reliable verification method.\nYou can use md5sum to verify that the file contents are identical on both servers. Run the following command on each server:\nmd5sum /path/to/file\nUsing rsync for Efficient File Transfers rsync is a powerful alternative to scp for transferring files. It only sends data if the file has changed, making it more efficient.\n# Transferring files between local machine-server\nrsync -avz local/path/to/file user@server:/remote/path/to/file\n# Transferring files between servers\nrsync -azv server1:/path/to/my_folder server2:/path/to/destination_folder\n\n\n\n\n\n\nNote\n\n\n\nTo transfer files directly between two servers from your local workstation, ensure your SSH setup (configuration, keys, etc.) allows access to both servers. Check [LINK] if you need help setting up your keys.\n\n\nAdvantages of rsync over scp: ‚Ä¢ Checksum Verification: rsync checks the hashsums of files and only transfers data if the hashes differ. This ensures that only the changed parts of the files are sent (so you can rsync a whole folder, and only the changes files will be send) ‚Ä¢ Timestamp Preservation: Using the -a flag with rsync preserves the modified timestamps of files, which is particularly useful for tools like Snakemake.\nImportant rsync Options ‚Ä¢ -a: Archive mode, preserves file attributes like timestamps and permissions (important if you are using snakemake). ‚Ä¢ -v: Verbose mode, provides detailed information during transfer. ‚Ä¢ -z: Compresses data during transfer, reducing the amount of data sent over the network. ‚Ä¢ -c: Enables checksum checking, ensuring that files are identical by comparing their contents rather than just their size and modification time.\nAdditional Tools For users who prefer a graphical interface, tools like Cyberduck and FileZilla can also be used for transferring files between servers.\n\n\n\n\n\n\n\nregular expression\ncookiecutter",
    "crumbs": [
      "HPC launch",
      "Data management"
    ]
  },
  {
    "objectID": "develop/fair_envs.html",
    "href": "develop/fair_envs.html",
    "title": "HPC Lab",
    "section": "",
    "text": "Recording and sharing the computational environment is essential for reproducibility and transparency. There are several methods to achieve this, but we are going to focus on two of them, conda (an environment manager) and Docker (a container). Environment managers are user-friendly, easy to share across different systems, and offer lightweight, efficient, and fast start-up times. However, Docker containers provide complete environment isolation (including the operating system), which ensures consistent behavior across various systems.\nEach computer or HPC (High-Performance Computing) platform has a unique computational environment that includes its operating system, installed software, versions of software packages, and other features. If a research project is moved to a different computer or platform, the analysis might not run or produce consistent results if it depends on any of these factors.\nFor research to be reproducible, the original computational environment must be recorded so others can replicate it. There are several methods to achieve this:\n\nContainerization platforms (e.g., Docker, Singularity): allow the researcher to package their software and dependencies into a standardized container image.\nVirtual Machines (e.g., VirtualBox): can share an entire virtualized computing environment (OS, software and dependencies)\nEnvironment managers: provide an isolated environment with specific packages and dependencies that can be installed without affecting the system-wide configuration. These environments are particularly useful for managing conflicting dependencies and ensuring reproducibility. Configuration files can automate the setup of the computational environment:\n\nconda: allows users to export environment specifications (software and dependencies) to YAML files enabling easy recreation of the environment on another system\nPython virtualenv: is a tool for creating isolated environments to manage dependencies specific to a project\nrequirements.txt: may contain commands for installing packages (such as pip for Python packages or apt-get for system-level dependencies), configuring system settings, and setting environment variables. Package managers can be used to install, upgrade and manage packages.\nR‚Äôs renv: The ‚Äòrenv‚Äô package creates isolated environments in R.\n\nEnvironment descriptors\n\nsessionInfo() or devtools::session_info(): In R, these functions provide detailed information about the current session\nsessionInfo(), similarly, in Python. Libraries like NumPy and Pandas have show_versions() methods to display package versions.\n\n\nWhile environment managers are very easy to use and share across different systems, and are lightweight and efficient, offering fast start-up times, Docker containers provide a full env isolation (including the operating system) which ensures consistent behavior across different systems.\n\n\n\n\n\nDocker helps developers build, share, run, and verify applications anywhere ‚Äî without tedious environment configuration or management.",
    "crumbs": [
      "HPC pipes",
      "Computational environments",
      "FAIR environments"
    ]
  },
  {
    "objectID": "develop/fair_envs.html#fair-environments",
    "href": "develop/fair_envs.html#fair-environments",
    "title": "HPC Lab",
    "section": "",
    "text": "Recording and sharing the computational environment is essential for reproducibility and transparency. There are several methods to achieve this, but we are going to focus on two of them, conda (an environment manager) and Docker (a container). Environment managers are user-friendly, easy to share across different systems, and offer lightweight, efficient, and fast start-up times. However, Docker containers provide complete environment isolation (including the operating system), which ensures consistent behavior across various systems.\nEach computer or HPC (High-Performance Computing) platform has a unique computational environment that includes its operating system, installed software, versions of software packages, and other features. If a research project is moved to a different computer or platform, the analysis might not run or produce consistent results if it depends on any of these factors.\nFor research to be reproducible, the original computational environment must be recorded so others can replicate it. There are several methods to achieve this:\n\nContainerization platforms (e.g., Docker, Singularity): allow the researcher to package their software and dependencies into a standardized container image.\nVirtual Machines (e.g., VirtualBox): can share an entire virtualized computing environment (OS, software and dependencies)\nEnvironment managers: provide an isolated environment with specific packages and dependencies that can be installed without affecting the system-wide configuration. These environments are particularly useful for managing conflicting dependencies and ensuring reproducibility. Configuration files can automate the setup of the computational environment:\n\nconda: allows users to export environment specifications (software and dependencies) to YAML files enabling easy recreation of the environment on another system\nPython virtualenv: is a tool for creating isolated environments to manage dependencies specific to a project\nrequirements.txt: may contain commands for installing packages (such as pip for Python packages or apt-get for system-level dependencies), configuring system settings, and setting environment variables. Package managers can be used to install, upgrade and manage packages.\nR‚Äôs renv: The ‚Äòrenv‚Äô package creates isolated environments in R.\n\nEnvironment descriptors\n\nsessionInfo() or devtools::session_info(): In R, these functions provide detailed information about the current session\nsessionInfo(), similarly, in Python. Libraries like NumPy and Pandas have show_versions() methods to display package versions.\n\n\nWhile environment managers are very easy to use and share across different systems, and are lightweight and efficient, offering fast start-up times, Docker containers provide a full env isolation (including the operating system) which ensures consistent behavior across different systems.\n\n\n\n\n\nDocker helps developers build, share, run, and verify applications anywhere ‚Äî without tedious environment configuration or management.",
    "crumbs": [
      "HPC pipes",
      "Computational environments",
      "FAIR environments"
    ]
  },
  {
    "objectID": "develop/fair_envs.html#sources",
    "href": "develop/fair_envs.html#sources",
    "title": "HPC Lab",
    "section": "Sources",
    "text": "Sources\n\nhttps://bioconda.github.io\nfaircookbook worflows\nDocker\nDocker get-started\nThe turing way - reproducible research",
    "crumbs": [
      "HPC pipes",
      "Computational environments",
      "FAIR environments"
    ]
  },
  {
    "objectID": "develop/fair_workflow.html",
    "href": "develop/fair_workflow.html",
    "title": "FAIR computational pipelines",
    "section": "",
    "text": "Course Overview\n\n\n\n\n‚è∞ Total Time Estimation: X hours\n\nüìÅ Supporting Materials:\n\nüë®‚Äçüíª Target Audience: Ph.D., MSc, anyone interested in workflow management systems for High-Throughput data or other related fields within bioinformatics.\nüë©‚Äçüéì Level: Advanced.\nüîí License: Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license.\n\nüí∞ Funding: This project was funded by the Novo Nordisk Fonden (NNF20OC0063268).\nData analysis typically involves the use of different tools, algorithms, and scripts. It often requires multiple steps to transform, filter, aggregate, and visualize data. The process can be time-consuming because each tool may demand specific inputs and parameter settings. As analyses become more complex, the importance of reproducible and scalable automated workflow management increases. Workflow management encompasses tasks such as parallelization, resumption, logging, and data provenance.\nIf you develop your own software make sure you follow FAIR principles. We highly endorse following these FAIR recommendations and to register your computational workflow here.\nUsing workflow managers, you ensure:\nPopular workflow management systems such as Snakemake, Nextflow, and Galaxy can be scaled effortlessly across server, cluster, and cloud environments without altering the workflow definition. They also allow for specifying the necessary software, ensuring the workflows can be deployed in any setting.\nDuring this lesson, you will learn about:",
    "crumbs": [
      "HPC pipes",
      "Pipeline languages",
      "FAIR computational pipelines"
    ]
  },
  {
    "objectID": "develop/fair_workflow.html#good-practices",
    "href": "develop/fair_workflow.html#good-practices",
    "title": "FAIR computational pipelines",
    "section": "Good practices",
    "text": "Good practices\nRemember to create portable code and use relative paths to ensure transferability between users.\nUse git repositories to save your projects and pipelines. Register and publish your scientific computational workflow on WorkflowHub.",
    "crumbs": [
      "HPC pipes",
      "Pipeline languages",
      "FAIR computational pipelines"
    ]
  },
  {
    "objectID": "develop/fair_workflow.html#sources",
    "href": "develop/fair_workflow.html#sources",
    "title": "FAIR computational pipelines",
    "section": "Sources",
    "text": "Sources\n\nSnakemake tutorial\nSnakemake turorial slides by Johannes Koster\nhttps://bioconda.github.io\nK√∂ster, Johannes and Rahmann, Sven. ‚ÄúSnakemake - A scalable bioinformatics workflow engine‚Äù. Bioinformatics 2012.\nK√∂ster, Johannes. ‚ÄúParallelization, Scalability, and Reproducibility in Next-Generation Sequencing Analysis‚Äù, PhD thesis, TU Dortmund 2014.\nfaircookbook worflows\nThe turing way - reproducible research",
    "crumbs": [
      "HPC pipes",
      "Pipeline languages",
      "FAIR computational pipelines"
    ]
  },
  {
    "objectID": "develop/nextflow.html",
    "href": "develop/nextflow.html",
    "title": "Nextflow",
    "section": "",
    "text": "Nextflow is a workfow management system that offers scalable and portable NGS data analysis pipelines, facilitating data processing across diverse computing environments. It streamlines and automates various data analysis steps, enabling parallel processing and seamless integration with existing tools. √ß",
    "crumbs": [
      "HPC pipes",
      "Pipeline languages",
      "Nextflow"
    ]
  },
  {
    "objectID": "develop/nextflow.html#nf-core",
    "href": "develop/nextflow.html#nf-core",
    "title": "Nextflow",
    "section": "nf-core",
    "text": "nf-core\nTemplates",
    "crumbs": [
      "HPC pipes",
      "Pipeline languages",
      "Nextflow"
    ]
  },
  {
    "objectID": "develop/nextflow.html#sources",
    "href": "develop/nextflow.html#sources",
    "title": "Nextflow",
    "section": "Sources",
    "text": "Sources\n\nfaircookbook worflows",
    "crumbs": [
      "HPC pipes",
      "Pipeline languages",
      "Nextflow"
    ]
  },
  {
    "objectID": "develop/data_management.html#data-management-on-hpc-systems",
    "href": "develop/data_management.html#data-management-on-hpc-systems",
    "title": "HPC Lab",
    "section": "",
    "text": "Overview rsync and scp\nBasic commands\nUse cases\n\nWhen transferring files between servers, it‚Äôs important to ensure that the files are consistent. Differences in file size can occur due to variations in filesystems, but large differences might indicate an issue. Manually checking file sizes in a terminal (ls -lh or du -sh) to determine if a transfer was successful might not be ideal, as it doesn‚Äôt guarantee file integrity. Using checksums provides a more reliable verification method.\nYou can use md5sum to verify that the file contents are identical on both servers. Run the following command on each server:\nmd5sum /path/to/file\nUsing rsync for Efficient File Transfers rsync is a powerful alternative to scp for transferring files. It only sends data if the file has changed, making it more efficient.\n# Transferring files between local machine-server\nrsync -avz local/path/to/file user@server:/remote/path/to/file\n# Transferring files between servers\nrsync -azv server1:/path/to/my_folder server2:/path/to/destination_folder\n\n\n\n\n\n\nNote\n\n\n\nTo transfer files directly between two servers from your local workstation, ensure your SSH setup (configuration, keys, etc.) allows access to both servers. Check [LINK] if you need help setting up your keys (generating, configuring and managing).\n\n\n\n\nrsync is an efficient protocol to compare and copy files between directories or server. It can resume interrupted transfers and compress files on the fly.\n\nChecksum Verification: rsync checks the hashsums of files and only transfers data if the hashes differ. This ensures that only the changed parts of the files are sent (so you can rsync a whole folder, and only the changes files will be send).\nTimestamp Preservation: Using the -a flag with rsync preserves the modified timestamps of files, which is particularly useful for tools like Snakemake.\n\n\n\n\n\n-a: Archive mode, preserves file attributes like timestamps and permissions (important if you are using snakemake).\n-v: Verbose mode, provides detailed information during transfer.\n-z: Compresses data during transfer, reducing the amount of data sent over the network.\n-c: Enables checksum checking, ensuring that files are identical by comparing their contents rather than just their size and modification time.\n\n\n\n\nFor users who prefer a graphical interface, tools like Cyberduck and FileZilla can also be used for transferring files between servers.\n\n\n\n\n\n\n\n\nregular expression\ncookiecutter",
    "crumbs": [
      "HPC launch",
      "Data management on HPC systems"
    ]
  },
  {
    "objectID": "index.html#status-content-in-progress",
    "href": "index.html#status-content-in-progress",
    "title": "HPC best practices",
    "section": "",
    "text": "High Performance Computing (HPC) plays a crucial role for researchers by offering the computational speed and power needed to manage large and complex data sets, perform simulations, and address intricate problems that would be impractical or too time-consuming with standard computing methods.\nKnowing which HPC resources are accessible and how to use them efficiently is essential for researchers. Making the most of these resources can significantly expedite research and drive innovation. By becoming proficient with the available tools and technologies, researchers can address complex challenges, analyze extensive data sets, and execute advanced simulations with increased speed and accuracy. This module provides essential knowledge on HPC resources and best practices for their utilization.\nThis module offers content for three distinct courses:\n\nHPC Launch: Foundations on HPC and essential knowledge on national HPC resources\nHPC Pipes: Best practices for using workflow management systems and computational environments with HPC\nHPC ML (Machine Learning): Insights into applying HPC for machine learning tasks, including model training, data analysis, and optimization techniques.\n\nBy the end of all the modules, you will gain practical skills in promoting reproducibility through comprehensive training in HPC resource management, workflow pipelines, and computing environments.\n\n\n\n\n\n\nCourse Goals\n\n\n\nBy the end of this workshop, you should be able to apply the following concepts in the context of Next Generation Sequencing data:\n\nUnderstand the Importance of Research Data Management (RDM)\nMake your data analysis and workflows reproducible and FAIR\nMake FAIR environment using conda or Docker"
  },
  {
    "objectID": "develop/ssh_keys.html",
    "href": "develop/ssh_keys.html",
    "title": "Setting up SSH keys",
    "section": "",
    "text": "Setting up SSH keys\nUsing SSH keys for authentication is recommended for accessing servers and version control platforms such as GitLab, GitHub, or BitBucket. SSH keys provide a more convenient and secure method than passwords, allowing you to avoid typing a password every time you log in.\nThis tutorial guides you through configuring SSH keys, using GitHub as an example.\n\nWhat is an SSH key?\nAn SSH (Secure Shell) is a network protocol that allows you to execute commands on a remote server securely. SSH keys are cryptographic keys used for authenticating a user or device within the SSH protocol. They come in pairs: the ‚Äúprivate‚Äù key and the ‚Äúpublic‚Äù key.\n\nPublic Key: This key can be freely shared with anyone and is added to the ~/.ssh/authorized_keys file on the remote server you wish to access.\nPrivate Key: This key should remain on your local machine and be carefully protested, similar to a password. It should never be shared.\n\nThe holder of the private key can authenticate with any remote system that has the corresponding public key. For example, you can use a single public/private key pair to authenticate with the cluster at your research center and your GitHub account.\n\n\n\n\n\n\nHow SSH keys work\n\n\n\nThink of the public key as a complex number x, and the private key as its inverse 1/x. Encrypting a message m with your public key is like multiplying your message by x, making mx completely unreadable. Using the private key, you can decrypt m by multiplying mx by 1/x. This process is secure because you cannot derive 1/x from x. Therefore, you can safely distribute the public key to the machines you want to access while keeping the private key on your machine.\n\n\n\n\nHow do I create an SSH key?\n\nStep 1: Creating a public/private key pair\nOn your (Mac or Linux) workstation, create the public/private key pair. There are several types of keys and by default you will get an RSA pair. When prompted for a passphrase, just hit enter so that you won‚Äôt need a password when logging in.\nWhen prompted, press Enter to store the keys in .ssh/id_rsa (we recommend not to modify this). Press Enter twice again to skip setting a passphrase, which otherwise would require you to enter it each time you use git push or git pull. This process generates two files: the public key (~/.ssh/id_rsa.pub) and the private key (~/.ssh/id_rsa).\nssh-keygen\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/Users/abc123/.ssh/id_rsa): \nCreated directory '/Users/abc123/.ssh'.\nEnter passphrase (empty for no passphrase): \nEnter same passphrase again: \nYour identification has been saved in /Users/abc123/.ssh/id_rsa\nYour public key has been saved in /Users/abc123/.ssh/id_rsa.pub\nThe key fingerprint is:\nSHA256:w8JIQXafdq6scMiNZefNd8lVz9UmIzm0eiO+pvGOM6A abc123@c9b1d769db6e\nThe key's randomart image is:\n+---[RSA 3072]----+\n|   .+ .     .    |\n|   . o . . . o  .|\n|    .   + . = o =|\n|   . o o o . o =+|\n|    . = S + o   +|\n|   . *.= * o o o |\n|    =.o.= + . +  |\n|    Eo .o+.o .   |\n|      . o*+      |\n+----[SHA256]-----+\ncat `~/.ssh/id_rsa.pub\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCs3nqhRr7FyB8UKxqQ5mA3Gk60oty5lGdzcpVxjL08QmjLz15X6Odo2EotmJQuzCyYbzPw4Tq404N3NOmVqYywrn03i3kmCnZd5MYq8yi72Zsk9qFfgEt7pxpf5WJ+AWSDXwUkUQK2FFby2RWbi4gwTmRHfkB999j8F3F/d5EpB0llQfxmZWIIrX9ampgxfI8+yTIyeG8gpx5028h1oR5qukEFJk6nAoj5on+/vBqHdc0AVPd6jNoNCDZ6ur4xwUPam8iQ6A4NfbnCVUPa95Wqt+QWEF6Mn4UB1WIvly+blDzg5rVXvd9B685bZjQWfUsxSiRPkhtV6X2yImklt+KjV0ufB/Yl1O+x6S8r2+I41WSxwJNr5h5L588l3GH4ehT79uCtrxiu4zas+4s9OW02Ox3auCfLi1/jp/CJ5IsuLwM/jPBXrgzw+HREKM2gtm9d12gU2b9o9bmASZVhEKqeTb0aRPjcTYYdnjOWUl+pqkdVPyB7mJs8NAQemN/shvM= abc123@c9b1d769db6e\nThe public key can be safely displayed cat and shared with any achine where it will function as a personal lock that can be only opened with your private key.\n\n\n\n\n\n\nTip\n\n\n\nTip 1: If you provided a passphrase when generating your keys, you would need to enter this passphrase each time you use your private key. The passphrase adds an extra layer of security in case your private key is compromised. For simplicity, we not always recommend using a passphrase unless you are working in environments where people not part of the project might have access.\nTip 2: DSA keys should be avoided. An RSA key with at least 2048 bits should provide sufficient security. Finally, ecdsa and ed25519 keys are also fine (don‚Äôt worry about the number of bits for these keys).\n# Recommended\nssh-keygen -t rsa -b 4096\n# Alternative\nssh-keygen -t ed25519\n\n\nUsing an existing public/private key pair\nIf you already have an SSH key pair, you can use it without generating a new one. To check the details of your existing key, use the ssh-keygen command to display the key‚Äôs fingerprint.\nssh-keygen -l -f ~/.ssh/id_rsa\n2048 SHA256:7l0HauYJVRaQhuzmti8XEZImnRbzipu3NKGnE6tDFRk grg@t430s (RSA)\nAlternatively, check the content of the ssh directory: - Mac/Unix: ls -la ~/.ssh/ - Windows: ls ~/.ssh/\n\n\nStep 2: Adding your SSH key to the ssh-agent\nFollow the steps below if you are using Mac or Linux.\n\n\n\n\n\n\nWarning\n\n\n\n\nWindows users: follow these instructions instead, the process is very similar.\nIf you are getting errors when running the commands below, please read carefully these guidelines as GitHUb provides instructions on what to do with common issues users run into for this specific step 2.\nGeneral GitHUbtroubleshooting\n\n\n\n# ONLY if you need to start the ssh agent. First, verify it is not already running: \n# pgrep -u $USER ssh-agent\n# If that command returns a process ID, your SSH agent is running! If not, run the command below: \n# eval \"$(ssh-agent -s)\"\n\n# Manually modify the `~/.ssh/config` file to automatically load keys into ssh-agent, like in the example below. For GitHub, the User is **git**, do not write your own. \n\nHost github.com\n  AddKeysToAgent yes\n  IdentityFile ~/.ssh/id_rsa\n\n# Note: if the file does not `touch ~/.ssh/config`to create the file. \n\n# Add the SSH private to the ssh-agent\nssh-add ~/.ssh/id_rsa\n\n# Optional, verify the keys are loaded\nssh-add -l\n\n\n\n\n\n\nOther tips\n\n\n\n\n~/.ssh/config: add several hosts (servers, GitLab, GitHUb, etc.).\n\nEdit the SSH config file Create and use a ~/.ssh/config file on your workstation. E.g., using a password-less ssh key for authentication and the following ~/.ssh/config, user abc123 can login with the ssh xyz command.\nHost xyz\n    Hostname ssh-myserver.dk\n    User abc123\n    ServerAliveInterval 60\n    ForwardX11 yes\n    ForwardX11Trusted yes\n\nUsing paraphrases: store paraphrases in your keychain if you are using one. Please, follow the instructions here.\n\n\n\nStep 3: Transferring the key\nA. Transfer the key to GitHub/GitLab\nCopy your SSH public key ~/.ssh/id_rsa.pub with your favorite editor, go to your GitHub profile (top left side) &gt; Settings &gt; Access &gt; SSH and GPG keys &gt; New/Add SSH key. You just need to fill up the title (e.g.: Personal laptop) and paste the copied key (e.g.¬†ssh-rsa or ecdsa-xxx).\n# Mac/Linux: copy key \ncat ~/.ssh/id_rsa.pub | pbcopy\n# Windows\ncat ~/.ssh/id_rsa.pub | clip\n\n\n\n\n\n\nGitHub Guidelines Links\n\n\n\n\n\n\nGeneral\nLast step, key transfer\n\n\n\n\nB: Transfer the key to a remote server\nTransfer the public part of your key pair to the remote server. This can be done manually, but the easiest way is using ssh-copy-id:\nssh-copy-id -i ~/.ssh/id_rsa.pub &lt;userid&gt;@ssh-myserver.dk\nThis will prompt for a password to authenticate with the remote server in order to copy the file. Once successful though, future logins will use the key pair for authentication, and will not prompt for a password (assuming you login from your workstation, which has the private part of your key pair).\n\n\n\n\n\n\nNote\n\n\n\nssh-copy-id will blindly append the given key file to the remote ~/.ssh/authorized_keys file. If you made a mistake or copied the same key multiple times, you may want to edit this file! Each line in the file corresponds to one key.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nYou are set to start using your SSH key. If you are using UCloud, follow the next section; otherwise, you are done with this tutorial.\n\n\n\n\n\nUCloud Users\nSimilarly to what is explained above, copy your key (manually or using e.g.¬†pbcopy ~/.ssh/id_rsa.pub) and go to Resources (navigation panel on the left) &gt; SSH keys.\n\nPaste the public key here and give a title to your key as in the image below:\n\nWhen you have added the public part of the SSH key pair to your UCloud account, you can access jobs from your local computer if SSH access is possible for the given app and enabled upon job submission. Open Visual Studio Code to test this yourself! You will need to click on ‚ÄúEnable SSH Server‚Äù when submitting the job as follows:\n\n\n\nSelect ‚ÄúEnable SSH server‚Äù\n\n\nWhen a job with SSH access enabled starts, the command for accessing the job from your local computer via SSH is displayed from the job‚Äôs progress view.\n\n\n\nCopy ssh command\n\n\nNow, open a terminal and run the command:\n# the 4 digits will change everytime you start a new job\nssh ucloud@ssh.cloud.sdu.dk -p 2465\nIf it is the first time you do this, this message will prompt, write yes\nThe authenticity of host '[ssh.cloud.sdu.dk]:2465 ([130.225.164.106]:2465)' can't be established.\nED25519 key fingerprint is SHA256:0Q5WMne+hzOwj5bEfssH/gQrxFDz2fvclCGsQbfLLb8.\nThis key is not known by any other names.\nAre you sure you want to continue connecting (yes/no/[fingerprint])?\nYour terminal will look like the image below which means you can starting interacting with the job from the console on your computer. \nThe connection can be closed using the command\nexit\n\n\n\nSelect ‚ÄúEnable SSH server‚Äù\nCopy ssh command\n\n\n\n\nCopyrightCC-BY-SA 4.0 license"
  }
]
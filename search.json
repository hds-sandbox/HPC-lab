[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "HPC best practices",
    "section": "",
    "text": "High Performance Computing (HPC) plays a crucial role for researchers by offering the computational speed and power needed to manage large and complex data sets, perform simulations, and address intricate problems that would be impractical or too time-consuming with standard computing methods.\nKnowing which HPC resources are accessible and how to use them efficiently is essential for researchers. Making the most of these resources can significantly expedite research and drive innovation. By becoming proficient with the available tools and technologies, researchers can address complex challenges, analyze extensive data sets, and execute advanced simulations with increased speed and accuracy. This module provides essential knowledge on HPC resources and best practices for their utilization.\nThis module offers content for three distinct courses:\n\nHPC Launch: Foundations on HPC and essential knowledge on national HPC resources\nHPC Pipes: Best practices for using workflow management systems and computational environments with HPC\nHPC ML (Machine Learning): Insights into applying HPC for machine learning tasks, including model training, data analysis, and optimization techniques.\n\nBy the end of all the modules, you will gain practical skills in promoting reproducibility through comprehensive training in HPC resource management, workflow pipelines, and computing environments.\n\n\n\n\n\n\nGeneral Course Goals\n\n\n\nBy the end of this workshop, you should be able to apply the following concepts in the context of Next Generation Sequencing data:\n\nUnderstand the Importance of Research Data Management (RDM)\nMake your data analysis and workflows reproducible and FAIR\nMake FAIR environment using conda or Docker\n\n\n\n\n\n\n\n\n\nHPC best practices\n\n\n\nWe offer in-person workshops, keep an eye on the upcoming events on the Sandbox website."
  },
  {
    "objectID": "index.html#hpc-pipes",
    "href": "index.html#hpc-pipes",
    "title": "HPC best practices",
    "section": "HPC pipes",
    "text": "HPC pipes\nThe course “HPC pipes” is designed to provide participants with foundational knowledge and practical skills in writing reproducible pipelines. As part of effective data management, it is crucial that researchers create reproducible analyses that enable others to validate and build upon their work. We will explore essential elements of reproducibility and efficiency in computational research, highlighting techniques and tools for creating robust and transparent coding and workflows. By prioritizing reproducibility and replicability, researchers can enhance the credibility and impact of their findings while fostering collaboration and knowledge dissemination within the scientific community. This approach guarantees efficient research management. Explore our content on practical RDM for more details.\n\n\n\n\n\n\nCourse Requirements\n\n\n\nIt is important to be familiar with unix and python. Follow the tutorials in the links below if you need a refresher.\n\nCommand Line experience (Software Carprentry Shell)\nProgramming experience (Python)\n\n\n\n\n\n\n\n\n\nModule Goals\n\n\n\n\nUnderstand the rol of scientific pipelines\nRun existing pipelines\nImplement and modify pipelines\nSpecify software and computational resource needs\nCustomise your pipeline to accept user-defined configurations (params)\nCreate reproducible analyses that can be adapted to new data with little effort\nIntegrate workflows with software environments\n\n\n\n\nAcknowledgements\nOur exercises are developed using the R packaged developed by Barr and DeBruine (2023)."
  },
  {
    "objectID": "develop/data_transfer.html",
    "href": "develop/data_transfer.html",
    "title": "HPC Lab",
    "section": "",
    "text": "To ensure efficient file transfers on HPC systems, we will go through:\n\nOverview rsync and scp\nBasic commands\n\nWhen transferring files between servers, it’s important to ensure that the files are consistent. Differences in file size can occur due to variations in filesystems, but large differences might indicate an issue. Manually checking file sizes in a terminal (ls -lh or du -sh) to determine if a transfer was successful might not be ideal, as it doesn’t guarantee file integrity. Using checksums provides a more reliable verification method.\nYou can use md5sum to verify that the file contents are identical on both servers. Run the following command on each server:\nmd5sum /path/to/file\nUsing rsync for Efficient File Transfers rsync is a powerful alternative to scp for transferring files. It only sends data if the file has changed, making it more efficient.\n# Transferring files between local machine-server\nrsync -avz local/path/to/file user@server:/remote/path/to/file\n# Transferring files between servers\nrsync -azv server1:/path/to/my_folder server2:/path/to/destination_folder\n\n\n\n\n\n\nNote\n\n\n\nTo transfer files directly between two servers from your local workstation, ensure your SSH setup (configuration, keys, etc.) allows access to both servers. Check this section if you need help setting up your keys (generating, configuring and managing).\n\n\n\n\nrsync is an efficient protocol to compare and copy files between directories or server. It can resume interrupted transfers and compress files on the fly.\n\nChecksum Verification: rsync checks the hashsums of files and only transfers data if the hashes differ. This ensures that only the changed parts of the files are sent (so you can rsync a whole folder, and only the changes files will be send).\nTimestamp Preservation: Using the -a flag with rsync preserves the modified timestamps of files, which is particularly useful for tools like Snakemake.\n\n\n\n\n\n-a: Archive mode, preserves file attributes like timestamps and permissions (important if you are using snakemake).\n-v: Verbose mode, provides detailed information during transfer.\n-z: Compresses data during transfer, reducing the amount of data sent over the network.\n-c: Enables checksum checking, ensuring that files are identical by comparing their contents rather than just their size and modification time.\n\n\n\n\nFor users who prefer a graphical interface, tools like Cyberduck and FileZilla can also be used for transferring files between servers.",
    "crumbs": [
      "HPC launch",
      "Data management",
      "Data transfer"
    ]
  },
  {
    "objectID": "develop/data_transfer.html#data-transfer",
    "href": "develop/data_transfer.html#data-transfer",
    "title": "HPC Lab",
    "section": "",
    "text": "To ensure efficient file transfers on HPC systems, we will go through:\n\nOverview rsync and scp\nBasic commands\n\nWhen transferring files between servers, it’s important to ensure that the files are consistent. Differences in file size can occur due to variations in filesystems, but large differences might indicate an issue. Manually checking file sizes in a terminal (ls -lh or du -sh) to determine if a transfer was successful might not be ideal, as it doesn’t guarantee file integrity. Using checksums provides a more reliable verification method.\nYou can use md5sum to verify that the file contents are identical on both servers. Run the following command on each server:\nmd5sum /path/to/file\nUsing rsync for Efficient File Transfers rsync is a powerful alternative to scp for transferring files. It only sends data if the file has changed, making it more efficient.\n# Transferring files between local machine-server\nrsync -avz local/path/to/file user@server:/remote/path/to/file\n# Transferring files between servers\nrsync -azv server1:/path/to/my_folder server2:/path/to/destination_folder\n\n\n\n\n\n\nNote\n\n\n\nTo transfer files directly between two servers from your local workstation, ensure your SSH setup (configuration, keys, etc.) allows access to both servers. Check this section if you need help setting up your keys (generating, configuring and managing).\n\n\n\n\nrsync is an efficient protocol to compare and copy files between directories or server. It can resume interrupted transfers and compress files on the fly.\n\nChecksum Verification: rsync checks the hashsums of files and only transfers data if the hashes differ. This ensures that only the changed parts of the files are sent (so you can rsync a whole folder, and only the changes files will be send).\nTimestamp Preservation: Using the -a flag with rsync preserves the modified timestamps of files, which is particularly useful for tools like Snakemake.\n\n\n\n\n\n-a: Archive mode, preserves file attributes like timestamps and permissions (important if you are using snakemake).\n-v: Verbose mode, provides detailed information during transfer.\n-z: Compresses data during transfer, reducing the amount of data sent over the network.\n-c: Enables checksum checking, ensuring that files are identical by comparing their contents rather than just their size and modification time.\n\n\n\n\nFor users who prefer a graphical interface, tools like Cyberduck and FileZilla can also be used for transferring files between servers.",
    "crumbs": [
      "HPC launch",
      "Data management",
      "Data transfer"
    ]
  },
  {
    "objectID": "develop/exercise_pipes.html",
    "href": "develop/exercise_pipes.html",
    "title": "Exercises",
    "section": "",
    "text": "Put your learning to the test with what you’ve covered so far.",
    "crumbs": [
      "HPC pipes",
      "Pipeline languages",
      "Exercises"
    ]
  },
  {
    "objectID": "develop/exercise_pipes.html#general-hpc-pipes",
    "href": "develop/exercise_pipes.html#general-hpc-pipes",
    "title": "Exercises",
    "section": "General HPC pipes",
    "text": "General HPC pipes\n1. What role does a workflow manager play in computational research??\n\n Automating the execution of complex computational processes To manually execute each step of a computation Offering real-time and intuitive data analysis To minimize the reliance on specific software environments\n\n2.What is the primary drawback of using shell scripts for automating computations?\n\n Limited error handling and debugging capabilities Difficulty in integrating with notebooks Not compatible with all operating systems They re-run all the steps every time Insufficient support for parallel processing Complex and extensive coding for simple tasks\n\n3. What are the key features of workflow manager in computational research? (Several possible solutions)\n\n Executing tasks only when required Managing task dependencies Overseeing storage and resource allocation Providing intuitive graphical interfaces\n\n4. Workflow managers can run tasks (different) concurrently if there are no dependencies (True or False) TRUEFALSE\n5. A workflow manager can execute a single parallelized task on multiple nodes in a computing cluster (True or False) TRUEFALSE",
    "crumbs": [
      "HPC pipes",
      "Pipeline languages",
      "Exercises"
    ]
  },
  {
    "objectID": "develop/exercise_pipes.html#snakemake",
    "href": "develop/exercise_pipes.html#snakemake",
    "title": "Exercises",
    "section": "Snakemake",
    "text": "Snakemake\n\n\n\n\n\n\nExercise 1S: Exploring Rule Invocation in Snakemake\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this exercise, we will explore how rules are invoked in a Snakemake workflow. Download the Snakefile and data required for this exercise using the links below.\n Download data input   Download Snakefile \nNow follow these steps and answer the questions:\n\nOpen the snakefile, named process_1kgp.smk and try to understand every single line. If you request Snakemake to generate the file results/all_female.txt, what commands will be executed and in what sequence?\nDry run the workflow: Check the number of jobs that will be executed.\n6. How many jobs will Snakemake run? \nRun the workflow: Use the name flag --snakefile | -s follow by the name of the file.\nVerify output: Ensure that the output files are in your working directory.\nClean Up: remove all files starting with EUR in your results folder.\nRerun the workflow: Execute the Snakefile again.\n7. How many jobs did Snakemake run in this last execution? \nRemove lines 4-6 in the process_1kgp.smk. How else can you run the workflow but to generate instead all_male.txt using only the command-line?\nrule all:\n   input:\n      expand(\"results/all_{gender}.txt\", gender=[\"female\"])\n8. Tip: what is missing at the end of the command ( e.g. what should be added to ensure all_male.txt is generated)? snakemake -s process_1kgp.smk -c1 \n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\n# dry run \nsnakemake -s process_1kgp.smk -n \n# run the workflow \nsnakemake -s process_1kgp.smk-c1 &lt;name_rule|name_output&gt;\n# verify output \nls &lt;name_output&gt;\n# remove file belonging to european individuals \nrm results/EUR.tsv results/all_female.txt\n# rerun again \nsnakemake -s process_1kgp.smk -c1 &lt;name_rule|name_output&gt;",
    "crumbs": [
      "HPC pipes",
      "Pipeline languages",
      "Exercises"
    ]
  },
  {
    "objectID": "develop/fair_envs.html",
    "href": "develop/fair_envs.html",
    "title": "HPC Lab",
    "section": "",
    "text": "Recording and sharing the computational environment is essential for reproducibility and transparency. There are several methods to achieve this, but we are going to focus on two of them, mamba (an environment manager) and Docker (a container). Environment managers are user-friendly, easy to share across different systems, and offer lightweight, efficient, and fast start-up times. However, Docker containers provide complete environment isolation (including the operating system), which ensures consistent behavior across various systems.\nEach computer or HPC (High-Performance Computing) platform has a unique computational environment that includes its operating system, installed software, versions of software packages, and other features. If a research project is moved to a different computer or platform, the analysis might not run or produce consistent results if it depends on any of these factors.\nFor research to be reproducible, the original computational environment must be recorded so others can replicate it. There are several methods to achieve this:\n\nContainerization platforms (e.g., Docker, Singularity): allow the researcher to package their software and dependencies into a standardized container image.\nVirtual Machines (e.g., VirtualBox): can share an entire virtualized computing environment (OS, software and dependencies)\nEnvironment managers: provide an isolated environment with specific packages and dependencies that can be installed without affecting the system-wide configuration. These environments are particularly useful for managing conflicting dependencies and ensuring reproducibility. Configuration files can automate the setup of the computational environment:\n\nconda or mamba: allows users to export environment specifications (software and dependencies) to YAML files enabling easy recreation of the environment on another system\nPython virtualenv or pipenv: is a tool for creating isolated environments to manage dependencies specific to a project\nrequirements.txt: may contain commands for installing packages (such as pip for Python packages or apt-get for system-level dependencies), configuring system settings, and setting environment variables. Package managers can be used to install, upgrade and manage packages.\nR’s renv: The ‘renv’ package creates isolated environments in R.\n\nEnvironment descriptors\n\nsessionInfo() or devtools::session_info(): In R, these functions provide detailed information about the current session\nsessionInfo(), similarly, in Python. Libraries like NumPy and Pandas have show_versions() methods to display package versions.\n\n\nWhile environment managers are very easy to use and share across different systems, and are lightweight and efficient, offering fast start-up times, Docker containers provide a full env isolation (including the operating system) which ensures consistent behavior across different systems. Check the table below for a full overview.\n\n\n\nMamba is a reimplementation of the Conda package manager in C++. While our focus will be on Mamba, it’s important to note that it maintains compatibility with Conda by using the same command-line parser, package installation and uninstallation code, and transaction verification routines.\nMamba uses software installation specifications that are maintained by extensive communities of developers, organized into channels, which serve as software repositories. For example, the “bioconda” channel specializes in bioinformatics tools, while “conda-forge” covers a broad range of data science packages.\n\n\n\n\n\n\nMamba vs. conda\n\n\n\nAs previously mentioned, mamba is a newer and faster implementation. The two commands can be used interchangeable (for most tasks). If you use Conda, you should still complete the exercises, as you’ll gain experience with both tools. For more information on their ecosystem and advantages here.\n\n\nMamba allows you to create mutluple software envrinoments, where mutluple pacjage version can co-ecit on your system.\n\n\n\n\n\n\nBuild your mamba environment\n\n\n\n\n\n\n\nFollow mamba instructions to install it. Let’s also include bioconda and conda-forge channels which will come very handy.\nconda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge\nNow you are set to create your first environment. Follow these steps:\n\nCreate a new environment named myenv\nInstall the following packages in myenv: bowtie2, numpy=1.26.4, matplotlib=3.8.3\nCheck the environments available\nLoad/activate the environment\nCheck which python executable is being used and that bowtie2 is installed.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nHere are some of the commands you need for the exercise.\nmamba create &lt;ENV-NAME&gt;\nmamba install --channel &lt;CHANNEL-NAME&gt; --name &lt;ENV-NAME&gt;\nmamba env list\nmamba activate &lt;ENV-NAME&gt;\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThe syntax to create a new environment is: mamba create --name myenv\nExample “bowtie2”: Go to anaconda.org and search for “bowtie2” to confirm it is available through Mamba and which software channel it is provided from. You will find that it is available via the “bioconda” channel: https://anaconda.org/bioconda/bowtie2. The syntax to install packages is: mamba install --channel &lt;CHANNEL-NAME&gt; --name &lt;ENV-NAME&gt; &lt;SOFTWARE-NAME&gt;\n\nmamba install --name myenv --channel bioconda bowtie2=2.5.3 \"matplotlib=3.8.3\" \"numpy=1.26.4\"\nDo the same with the others. 3. To see al environments available mamba env list. There will be a “*” showi8ng the one is activated. 4. Load the environment mamba activate myenv. 5. which python -&gt; should print the one in the environment that is active (path similar to /home/mambaforge/envs/myenv/bin/python). bowtie2 --help\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLoading mamba environments in shell scripts\n\n\n\nIf you need to activate an environment in a shell script that will be submitted to SLURM, you must first source Mamba’s configuration file. For instance, to load the myenv environment we created, the script would include the following code:\n# Always add these two commands to your scripts\neval \"$(conda shell.bash hook)\"\nsource $CONDA_PREFIX/etc/profile.d/mamba.sh\n\n# then you can activate the environment\nmamba activate myenv\nWhen jobs are submitted to SLURM, they run in a non-interactive shell where Mamba isn’t automatically set up. By running the source command, you ensure that Mamba’s activate function is available. It’s important to remember that even if the environment is loaded on the login node, the scripts will execute on a different machine (one of the compute nodes). Therefore, always include the command to load the Mamba environment in your SLURM submission scripts.\n\n\n\n\n\nEssentially, a container is a self-contained, lightweight package that includes everything needed to run a specific application—such as the operating system, libraries, and the application code itself. Containers operate independently of the host system, which allows them to run the same software across various environments without any conflicts or interference. This isolation ensures that researchers can consistently execute their code on different systems and platforms, without worrying about dependency issues or conflicts with other software on the host machine.\n\n\n\n\n\n\nDocker vs. Singularity\n\n\n\nThe most significant difference is at the permission level required to run them. Docker containers operate as root by default, giving them full access to the host system. While this can be useful in certain situations, it also poses security risks, especially in multi-user environments. In contrast, Singularity containers run as non-root users by default, enhancing security and preventing unauthorized access to the host system.\n\nDocker is ideal for building and distributing software across different operating systems\nSingularity is designed for HPC environments and offers high performance without needing root access\n\n\n\n\n\nDocker helps developers build, share, run, and verify applications anywhere — without tedious environment configuration or management.\n\n\n\nWhile you can build your own Singularity images, many popular software packages already have pre-built images available from public repositories. The two repositories you’ll most likely use or hear about are:\n\ndepot.galaxyproject.org\nSylabs\n\n\n\n\n\n\n\nTip\n\n\n\n\nWe recommend using the pre-installed version provided by your system administrators if you’re working on a shared system. If you’re working on your own computer, you can install the necessary software using Mamba.\nThey might host different versions of the same software, so it’s worth checking both to find the version you need.\nTo download a software container from public repositories, use the singularity pull command.\nTo execute a command within the software container, use the singularity run command.\nGood practice: create a directory to save all singularity images together. .sif is the standard extension for the images.\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nDownload a singularity image from one of the two repositories listed above (choose a software like bcftools, bedtools, bowtie2, seqkit…) and run the --help command. This command displays the help documentation of the program, verifying that our image is functioning correctly and includes the intended software.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n# create a directory for our singularity images\nmkdir images\n\n# download the image\nsingularity pull images/bowtie2-2.5.4.sif https://depot.galaxyproject.org/singularity/bowtie2%3A2.5.4--he20e202_2\n\n# run the image: singularity run &lt;PATH-TO-IMAGE&gt; &lt;YOUR COMMANDS&gt;\nsingularity run images/bowtie2-2.5.4.sif bowtie2 --help",
    "crumbs": [
      "HPC pipes",
      "Computational environments",
      "FAIR environments"
    ]
  },
  {
    "objectID": "develop/fair_envs.html#fair-environments",
    "href": "develop/fair_envs.html#fair-environments",
    "title": "HPC Lab",
    "section": "",
    "text": "Recording and sharing the computational environment is essential for reproducibility and transparency. There are several methods to achieve this, but we are going to focus on two of them, mamba (an environment manager) and Docker (a container). Environment managers are user-friendly, easy to share across different systems, and offer lightweight, efficient, and fast start-up times. However, Docker containers provide complete environment isolation (including the operating system), which ensures consistent behavior across various systems.\nEach computer or HPC (High-Performance Computing) platform has a unique computational environment that includes its operating system, installed software, versions of software packages, and other features. If a research project is moved to a different computer or platform, the analysis might not run or produce consistent results if it depends on any of these factors.\nFor research to be reproducible, the original computational environment must be recorded so others can replicate it. There are several methods to achieve this:\n\nContainerization platforms (e.g., Docker, Singularity): allow the researcher to package their software and dependencies into a standardized container image.\nVirtual Machines (e.g., VirtualBox): can share an entire virtualized computing environment (OS, software and dependencies)\nEnvironment managers: provide an isolated environment with specific packages and dependencies that can be installed without affecting the system-wide configuration. These environments are particularly useful for managing conflicting dependencies and ensuring reproducibility. Configuration files can automate the setup of the computational environment:\n\nconda or mamba: allows users to export environment specifications (software and dependencies) to YAML files enabling easy recreation of the environment on another system\nPython virtualenv or pipenv: is a tool for creating isolated environments to manage dependencies specific to a project\nrequirements.txt: may contain commands for installing packages (such as pip for Python packages or apt-get for system-level dependencies), configuring system settings, and setting environment variables. Package managers can be used to install, upgrade and manage packages.\nR’s renv: The ‘renv’ package creates isolated environments in R.\n\nEnvironment descriptors\n\nsessionInfo() or devtools::session_info(): In R, these functions provide detailed information about the current session\nsessionInfo(), similarly, in Python. Libraries like NumPy and Pandas have show_versions() methods to display package versions.\n\n\nWhile environment managers are very easy to use and share across different systems, and are lightweight and efficient, offering fast start-up times, Docker containers provide a full env isolation (including the operating system) which ensures consistent behavior across different systems. Check the table below for a full overview.\n\n\n\nMamba is a reimplementation of the Conda package manager in C++. While our focus will be on Mamba, it’s important to note that it maintains compatibility with Conda by using the same command-line parser, package installation and uninstallation code, and transaction verification routines.\nMamba uses software installation specifications that are maintained by extensive communities of developers, organized into channels, which serve as software repositories. For example, the “bioconda” channel specializes in bioinformatics tools, while “conda-forge” covers a broad range of data science packages.\n\n\n\n\n\n\nMamba vs. conda\n\n\n\nAs previously mentioned, mamba is a newer and faster implementation. The two commands can be used interchangeable (for most tasks). If you use Conda, you should still complete the exercises, as you’ll gain experience with both tools. For more information on their ecosystem and advantages here.\n\n\nMamba allows you to create mutluple software envrinoments, where mutluple pacjage version can co-ecit on your system.\n\n\n\n\n\n\nBuild your mamba environment\n\n\n\n\n\n\n\nFollow mamba instructions to install it. Let’s also include bioconda and conda-forge channels which will come very handy.\nconda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge\nNow you are set to create your first environment. Follow these steps:\n\nCreate a new environment named myenv\nInstall the following packages in myenv: bowtie2, numpy=1.26.4, matplotlib=3.8.3\nCheck the environments available\nLoad/activate the environment\nCheck which python executable is being used and that bowtie2 is installed.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nHere are some of the commands you need for the exercise.\nmamba create &lt;ENV-NAME&gt;\nmamba install --channel &lt;CHANNEL-NAME&gt; --name &lt;ENV-NAME&gt;\nmamba env list\nmamba activate &lt;ENV-NAME&gt;\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThe syntax to create a new environment is: mamba create --name myenv\nExample “bowtie2”: Go to anaconda.org and search for “bowtie2” to confirm it is available through Mamba and which software channel it is provided from. You will find that it is available via the “bioconda” channel: https://anaconda.org/bioconda/bowtie2. The syntax to install packages is: mamba install --channel &lt;CHANNEL-NAME&gt; --name &lt;ENV-NAME&gt; &lt;SOFTWARE-NAME&gt;\n\nmamba install --name myenv --channel bioconda bowtie2=2.5.3 \"matplotlib=3.8.3\" \"numpy=1.26.4\"\nDo the same with the others. 3. To see al environments available mamba env list. There will be a “*” showi8ng the one is activated. 4. Load the environment mamba activate myenv. 5. which python -&gt; should print the one in the environment that is active (path similar to /home/mambaforge/envs/myenv/bin/python). bowtie2 --help\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLoading mamba environments in shell scripts\n\n\n\nIf you need to activate an environment in a shell script that will be submitted to SLURM, you must first source Mamba’s configuration file. For instance, to load the myenv environment we created, the script would include the following code:\n# Always add these two commands to your scripts\neval \"$(conda shell.bash hook)\"\nsource $CONDA_PREFIX/etc/profile.d/mamba.sh\n\n# then you can activate the environment\nmamba activate myenv\nWhen jobs are submitted to SLURM, they run in a non-interactive shell where Mamba isn’t automatically set up. By running the source command, you ensure that Mamba’s activate function is available. It’s important to remember that even if the environment is loaded on the login node, the scripts will execute on a different machine (one of the compute nodes). Therefore, always include the command to load the Mamba environment in your SLURM submission scripts.\n\n\n\n\n\nEssentially, a container is a self-contained, lightweight package that includes everything needed to run a specific application—such as the operating system, libraries, and the application code itself. Containers operate independently of the host system, which allows them to run the same software across various environments without any conflicts or interference. This isolation ensures that researchers can consistently execute their code on different systems and platforms, without worrying about dependency issues or conflicts with other software on the host machine.\n\n\n\n\n\n\nDocker vs. Singularity\n\n\n\nThe most significant difference is at the permission level required to run them. Docker containers operate as root by default, giving them full access to the host system. While this can be useful in certain situations, it also poses security risks, especially in multi-user environments. In contrast, Singularity containers run as non-root users by default, enhancing security and preventing unauthorized access to the host system.\n\nDocker is ideal for building and distributing software across different operating systems\nSingularity is designed for HPC environments and offers high performance without needing root access\n\n\n\n\n\nDocker helps developers build, share, run, and verify applications anywhere — without tedious environment configuration or management.\n\n\n\nWhile you can build your own Singularity images, many popular software packages already have pre-built images available from public repositories. The two repositories you’ll most likely use or hear about are:\n\ndepot.galaxyproject.org\nSylabs\n\n\n\n\n\n\n\nTip\n\n\n\n\nWe recommend using the pre-installed version provided by your system administrators if you’re working on a shared system. If you’re working on your own computer, you can install the necessary software using Mamba.\nThey might host different versions of the same software, so it’s worth checking both to find the version you need.\nTo download a software container from public repositories, use the singularity pull command.\nTo execute a command within the software container, use the singularity run command.\nGood practice: create a directory to save all singularity images together. .sif is the standard extension for the images.\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nDownload a singularity image from one of the two repositories listed above (choose a software like bcftools, bedtools, bowtie2, seqkit…) and run the --help command. This command displays the help documentation of the program, verifying that our image is functioning correctly and includes the intended software.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n# create a directory for our singularity images\nmkdir images\n\n# download the image\nsingularity pull images/bowtie2-2.5.4.sif https://depot.galaxyproject.org/singularity/bowtie2%3A2.5.4--he20e202_2\n\n# run the image: singularity run &lt;PATH-TO-IMAGE&gt; &lt;YOUR COMMANDS&gt;\nsingularity run images/bowtie2-2.5.4.sif bowtie2 --help",
    "crumbs": [
      "HPC pipes",
      "Computational environments",
      "FAIR environments"
    ]
  },
  {
    "objectID": "develop/fair_envs.html#sources",
    "href": "develop/fair_envs.html#sources",
    "title": "HPC Lab",
    "section": "Sources",
    "text": "Sources\n\nAnaconda for searching Mamba/conda packages\nBioconda for installing software package related to biomedical research\nConda cheat sheet\nfaircookbook worflows\nDocker\nDocker get-started\nThe turing way - reproducible research\n\nFind pre-built singularity images:\n\ndepot.galaxyproject.org\nSylabs\n\nOther training resources: The turing way - reproducible research and HPC intro by Cambridge",
    "crumbs": [
      "HPC pipes",
      "Computational environments",
      "FAIR environments"
    ]
  },
  {
    "objectID": "develop/ssh_keys.html",
    "href": "develop/ssh_keys.html",
    "title": "HPC Lab",
    "section": "",
    "text": "Using SSH keys for authentication is recommended for accessing servers and version control platforms such as GitLab, GitHub, or BitBucket. SSH keys provide a more convenient and secure method than passwords, allowing you to avoid typing a password every time you log in.\nThis tutorial guides you through configuring SSH keys, using GitHub as an example.\n\n\nAn SSH (Secure Shell) is a network protocol that allows you to execute commands on a remote server securely. SSH keys are cryptographic keys used for authenticating a user or device within the SSH protocol. They come in pairs: the “private” key and the “public” key.\n\nPublic Key: This key can be freely shared with anyone and is added to the ~/.ssh/authorized_keys file on the remote server you wish to access.\nPrivate Key: This key should remain on your local machine and be carefully protested, similar to a password. It should never be shared.\n\nThe holder of the private key can authenticate with any remote system that has the corresponding public key. For example, you can use a single public/private key pair to authenticate with the cluster at your research center and your GitHub account.\n\n\n\n\n\n\nHow SSH keys work\n\n\n\nThink of the public key as a complex number x, and the private key as its inverse 1/x. Encrypting a message m with your public key is like multiplying your message by x, making mx completely unreadable. Using the private key, you can decrypt m by multiplying mx by 1/x. This process is secure because you cannot derive 1/x from x. Therefore, you can safely distribute the public key to the machines you want to access while keeping the private key on your machine.\n\n\n\n\n\n\n\nOn your (Mac or Linux) workstation, create the public/private key pair. There are several types of keys and by default you will get an RSA pair. When prompted for a passphrase, just hit enter so that you won’t need a password when logging in.\nWhen prompted, press Enter to store the keys in .ssh/id_rsa (we recommend not to modify this). Press Enter twice again to skip setting a passphrase, which otherwise would require you to enter it each time you use git push or git pull. This process generates two files: the public key (~/.ssh/id_rsa.pub) and the private key (~/.ssh/id_rsa).\nssh-keygen\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/Users/abc123/.ssh/id_rsa): \nCreated directory '/Users/abc123/.ssh'.\nEnter passphrase (empty for no passphrase): \nEnter same passphrase again: \nYour identification has been saved in /Users/abc123/.ssh/id_rsa\nYour public key has been saved in /Users/abc123/.ssh/id_rsa.pub\nThe key fingerprint is:\nSHA256:w8JIQXafdq6scMiNZefNd8lVz9UmIzm0eiO+pvGOM6A abc123@c9b1d769db6e\nThe key's randomart image is:\n+---[RSA 3072]----+\n|   .+ .     .    |\n|   . o . . . o  .|\n|    .   + . = o =|\n|   . o o o . o =+|\n|    . = S + o   +|\n|   . *.= * o o o |\n|    =.o.= + . +  |\n|    Eo .o+.o .   |\n|      . o*+      |\n+----[SHA256]-----+\ncat `~/.ssh/id_rsa.pub\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCs3nqhRr7FyB8UKxqQ5mA3Gk60oty5lGdzcpVxjL08QmjLz15X6Odo2EotmJQuzCyYbzPw4Tq404N3NOmVqYywrn03i3kmCnZd5MYq8yi72Zsk9qFfgEt7pxpf5WJ+AWSDXwUkUQK2FFby2RWbi4gwTmRHfkB999j8F3F/d5EpB0llQfxmZWIIrX9ampgxfI8+yTIyeG8gpx5028h1oR5qukEFJk6nAoj5on+/vBqHdc0AVPd6jNoNCDZ6ur4xwUPam8iQ6A4NfbnCVUPa95Wqt+QWEF6Mn4UB1WIvly+blDzg5rVXvd9B685bZjQWfUsxSiRPkhtV6X2yImklt+KjV0ufB/Yl1O+x6S8r2+I41WSxwJNr5h5L588l3GH4ehT79uCtrxiu4zas+4s9OW02Ox3auCfLi1/jp/CJ5IsuLwM/jPBXrgzw+HREKM2gtm9d12gU2b9o9bmASZVhEKqeTb0aRPjcTYYdnjOWUl+pqkdVPyB7mJs8NAQemN/shvM= abc123@c9b1d769db6e\nThe public key can be safely displayed cat and shared with any achine where it will function as a personal lock that can be only opened with your private key.\n\n\n\n\n\n\nTip\n\n\n\nTip 1: If you provided a passphrase when generating your keys, you would need to enter this passphrase each time you use your private key. The passphrase adds an extra layer of security in case your private key is compromised. For simplicity, we not always recommend using a passphrase unless you are working in environments where people not part of the project might have access.\nTip 2: DSA keys should be avoided. An RSA key with at least 2048 bits should provide sufficient security. Finally, ecdsa and ed25519 keys are also fine (don’t worry about the number of bits for these keys).\n# Recommended\nssh-keygen -t rsa -b 4096\n# Alternative\nssh-keygen -t ed25519\n\n\nUsing an existing public/private key pair\nIf you already have an SSH key pair, you can use it without generating a new one. To check the details of your existing key, use the ssh-keygen command to display the key’s fingerprint.\nssh-keygen -l -f ~/.ssh/id_rsa\n2048 SHA256:7l0HauYJVRaQhuzmti8XEZImnRbzipu3NKGnE6tDFRk grg@t430s (RSA)\nAlternatively, check the content of the ssh directory: - Mac/Unix: ls -la ~/.ssh/ - Windows: ls ~/.ssh/\n\n\n\nFollow the steps below if you are using Mac or Linux.\n\n\n\n\n\n\nWarning\n\n\n\n\nWindows users: follow these instructions instead, the process is very similar.\nIf you are getting errors when running the commands below, please read carefully these guidelines as GitHUb provides instructions on what to do with common issues users run into for this specific step 2.\nGeneral GitHUbtroubleshooting\n\n\n\n# ONLY if you need to start the ssh agent. First, verify it is not already running: \n# pgrep -u $USER ssh-agent\n# If that command returns a process ID, your SSH agent is running! If not, run the command below: \n# eval \"$(ssh-agent -s)\"\n\n# Manually modify the `~/.ssh/config` file to automatically load keys into ssh-agent, like in the example below. For GitHub, the User is **git**, do not write your own. \n\nHost github.com\n  AddKeysToAgent yes\n  IdentityFile ~/.ssh/id_rsa\n\n# Note: if the file does not `touch ~/.ssh/config`to create the file. \n\n# Add the SSH private to the ssh-agent\nssh-add ~/.ssh/id_rsa\n\n# Optional, verify the keys are loaded\nssh-add -l\n\n\n\n\n\n\nOther tips\n\n\n\n\n~/.ssh/config: add several hosts (servers, GitLab, GitHUb, etc.).\n\nEdit the SSH config file Create and use a ~/.ssh/config file on your workstation. E.g., using a password-less ssh key for authentication and the following ~/.ssh/config, user abc123 can login with the ssh xyz command.\nHost xyz\n    Hostname ssh-myserver.dk\n    User abc123\n    ServerAliveInterval 60\n    ForwardX11 yes\n    ForwardX11Trusted yes\n\nUsing paraphrases: store paraphrases in your keychain if you are using one. Please, follow the instructions here.\n\n\n\nStep 3: Transferring the key\nA. Transfer the key to GitHub/GitLab\nCopy your SSH public key ~/.ssh/id_rsa.pub with your favorite editor, go to your GitHub profile (top left side) &gt; Settings &gt; Access &gt; SSH and GPG keys &gt; New/Add SSH key. You just need to fill up the title (e.g.: Personal laptop) and paste the copied key (e.g. ssh-rsa or ecdsa-xxx).\n# Mac/Linux: copy key \ncat ~/.ssh/id_rsa.pub | pbcopy\n# Windows\ncat ~/.ssh/id_rsa.pub | clip\n\n\n\n\n\n\nGitHub Guidelines Links\n\n\n\n\n\n\nGeneral\nLast step, key transfer\n\n\n\n\nB: Transfer the key to a remote server\nTransfer the public part of your key pair to the remote server. This can be done manually, but the easiest way is using ssh-copy-id:\nssh-copy-id -i ~/.ssh/id_rsa.pub &lt;userid&gt;@ssh-myserver.dk\nThis will prompt for a password to authenticate with the remote server in order to copy the file. Once successful though, future logins will use the key pair for authentication, and will not prompt for a password (assuming you login from your workstation, which has the private part of your key pair).\n\n\n\n\n\n\nNote\n\n\n\nssh-copy-id will blindly append the given key file to the remote ~/.ssh/authorized_keys file. If you made a mistake or copied the same key multiple times, you may want to edit this file! Each line in the file corresponds to one key.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nYou are set to start using your SSH key. If you are using UCloud, follow the next section; otherwise, you are done with this tutorial.\n\n\n\n\n\n\nSimilarly to what is explained above, copy your key (manually or using e.g. pbcopy ~/.ssh/id_rsa.pub) and go to Resources (navigation panel on the left) &gt; SSH keys.\n\nPaste the public key here and give a title to your key as in the image below:\n\nWhen you have added the public part of the SSH key pair to your UCloud account, you can access jobs from your local computer if SSH access is possible for the given app and enabled upon job submission. Open Visual Studio Code to test this yourself! You will need to click on “Enable SSH Server” when submitting the job as follows:\n\n\n\nSelect “Enable SSH server”\n\n\nWhen a job with SSH access enabled starts, the command for accessing the job from your local computer via SSH is displayed from the job’s progress view.\n\n\n\nCopy ssh command\n\n\nNow, open a terminal and run the command:\n# the 4 digits will change everytime you start a new job\nssh ucloud@ssh.cloud.sdu.dk -p 2465\nIf it is the first time you do this, this message will prompt, write yes\nThe authenticity of host '[ssh.cloud.sdu.dk]:2465 ([130.225.164.106]:2465)' can't be established.\nED25519 key fingerprint is SHA256:0Q5WMne+hzOwj5bEfssH/gQrxFDz2fvclCGsQbfLLb8.\nThis key is not known by any other names.\nAre you sure you want to continue connecting (yes/no/[fingerprint])?\nYour terminal will look like the image below which means you can starting interacting with the job from the console on your computer. \nThe connection can be closed using the command:\nexit",
    "crumbs": [
      "HPC launch",
      "Data management",
      "SSH keys"
    ]
  },
  {
    "objectID": "develop/ssh_keys.html#ssh-keys",
    "href": "develop/ssh_keys.html#ssh-keys",
    "title": "HPC Lab",
    "section": "",
    "text": "Using SSH keys for authentication is recommended for accessing servers and version control platforms such as GitLab, GitHub, or BitBucket. SSH keys provide a more convenient and secure method than passwords, allowing you to avoid typing a password every time you log in.\nThis tutorial guides you through configuring SSH keys, using GitHub as an example.\n\n\nAn SSH (Secure Shell) is a network protocol that allows you to execute commands on a remote server securely. SSH keys are cryptographic keys used for authenticating a user or device within the SSH protocol. They come in pairs: the “private” key and the “public” key.\n\nPublic Key: This key can be freely shared with anyone and is added to the ~/.ssh/authorized_keys file on the remote server you wish to access.\nPrivate Key: This key should remain on your local machine and be carefully protested, similar to a password. It should never be shared.\n\nThe holder of the private key can authenticate with any remote system that has the corresponding public key. For example, you can use a single public/private key pair to authenticate with the cluster at your research center and your GitHub account.\n\n\n\n\n\n\nHow SSH keys work\n\n\n\nThink of the public key as a complex number x, and the private key as its inverse 1/x. Encrypting a message m with your public key is like multiplying your message by x, making mx completely unreadable. Using the private key, you can decrypt m by multiplying mx by 1/x. This process is secure because you cannot derive 1/x from x. Therefore, you can safely distribute the public key to the machines you want to access while keeping the private key on your machine.\n\n\n\n\n\n\n\nOn your (Mac or Linux) workstation, create the public/private key pair. There are several types of keys and by default you will get an RSA pair. When prompted for a passphrase, just hit enter so that you won’t need a password when logging in.\nWhen prompted, press Enter to store the keys in .ssh/id_rsa (we recommend not to modify this). Press Enter twice again to skip setting a passphrase, which otherwise would require you to enter it each time you use git push or git pull. This process generates two files: the public key (~/.ssh/id_rsa.pub) and the private key (~/.ssh/id_rsa).\nssh-keygen\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/Users/abc123/.ssh/id_rsa): \nCreated directory '/Users/abc123/.ssh'.\nEnter passphrase (empty for no passphrase): \nEnter same passphrase again: \nYour identification has been saved in /Users/abc123/.ssh/id_rsa\nYour public key has been saved in /Users/abc123/.ssh/id_rsa.pub\nThe key fingerprint is:\nSHA256:w8JIQXafdq6scMiNZefNd8lVz9UmIzm0eiO+pvGOM6A abc123@c9b1d769db6e\nThe key's randomart image is:\n+---[RSA 3072]----+\n|   .+ .     .    |\n|   . o . . . o  .|\n|    .   + . = o =|\n|   . o o o . o =+|\n|    . = S + o   +|\n|   . *.= * o o o |\n|    =.o.= + . +  |\n|    Eo .o+.o .   |\n|      . o*+      |\n+----[SHA256]-----+\ncat `~/.ssh/id_rsa.pub\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCs3nqhRr7FyB8UKxqQ5mA3Gk60oty5lGdzcpVxjL08QmjLz15X6Odo2EotmJQuzCyYbzPw4Tq404N3NOmVqYywrn03i3kmCnZd5MYq8yi72Zsk9qFfgEt7pxpf5WJ+AWSDXwUkUQK2FFby2RWbi4gwTmRHfkB999j8F3F/d5EpB0llQfxmZWIIrX9ampgxfI8+yTIyeG8gpx5028h1oR5qukEFJk6nAoj5on+/vBqHdc0AVPd6jNoNCDZ6ur4xwUPam8iQ6A4NfbnCVUPa95Wqt+QWEF6Mn4UB1WIvly+blDzg5rVXvd9B685bZjQWfUsxSiRPkhtV6X2yImklt+KjV0ufB/Yl1O+x6S8r2+I41WSxwJNr5h5L588l3GH4ehT79uCtrxiu4zas+4s9OW02Ox3auCfLi1/jp/CJ5IsuLwM/jPBXrgzw+HREKM2gtm9d12gU2b9o9bmASZVhEKqeTb0aRPjcTYYdnjOWUl+pqkdVPyB7mJs8NAQemN/shvM= abc123@c9b1d769db6e\nThe public key can be safely displayed cat and shared with any achine where it will function as a personal lock that can be only opened with your private key.\n\n\n\n\n\n\nTip\n\n\n\nTip 1: If you provided a passphrase when generating your keys, you would need to enter this passphrase each time you use your private key. The passphrase adds an extra layer of security in case your private key is compromised. For simplicity, we not always recommend using a passphrase unless you are working in environments where people not part of the project might have access.\nTip 2: DSA keys should be avoided. An RSA key with at least 2048 bits should provide sufficient security. Finally, ecdsa and ed25519 keys are also fine (don’t worry about the number of bits for these keys).\n# Recommended\nssh-keygen -t rsa -b 4096\n# Alternative\nssh-keygen -t ed25519\n\n\nUsing an existing public/private key pair\nIf you already have an SSH key pair, you can use it without generating a new one. To check the details of your existing key, use the ssh-keygen command to display the key’s fingerprint.\nssh-keygen -l -f ~/.ssh/id_rsa\n2048 SHA256:7l0HauYJVRaQhuzmti8XEZImnRbzipu3NKGnE6tDFRk grg@t430s (RSA)\nAlternatively, check the content of the ssh directory: - Mac/Unix: ls -la ~/.ssh/ - Windows: ls ~/.ssh/\n\n\n\nFollow the steps below if you are using Mac or Linux.\n\n\n\n\n\n\nWarning\n\n\n\n\nWindows users: follow these instructions instead, the process is very similar.\nIf you are getting errors when running the commands below, please read carefully these guidelines as GitHUb provides instructions on what to do with common issues users run into for this specific step 2.\nGeneral GitHUbtroubleshooting\n\n\n\n# ONLY if you need to start the ssh agent. First, verify it is not already running: \n# pgrep -u $USER ssh-agent\n# If that command returns a process ID, your SSH agent is running! If not, run the command below: \n# eval \"$(ssh-agent -s)\"\n\n# Manually modify the `~/.ssh/config` file to automatically load keys into ssh-agent, like in the example below. For GitHub, the User is **git**, do not write your own. \n\nHost github.com\n  AddKeysToAgent yes\n  IdentityFile ~/.ssh/id_rsa\n\n# Note: if the file does not `touch ~/.ssh/config`to create the file. \n\n# Add the SSH private to the ssh-agent\nssh-add ~/.ssh/id_rsa\n\n# Optional, verify the keys are loaded\nssh-add -l\n\n\n\n\n\n\nOther tips\n\n\n\n\n~/.ssh/config: add several hosts (servers, GitLab, GitHUb, etc.).\n\nEdit the SSH config file Create and use a ~/.ssh/config file on your workstation. E.g., using a password-less ssh key for authentication and the following ~/.ssh/config, user abc123 can login with the ssh xyz command.\nHost xyz\n    Hostname ssh-myserver.dk\n    User abc123\n    ServerAliveInterval 60\n    ForwardX11 yes\n    ForwardX11Trusted yes\n\nUsing paraphrases: store paraphrases in your keychain if you are using one. Please, follow the instructions here.\n\n\n\nStep 3: Transferring the key\nA. Transfer the key to GitHub/GitLab\nCopy your SSH public key ~/.ssh/id_rsa.pub with your favorite editor, go to your GitHub profile (top left side) &gt; Settings &gt; Access &gt; SSH and GPG keys &gt; New/Add SSH key. You just need to fill up the title (e.g.: Personal laptop) and paste the copied key (e.g. ssh-rsa or ecdsa-xxx).\n# Mac/Linux: copy key \ncat ~/.ssh/id_rsa.pub | pbcopy\n# Windows\ncat ~/.ssh/id_rsa.pub | clip\n\n\n\n\n\n\nGitHub Guidelines Links\n\n\n\n\n\n\nGeneral\nLast step, key transfer\n\n\n\n\nB: Transfer the key to a remote server\nTransfer the public part of your key pair to the remote server. This can be done manually, but the easiest way is using ssh-copy-id:\nssh-copy-id -i ~/.ssh/id_rsa.pub &lt;userid&gt;@ssh-myserver.dk\nThis will prompt for a password to authenticate with the remote server in order to copy the file. Once successful though, future logins will use the key pair for authentication, and will not prompt for a password (assuming you login from your workstation, which has the private part of your key pair).\n\n\n\n\n\n\nNote\n\n\n\nssh-copy-id will blindly append the given key file to the remote ~/.ssh/authorized_keys file. If you made a mistake or copied the same key multiple times, you may want to edit this file! Each line in the file corresponds to one key.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nYou are set to start using your SSH key. If you are using UCloud, follow the next section; otherwise, you are done with this tutorial.\n\n\n\n\n\n\nSimilarly to what is explained above, copy your key (manually or using e.g. pbcopy ~/.ssh/id_rsa.pub) and go to Resources (navigation panel on the left) &gt; SSH keys.\n\nPaste the public key here and give a title to your key as in the image below:\n\nWhen you have added the public part of the SSH key pair to your UCloud account, you can access jobs from your local computer if SSH access is possible for the given app and enabled upon job submission. Open Visual Studio Code to test this yourself! You will need to click on “Enable SSH Server” when submitting the job as follows:\n\n\n\nSelect “Enable SSH server”\n\n\nWhen a job with SSH access enabled starts, the command for accessing the job from your local computer via SSH is displayed from the job’s progress view.\n\n\n\nCopy ssh command\n\n\nNow, open a terminal and run the command:\n# the 4 digits will change everytime you start a new job\nssh ucloud@ssh.cloud.sdu.dk -p 2465\nIf it is the first time you do this, this message will prompt, write yes\nThe authenticity of host '[ssh.cloud.sdu.dk]:2465 ([130.225.164.106]:2465)' can't be established.\nED25519 key fingerprint is SHA256:0Q5WMne+hzOwj5bEfssH/gQrxFDz2fvclCGsQbfLLb8.\nThis key is not known by any other names.\nAre you sure you want to continue connecting (yes/no/[fingerprint])?\nYour terminal will look like the image below which means you can starting interacting with the job from the console on your computer. \nThe connection can be closed using the command:\nexit",
    "crumbs": [
      "HPC launch",
      "Data management",
      "SSH keys"
    ]
  },
  {
    "objectID": "develop/data_compression.html#data-organization",
    "href": "develop/data_compression.html#data-organization",
    "title": "HPC Lab",
    "section": "Data organization",
    "text": "Data organization\n\nregular expression\ncookiecutter"
  },
  {
    "objectID": "develop/smk.html",
    "href": "develop/smk.html",
    "title": "Snakemake",
    "section": "",
    "text": "In this section, we will guide you through transitioning from bash scripts or notebooks to workflows. This approach is particularly useful as computations become more complex.",
    "crumbs": [
      "HPC pipes",
      "Pipeline languages",
      "Snakemake"
    ]
  },
  {
    "objectID": "develop/smk.html#sources",
    "href": "develop/smk.html#sources",
    "title": "Snakemake",
    "section": "Sources",
    "text": "Sources\n\nSnakemake tutorial\nSnakemake turorial slides by Johannes Koster\nKöster, Johannes and Rahmann, Sven. “Snakemake - A scalable bioinformatics workflow engine”. Bioinformatics 2012.\nKöster, Johannes. “Parallelization, Scalability, and Reproducibility in Next-Generation Sequencing Analysis”, PhD thesis, TU Dortmund 2014.\nfaircookbook worflows",
    "crumbs": [
      "HPC pipes",
      "Pipeline languages",
      "Snakemake"
    ]
  },
  {
    "objectID": "develop/hpc_intro.html",
    "href": "develop/hpc_intro.html",
    "title": "HPC launch",
    "section": "",
    "text": "Course Overview\n\n\n\n\n⏰ Total Time Estimation: X hours\n\n📁 Supporting Materials:\n\n👨‍💻 Target Audience: Ph.D., MSc, anyone interested in HPC systems.\n👩‍🎓 Level: Advanced.\n🔒 License: Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license.\n\n💰 Funding: This project was funded by the Novo Nordisk Fonden (NNF20OC0063268).",
    "crumbs": [
      "HPC launch",
      "HPC systems",
      "HPC launch"
    ]
  },
  {
    "objectID": "develop/hpc_intro.html#title",
    "href": "develop/hpc_intro.html#title",
    "title": "HPC launch",
    "section": "Title",
    "text": "Title\nIntroduction High-Performance Computing (HPCs) and HPC cluster.\nHPC main resources:\n\nCPU\nRAM\nGPU\n\nSchematic of components of an HPC\n\nNodes\nThere are two typoes of nodes on a cluster: - login nodes (also known as head or submit nodes). - compute nodes (also known as worker nodes).\n\n\n\n\n\n\nWhat can I run from a login node\n\n\n\nA straightforward rule: do not run anything on the login node to prevent potential problems. If the login node crashes, the entire system may need to be rebooted, affecting everyone. Remember, you’re not the only one using the HPC—so be considerate of others. For easy, quick tasks, request an interactive access to one of the compute nodes.\n\n\n\n\nJob scheduler\n\n\n\n\n\n\nNote\n\n\n\nSeveral job scheduler programs are available, and SLURM is among the most widely used. In the next section, we’ll explore SLURM in greater detail, along with general best practices for running jobs.\n\n\n\n\nFilesystem\n\nScratch\nUsers working space\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nI have an omics pipeline that produces a large number of files, resulting in a couple of terabytes of data after processing and analysis. The project will continue for a few more years, and I’ve decided to store the data in the scratch folder. Do you agree with this decision, and why? What factors should be considered when deciding which data to retain and where to store it?\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nTypically, scratch storage is not backed up, so it’s not advisable to rely on it for important data. At a minimum, ensure you back up the raw data and the scripts used for processing. This way, if some processed files are lost, you can replicate the analyses.\nWhen deciding which data to keep on the HPC, back up, or delete, consider the following:\n\nProcessing Time: Evaluate how long each step of the analysis takes to run. There may be significant computational costs associated with re-running heavy data processing steps.\nStorage Management: Use tools like Snakemake to manage intermediate files. You can configure Snakemake to automatically delete intermediate files once the final results are produced, helping you manage storage more efficiently.\n\n\n\n\n\n\n\n\n\n\n\n\n\nBefore start using an HPC\nHigh-Performance Computing (HPC) systems might be organized differently, but there is typically an HPC administration team you can contact to understand how your specific HPC is structured. Key information you should seek from them includes:\n\nThe types of compute nodes available.\nThe storage options you can access and the amount allocated per user.\nWhether a job scheduler software is in use, and if so, which one. You can also request a sample submission script to help you get started.\nThe policy on who bears the cost of using the HPC resources.\nWhether you can install your own software and create custom environments.\n\n\n\n\n\n\n\nBe nice\n\n\n\nIf your HPC system doesn’t have a job scheduler in place, we recommend using the nice command. This command allows you to adjust and manage the scheduling priority of your processes, giving you the ability to run tasks with lower priority when sharing resources with others. By using nice, you can ensure that your processes do not dominate the CPU, allowing other users’ tasks to run smoothly. This is particularly useful in environments where multiple users are working on the same system without a job scheduler to automatically manage resource allocation.\n\n\n\n\n\n\n\n\nHPC\n\n\n\n\n\n\n\n\nDescribe how a typical HPC is organised: nodes, job scheduler and filesystem.\nWhat are the roles of a login node and a compute node? how do they differ?\nDescribe the role of a job scheduler\nWhat are the differences between scratch and home storage and when each should be used?",
    "crumbs": [
      "HPC launch",
      "HPC systems",
      "HPC launch"
    ]
  },
  {
    "objectID": "develop/hpc_intro.html#sources",
    "href": "develop/hpc_intro.html#sources",
    "title": "HPC launch",
    "section": "Sources",
    "text": "Sources\nUseful links\n\nAcknowledgements",
    "crumbs": [
      "HPC launch",
      "HPC systems",
      "HPC launch"
    ]
  },
  {
    "objectID": "develop/fair_workflow.html",
    "href": "develop/fair_workflow.html",
    "title": "FAIR computational pipelines",
    "section": "",
    "text": "Course Overview\n\n\n\n\n⏰ Total Time Estimation: X hours\n\n📁 Supporting Materials:\n\n👨‍💻 Target Audience: Ph.D., MSc, anyone interested in workflow management systems for High-Throughput data or other related fields within bioinformatics.\n👩‍🎓 Level: Advanced.\n🔒 License: Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license.\n\n💰 Funding: This project was funded by the Novo Nordisk Fonden (NNF20OC0063268).\nData analysis typically involves the use of different tools, algorithms, and scripts. It often requires multiple steps to transform, filter, aggregate, and visualize data. The process can be time-consuming because each tool may demand specific inputs and parameter settings. As analyses become more complex, the importance of reproducible and scalable automated workflow management increases since the key for reproducibility is automation.",
    "crumbs": [
      "HPC pipes",
      "Pipeline languages",
      "FAIR computational pipelines"
    ]
  },
  {
    "objectID": "develop/fair_workflow.html#good-practices",
    "href": "develop/fair_workflow.html#good-practices",
    "title": "FAIR computational pipelines",
    "section": "Good practices",
    "text": "Good practices\nIf you develop your own software make sure you follow FAIR principles. We highly endorse following these FAIR recommendations.\n\nRemember to create portable code and use relative paths to ensure transferability between users.\nUse git repositories to save your projects and pipelines.\nRegister and publish your scientific computational workflow on WorkflowHub.",
    "crumbs": [
      "HPC pipes",
      "Pipeline languages",
      "FAIR computational pipelines"
    ]
  },
  {
    "objectID": "develop/fair_workflow.html#sources",
    "href": "develop/fair_workflow.html#sources",
    "title": "FAIR computational pipelines",
    "section": "Sources",
    "text": "Sources\n\nSnakemake tutorial\nSnakemake tutorial slides by Johannes Koster\nhttps://bioconda.github.io\nKöster, Johannes and Rahmann, Sven. “Snakemake - A scalable bioinformatics workflow engine”. Bioinformatics 2012.\nKöster, Johannes. “Parallelization, Scalability, and Reproducibility in Next-Generation Sequencing Analysis”, PhD thesis, TU Dortmund 2014.\nfaircookbook worflows\nThe turing way - reproducible research",
    "crumbs": [
      "HPC pipes",
      "Pipeline languages",
      "FAIR computational pipelines"
    ]
  },
  {
    "objectID": "develop/scripts/01.iris.html",
    "href": "develop/scripts/01.iris.html",
    "title": "Notebook Iris Dataset",
    "section": "",
    "text": "Before we dive in, here’s a quick summary: the dataset contains 150 samples of iris flowers, each characterized by four features: Sepal Length, Sepal Width, Petal Length, and Petal Width, all measured in centimeters. These samples are grouped into three species: Setosa, Versicolor, and Virginica. If you’re not familiar with the dataset, you can learn more about it here."
  },
  {
    "objectID": "develop/scripts/01.iris.html#loading-the-dataset",
    "href": "develop/scripts/01.iris.html#loading-the-dataset",
    "title": "Notebook Iris Dataset",
    "section": "1 Loading the dataset",
    "text": "1 Loading the dataset\nLet’s start by importing the iris dataset and manipulating the dataframe so that the column names match the feature names.\n\nfrom sklearn.datasets import load_iris\nimport matplotlib.pyplot as plt\nimport pandas as pd \n\n# Load toy dataset\niris = load_iris() \n # Create dataframe using feature names\ndf = pd.DataFrame(iris.data, columns=iris.feature_names)"
  },
  {
    "objectID": "develop/scripts/01.iris.html#exploring-the-dataset",
    "href": "develop/scripts/01.iris.html#exploring-the-dataset",
    "title": "Notebook Iris Dataset",
    "section": "2 Exploring the dataset",
    "text": "2 Exploring the dataset\nLet´s start by exploring the species by plotting the sepal length vs. width in a scatter plot\n\nimport matplotlib.pyplot as plt\n\n_, ax = plt.subplots()\nscatter = ax.scatter(iris.data[:, 0], iris.data[:, 1], c=iris.target)\nax.set(xlabel=iris.feature_names[0], ylabel=iris.feature_names[1])\n_ = ax.legend(\n    scatter.legend_elements()[0], iris.target_names, loc=\"lower right\", title=\"Classes\"\n)\n\n\n\n\n\n\n\n\nYou can already see a pattern regarding the Setosa type, which is easily identifiable based on its short and wide sepal. Only considering these 2 dimensions, sepal width and length, there’s still overlap between the Versicolor and Virginica types."
  },
  {
    "objectID": "develop/scripts/01.iris.html#transforming-the-dataset",
    "href": "develop/scripts/01.iris.html#transforming-the-dataset",
    "title": "Notebook Iris Dataset",
    "section": "3 Transforming the dataset",
    "text": "3 Transforming the dataset\nWe will now perform feature engineering and create a new feature called petal area (petal length * petal width), and will do the same for the sepal.\n\ndf['sepal_area'] = df['sepal length (cm)'] * df['sepal width (cm)']\ndf['petal_area'] = df['petal length (cm)'] * df['petal width (cm)']\n\nFinally, let’s new information by binning the sepal length into 3 categories (short, medium and long)\n\ndf['sepal_length_bin'] = pd.cut(df['sepal length (cm)'], bins=3, labels=[\"short\", \"medium\", \"long\"])\ndf\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\nsepal_area\npetal_area\nsepal_length_bin\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n17.85\n0.28\nshort\n\n\n1\n4.9\n3.0\n1.4\n0.2\n14.70\n0.28\nshort\n\n\n2\n4.7\n3.2\n1.3\n0.2\n15.04\n0.26\nshort\n\n\n3\n4.6\n3.1\n1.5\n0.2\n14.26\n0.30\nshort\n\n\n4\n5.0\n3.6\n1.4\n0.2\n18.00\n0.28\nshort\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\n20.10\n11.96\nmedium\n\n\n146\n6.3\n2.5\n5.0\n1.9\n15.75\n9.50\nmedium\n\n\n147\n6.5\n3.0\n5.2\n2.0\n19.50\n10.40\nmedium\n\n\n148\n6.2\n3.4\n5.4\n2.3\n21.08\n12.42\nmedium\n\n\n149\n5.9\n3.0\n5.1\n1.8\n17.70\n9.18\nmedium\n\n\n\n\n150 rows × 7 columns"
  },
  {
    "objectID": "develop/scripts/01.iris.html#computing-summary-statistics",
    "href": "develop/scripts/01.iris.html#computing-summary-statistics",
    "title": "Notebook Iris Dataset",
    "section": "4 Computing summary statistics",
    "text": "4 Computing summary statistics\nNow, we can extract summary statistics of the species “setosa” and compare it to another species\n\n# Map targets to species names and add them to a new column \ndf['species'] = iris.target_names[iris.target] \n# Display first few rows\ndf.head() \n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\nsepal_area\npetal_area\nsepal_length_bin\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n17.85\n0.28\nshort\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\n14.70\n0.28\nshort\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\n15.04\n0.26\nshort\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\n14.26\n0.30\nshort\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\n18.00\n0.28\nshort\nsetosa\n\n\n\n\n\n\n\n\n# Select setosa \ndf_setosa = df[df['species'] == \"setosa\"]\nsummary_stats = df_setosa.describe() \n\n# Display summary statistics\nprint(summary_stats)\n\n       sepal length (cm)  sepal width (cm)  petal length (cm)  \\\ncount           50.00000         50.000000          50.000000   \nmean             5.00600          3.428000           1.462000   \nstd              0.35249          0.379064           0.173664   \nmin              4.30000          2.300000           1.000000   \n25%              4.80000          3.200000           1.400000   \n50%              5.00000          3.400000           1.500000   \n75%              5.20000          3.675000           1.575000   \nmax              5.80000          4.400000           1.900000   \n\n       petal width (cm)  sepal_area  petal_area  \ncount         50.000000   50.000000   50.000000  \nmean           0.246000   17.257800    0.365600  \nstd            0.105386    2.933775    0.181155  \nmin            0.100000   10.350000    0.110000  \n25%            0.200000   15.040000    0.280000  \n50%            0.200000   17.170000    0.300000  \n75%            0.300000   19.155000    0.420000  \nmax            0.600000   25.080000    0.960000"
  },
  {
    "objectID": "develop/nextflow.html",
    "href": "develop/nextflow.html",
    "title": "Nextflow",
    "section": "",
    "text": "Nextflow is a workfow management system that offers scalable and portable NGS data analysis pipelines, facilitating data processing across diverse computing environments. It streamlines and automates various data analysis steps, enabling parallel processing and seamless integration with existing tools. ç",
    "crumbs": [
      "HPC pipes",
      "Pipeline languages",
      "Nextflow"
    ]
  },
  {
    "objectID": "develop/nextflow.html#nf-core",
    "href": "develop/nextflow.html#nf-core",
    "title": "Nextflow",
    "section": "nf-core",
    "text": "nf-core\nTemplates",
    "crumbs": [
      "HPC pipes",
      "Pipeline languages",
      "Nextflow"
    ]
  },
  {
    "objectID": "develop/nextflow.html#sources",
    "href": "develop/nextflow.html#sources",
    "title": "Nextflow",
    "section": "Sources",
    "text": "Sources\n\nfaircookbook worflows",
    "crumbs": [
      "HPC pipes",
      "Pipeline languages",
      "Nextflow"
    ]
  },
  {
    "objectID": "develop/exercise_launch.html",
    "href": "develop/exercise_launch.html",
    "title": "Exercises",
    "section": "",
    "text": "Put your learning to the test with what you’ve covered so far.",
    "crumbs": [
      "HPC launch",
      "Exercises"
    ]
  },
  {
    "objectID": "develop/exercise_launch.html#general-hpc-launch",
    "href": "develop/exercise_launch.html#general-hpc-launch",
    "title": "Exercises",
    "section": "General HPC launch",
    "text": "General HPC launch\n1. Should I run the commmand unzip myfile.zip to decompress the file from the login node? TRUEFALSE\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nAvoid running anything from the login node. Create an interactive session to do this!\n\n\n\n\n\n2. Is it a good idea to keep data in the scratch folder until the project is finished? TRUEFALSE",
    "crumbs": [
      "HPC launch",
      "Exercises"
    ]
  },
  {
    "objectID": "develop/jobs.html",
    "href": "develop/jobs.html",
    "title": "HPC jobs",
    "section": "",
    "text": "Note\n\n\n\nThere are many job scheduler programs available but a very common one is SLURM. Useful commands:\n# Submit the job\nsbatch \n# Check the job is in the queue\nsqueue \nToy example of a bash script to submit\n#!/bin/bash\n#SBATCH -D /home/USERNAME/  # working directory\n#SBATCH -c 1        # number of CPUs. Default: 1\n#SBATCH -t 00:10:00 # time for the job HH:MM:SS.\n#SBATCH --mem=1G    # RAM memory\n\n# my commands: software, pipeline, etc.\nsnakemake -j1\n\n\n\n\n\nJob parallelization is crucial for achieving high performance and running jobs effectively on an HPC. Here are two key scenarios where it is particularly important:\n\nIndependent computational tasks: When tasks are independent of each other, job parallelization can enhance efficiency by allowing them to run concurrently.\nMulti-threaded tools: Some tools are specifically designed to perform parallel computations through multi-threading, enabling them to utilize multiple CPU cores for increased performance.\n\n\n\n\n\n\n\nJob parallelisation using slurm\n\n\n\nJobs arrays -a",
    "crumbs": [
      "HPC launch",
      "HPC systems",
      "HPC jobs"
    ]
  },
  {
    "objectID": "develop/jobs.html#best-practices-for-running-a-job-on-a-cluster",
    "href": "develop/jobs.html#best-practices-for-running-a-job-on-a-cluster",
    "title": "HPC jobs",
    "section": "",
    "text": "Note\n\n\n\nThere are many job scheduler programs available but a very common one is SLURM. Useful commands:\n# Submit the job\nsbatch \n# Check the job is in the queue\nsqueue \nToy example of a bash script to submit\n#!/bin/bash\n#SBATCH -D /home/USERNAME/  # working directory\n#SBATCH -c 1        # number of CPUs. Default: 1\n#SBATCH -t 00:10:00 # time for the job HH:MM:SS.\n#SBATCH --mem=1G    # RAM memory\n\n# my commands: software, pipeline, etc.\nsnakemake -j1\n\n\n\n\n\nJob parallelization is crucial for achieving high performance and running jobs effectively on an HPC. Here are two key scenarios where it is particularly important:\n\nIndependent computational tasks: When tasks are independent of each other, job parallelization can enhance efficiency by allowing them to run concurrently.\nMulti-threaded tools: Some tools are specifically designed to perform parallel computations through multi-threading, enabling them to utilize multiple CPU cores for increased performance.\n\n\n\n\n\n\n\nJob parallelisation using slurm\n\n\n\nJobs arrays -a",
    "crumbs": [
      "HPC launch",
      "HPC systems",
      "HPC jobs"
    ]
  },
  {
    "objectID": "develop/jobs.html#sources",
    "href": "develop/jobs.html#sources",
    "title": "HPC jobs",
    "section": "Sources",
    "text": "Sources\nUseful links\n\nAcknowledgements",
    "crumbs": [
      "HPC launch",
      "HPC systems",
      "HPC jobs"
    ]
  }
]
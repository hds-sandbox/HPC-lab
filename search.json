[
  {
    "objectID": "develop/e1-fair_envs.html",
    "href": "develop/e1-fair_envs.html",
    "title": "FAIR environments",
    "section": "",
    "text": "Understanding the importance of computational environments is crucial for ensuring the consistency and reliability of research outcomes. These environments can vary significantly between systems, including different in operating systems, installed software, and software package versions. When a research project is transferred to a different computer or platform, analyses may fail to run or yield inconsistent results, particularly if the software depends on specific configurations or libraries. Dependencies can evolve over time or lack proper documentation, creating hidden variations between setups. Consequently, merely knowing a software version may not guarantee consistent performance across different environments, highlighting the need for robust management strategies.\nTo address these challenges, project and package managers offer valuable solutions for organizing software in isolated environments. For research to be reproducible, the original computational environment must also be recorded so others can replicate it accurately. This involves making your code easy to install and run by others, document the setup process thoroughly, and carefully manage and share your software environment.\nThere are several methods to achieve this:\nWhile package managers are very easy to use and share across different systems, and are lightweight and efficient, offering fast start-up times, containers provide an even greater level of isolation by virtualizing the entire operating system, making it possible to deploy applications seamlessly across various machines without requiring additional installations. Unlike Virtual Machines, which also virtualize hardware, containers encapsulate applications and their dependencies, ensuring that they function uniformly regardless of the underlying infrastructure. This approach enhances reproducibility and streamlines the deployment process, making it a powerful tool for researchers seeking to maintain consistent results in diverse computing environments.\nRecording and sharing the computational environment is essential for ensuring reproducibility and transparency. Below, we will explore two tools that can help with this: mamba, a package manager, and Docker, a container system. We will explain the differences between them and provide guidance on choosing the right tool for your specific scenario.",
    "crumbs": [
      "HPC Pipes",
      "Computational environments",
      "FAIR environments"
    ]
  },
  {
    "objectID": "develop/e1-fair_envs.html#package-managers",
    "href": "develop/e1-fair_envs.html#package-managers",
    "title": "FAIR environments",
    "section": "Package managers",
    "text": "Package managers\nWhen coding, it’s crucial to ensure that projects are developed under consistent software conditions. The packages and libraries (dependencies) used during development should remain unchanged throughout the project to prevent issues like variations in output formats or conflicts from new algorithm implementations, which can be difficult to trace. Environment and package managers allow users to create isolated frameworks (environments) where specific packages can be installed without affecting other software outside the environment. For even greater isolation, containers can be used (see the related section on this page).\n\n\n\nLink\nDescription\n\n\n\n\nConda\nA widely-used and user-friendly environment manager\n\n\nGetting started with conda\nOfficial guide to setting up and using conda\n\n\nConda cheat sheet\nQuick reference for conda usage\n\n\nYARN\nAn alternative to conda\n\n\n\nMamba is a reimplementation of the Conda package manager in C++. While our focus will be on Mamba, it’s important to note that it maintains compatibility with Conda by using the same command-line parser, package installation and uninstallation code, and transaction verification routines.\nMamba uses software installation specifications that are maintained by extensive communities of developers, organized into channels, which serve as software repositories. For example, the “bioconda” channel specializes in bioinformatics tools, while “conda-forge” covers a broad range of data science packages.\n\n\n\n\n\n\nMamba vs. conda\n\n\n\nAs previously mentioned, mamba is a newer and faster implementation. The two commands can be used interchangeable (for most tasks). If you use Conda, you should still complete the exercises, as you’ll gain experience with both tools. For more information on their ecosystem and advantages here.\n\n\nMamba allows you to create different software environments, where multiple package version can co-exit on your system.\n\n\n\n\n\n\nBuild your mamba environment\n\n\n\n\n\n\n\nFollow mamba instructions to install it. Let’s also include bioconda and conda-forge channels which will come very handy.\nconda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge\nNow you are set to create your first environment. Follow these steps:\n\nCreate a new environment named myenv\nInstall the following packages in myenv: bowtie2, numpy=1.26.4, matplotlib=3.8.3\nCheck the environments available\nLoad/activate the environment\nCheck which python executable is being used and that bowtie2 is installed.\nDeactivate the environment\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nHere are some of the commands you need for the exercise.\n# use conda or mamba commands \nmamba create -n &lt;ENV-NAME&gt;\nmamba install --channel &lt;CHANNEL-NAME&gt; --name &lt;ENV-NAME&gt;\nmamba env list\n# mamba init \nmamba activate &lt;ENV-NAME&gt;\nmamba deactivate \n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nThe syntax to create a new environment is: mamba create --name myenv\nExample “bowtie2”: Go to anaconda.org and search for “bowtie2” to confirm it is available through Mamba and which software channel it is provided from. You will find that it is available via the “bioconda” channel: https://anaconda.org/bioconda/bowtie2. The syntax to install packages is: mamba install --channel &lt;CHANNEL-NAME&gt; --name &lt;ENV-NAME&gt; &lt;SOFTWARE-NAME&gt;\n\nmamba install --name myenv --channel bioconda bowtie2=2.5.3 \"matplotlib=3.8.3\" \"numpy=1.26.4\"\nDo the same with the others.\n\nTo see al environments available mamba env list. There will be a “*” showing the one is activated.\nLoad the environment mamba activate myenv.\nwhich python -&gt; should print the one in the environment that is active (path similar to /home/mambaforge/envs/myenv/bin/python). bowtie2 --help\nConda deactivate\n\n\n\n\n\n\n\n\n\n\n\nIf you have different environments set up for various projects, you can switch between them or run commands directly within a specific environment using:\nmamba run -n &lt;ENV-NAME&gt; python myscript.py\n\n\n\n\n\n\nLoading mamba environments in shell scripts\n\n\n\nIf you need to activate an environment in a shell script that will be submitted to SLURM, you must first source Mamba’s configuration file. For instance, to load the myenv environment we created, the script would include the following code:\n# Always add these two commands to your scripts\neval \"$(conda shell.bash hook)\"\nsource $CONDA_PREFIX/etc/profile.d/mamba.sh\n\n# then you can activate the environment\nmamba activate myenv\nWhen jobs are submitted to SLURM, they run in a non-interactive shell where Mamba isn’t automatically set up. By running the source command, you ensure that Mamba’s activate function is available. It’s important to remember that even if the environment is loaded on the login node, the scripts will execute on a different machine (one of the compute nodes). Therefore, always include the command to load the Mamba environment in your SLURM submission scripts.\n\n\n\nBase environment\nIt is the primary environment that contains the conda package manager itself. It is activated by default unless indicated.\nYour command prompt will show which env is activated within parentheses.\n(base) [username@node-01 ~]$\nTo enhance package installation performance, we will update conda to utilize the libmamba solver (check documentation here), which significantly improves the speed of installing multiple packages concurrently.\n# Installation \nconda install -n base --yes conda-libmamba-solver\n# configuration \nconda config --set solver libmamba \n\n\n\n\n\n\nWarning\n\n\n\nAvoid modifications to the base environment. This is the only instance where you should perform installations within the base environment, as any further changes could jeopardize the integrity of the conda installation.\n\n\nA common practice is to disable the automatic activation of the base environment. There are several reasons but it helps creating a more organized and efficient workflow (e.g. activation time), reduce potential errors (e.g. unintended modifications), and maintain better control over your development environment.\nconda config --set auto_activate_base false",
    "crumbs": [
      "HPC Pipes",
      "Computational environments",
      "FAIR environments"
    ]
  },
  {
    "objectID": "develop/e1-fair_envs.html#containers",
    "href": "develop/e1-fair_envs.html#containers",
    "title": "FAIR environments",
    "section": "Containers",
    "text": "Containers\nEssentially, a container is a self-contained, lightweight package that includes everything needed to run a specific application—such as the operating system, libraries, and the application code itself. Containers operate independently of the host system, which allows them to run the same software across various environments without any conflicts or interference. This isolation ensures that researchers can consistently execute their code on different systems and platforms, without worrying about dependency issues or conflicts with other software on the host machine.\n\n\n\nLink\nDescription\n\n\n\n\nDocker\nAn open source widespread container that is popular both in research and industry\n\n\nDocker course\nA course to use Docker, freely hosted on youtube\n\n\nDocker curriculum\nBeginner introduction to docker\n\n\nDocker basics\nIntroduction tutorials to Docker from the official documentation page\n\n\nSingularity\nSingularity is another containerization tool. It allows you to decide at which degree a container interacts with the hosting system\n\n\nSingularity tutorial\nA well done Singularity tutorial for HPC users\n\n\nSingularity video tutorial\nA video tutorial on Singularity\n\n\nReproducibility by containerization\nA video on reproducibility with Singularity containers\n\n\n\n\n\n\n\n\n\nDocker vs. Singularity\n\n\n\nThe most significant difference is at the permission level required to run them. Docker containers operate as root by default, giving them full access to the host system. While this can be useful in certain situations, it also poses security risks, especially in multi-user environments. In contrast, Singularity containers run as non-root users by default, enhancing security and preventing unauthorized access to the host system.\n\nDocker is ideal for building and distributing software across different operating systems\nSingularity is designed for HPC environments and offers high performance without needing root access\n\n\n\nIn the following sections, we’ll cover how to retrieve environment information, utilize containers, and automate environment setup to improve reproducibility.\n\nSingularity on a remote server\nWhile you can build your own Singularity images, many popular software packages already have pre-built images available from public repositories. The two repositories you’ll most likely use or hear about are:\n\ndepot.galaxyproject.org\nContainer Library (Sylabs)\nDocker Hub\n\n\n\n\n\n\n\nInstallation\n\n\n\n\nSingularity installation guides\n\n# You will only need to vagrant init once \nexport VM=sylabs/singularity-3.0-ubuntu-bionic64 && \\\n    vagrant init $VM && \\\n    vagrant up && \\\n    vagrant ssh\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nWe recommend using the pre-installed version provided by your system administrators if you’re working on a shared system. If you’re working on your own computer, you can install the necessary software using Mamba.\nThey might host different versions of the same software, so it’s worth checking both to find the version you need.\nTo download a software container from public repositories, use the singularity pull command.\nTo execute a command within the software container, use the singularity run command.\nGood practice: create a directory to save all singularity images together. .sif is the standard extension for the images.\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nDownload a singularity image from one of the two repositories listed above (choose a software like bcftools, bedtools, bowtie2, seqkit…) and run the --help command. This command displays the help documentation of the program, verifying that our image is functioning correctly and includes the intended software.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n# create a directory for our singularity images\nmkdir images\n\n# download the image\nsingularity pull images/bowtie2-2.5.4.sif https://depot.galaxyproject.org/singularity/bowtie2%3A2.5.4--he20e202_2\n\n# run the image: singularity run &lt;PATH-TO-IMAGE&gt; &lt;YOUR COMMANDS&gt;\nsingularity run images/bowtie2-2.5.4.sif bowtie2 --help",
    "crumbs": [
      "HPC Pipes",
      "Computational environments",
      "FAIR environments"
    ]
  },
  {
    "objectID": "develop/e1-fair_envs.html#sources",
    "href": "develop/e1-fair_envs.html#sources",
    "title": "FAIR environments",
    "section": "Sources",
    "text": "Sources\n\nAnaconda for searching Mamba/conda packages\nBioconda for installing software package related to biomedical research\nConda cheat sheet\nfaircookbook worflows\nConda blog freecodecamp\nDocker\nDocker get-started\n\nFind pre-built singularity images:\n\ndepot.galaxyproject.org\nSylabs\n\nOther training resources:\n\nThe turing way - reproducible research\nHPC intro by Cambridge\nHighly recommend Reproducible Research II: Practices and Tools for Managing Computations and Data by members of France Université Numérique.",
    "crumbs": [
      "HPC Pipes",
      "Computational environments",
      "FAIR environments"
    ]
  },
  {
    "objectID": "develop/hpc-ssh_keys.html",
    "href": "develop/hpc-ssh_keys.html",
    "title": "HPC Lab",
    "section": "",
    "text": "Using SSH keys for authentication is recommended for accessing servers and version control platforms such as GitLab, GitHub, or BitBucket. SSH keys provide a more convenient and secure method than passwords, allowing you to avoid typing a password every time you log in.\nThis tutorial guides you through configuring SSH keys, using GitHub as an example.\n\n\nAn SSH (Secure Shell) is a network protocol that allows you to execute commands on a remote server securely. SSH keys are cryptographic keys used for authenticating a user or device within the SSH protocol. They come in pairs: the “private” and the “public” key.\n\nPublic Key: This key can be freely shared with anyone and is added to the ~/.ssh/authorized_keys file on the remote server you wish to access.\nPrivate Key: This key should remain on your local machine and be carefully protested, similar to a password. It should never be shared.\n\nThe holder of the private key can authenticate with any remote system that has the corresponding public key. For example, you can use a single public/private key pair to authenticate with the cluster at your research center and your GitHub account.\n\n\n\n\n\n\nHow SSH keys work\n\n\n\nThink of the public key as a complex number x, and the private key as its inverse 1/x. Encrypting a message m with your public key is like multiplying your message by x, making mx completely unreadable. Using the private key, you can decrypt m by multiplying mx by 1/x. This process is secure because you cannot derive 1/x from x. Therefore, you can safely distribute the public key to the machines you want to access while keeping the private key on your machine.\n\n\n\n\n\n\n\nOn your (Mac or Linux) workstation, create the public/private key pair. There are several types of keys and by default you will get an RSA pair. When prompted for a passphrase, just hit enter so that you won’t need a password when logging in.\nWhen prompted, press Enter to store the keys in .ssh/id_rsa (we recommend not to modify this). Press Enter twice again to skip setting a passphrase, which otherwise would require you to enter it each time (e.g. if you set up SSH keys for authentification in GitHub, you will need to enter the passprhase every time you use git push or git pull). This process generates two files: the public key (~/.ssh/id_rsa.pub) and the private key (~/.ssh/id_rsa).\nssh-keygen\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/Users/abc123/.ssh/id_rsa): \nCreated directory '/Users/abc123/.ssh'.\nEnter passphrase (empty for no passphrase): \nEnter same passphrase again: \nYour identification has been saved in /Users/abc123/.ssh/id_rsa\nYour public key has been saved in /Users/abc123/.ssh/id_rsa.pub\nThe key fingerprint is:\nSHA256:w8JIQXafdq6scMiNZefNd8lVz9UmIzm0eiO+pvGOM6A abc123@c9b1d769db6e\nThe key's randomart image is:\n+---[RSA 3072]----+\n|   .+ .     .    |\n|   . o . . . o  .|\n|    .   + . = o =|\n|   . o o o . o =+|\n|    . = S + o   +|\n|   . *.= * o o o |\n|    =.o.= + . +  |\n|    Eo .o+.o .   |\n|      . o*+      |\n+----[SHA256]-----+\ncat `~/.ssh/id_rsa.pub\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCs3nqhRr7FyB8UKxqQ5mA3Gk60oty5lGdzcpVxjL08QmjLz15X6Odo2EotmJQuzCyYbzPw4Tq404N3NOmVqYywrn03i3kmCnZd5MYq8yi72Zsk9qFfgEt7pxpf5WJ+AWSDXwUkUQK2FFby2RWbi4gwTmRHfkB999j8F3F/d5EpB0llQfxmZWIIrX9ampgxfI8+yTIyeG8gpx5028h1oR5qukEFJk6nAoj5on+/vBqHdc0AVPd6jNoNCDZ6ur4xwUPam8iQ6A4NfbnCVUPa95Wqt+QWEF6Mn4UB1WIvly+blDzg5rVXvd9B685bZjQWfUsxSiRPkhtV6X2yImklt+KjV0ufB/Yl1O+x6S8r2+I41WSxwJNr5h5L588l3GH4ehT79uCtrxiu4zas+4s9OW02Ox3auCfLi1/jp/CJ5IsuLwM/jPBXrgzw+HREKM2gtm9d12gU2b9o9bmASZVhEKqeTb0aRPjcTYYdnjOWUl+pqkdVPyB7mJs8NAQemN/shvM= abc123@c9b1d769db6e\nThe public key can be safely displayed cat and shared with any machine where it will function as a personal lock that can be only opened with your private key.\n\n\n\n\n\n\nTip\n\n\n\nTip 1: If you provided a passphrase when generating your keys, you would need to enter this passphrase each time you use your private key. The passphrase adds an extra layer of security in case your private key is compromised. For simplicity, we not always recommend using a passphrase unless you are working in environments where people not part of the project might have access.\nTip 2: DSA keys should be avoided. An RSA key with at least 2048 bits should provide sufficient security. Finally, ecdsa and ed25519 keys are also fine (don’t worry about the number of bits for these keys).\n# Recommended\nssh-keygen -t rsa -b 4096\n# Alternative\nssh-keygen -t ed25519\n\n\nUsing an existing public/private key pair\nIf you already have an SSH key pair, you can use it without generating a new one. To check the details of your existing key, use the ssh-keygen command to display the key’s fingerprint.\nssh-keygen -l -f ~/.ssh/id_rsa\n2048 SHA256:7l0HauYJVRaQhuzmti8XEZImnRbzipu3NKGnE6tDFRk grg@t430s (RSA)\nAlternatively, check the content of the ssh directory:\n\nMac/Unix: ls -la ~/.ssh/\nWindows: ls ~/.ssh/\n\n\n\n\nFollow the steps below if you are using Mac or Linux.\n\n\n\n\n\n\nWarning\n\n\n\n\nWindows users: follow these instructions instead, the process is very similar.\nIf you are getting errors when running the commands below, please read carefully these guidelines as GitHub provides instructions on what to do with common issues users run into for this specific step 2.\nGeneral GitHUbtroubleshooting\n\n\n\n# ONLY if you need to start the ssh agent. First, verify it is not already running: \n# pgrep -u $USER ssh-agent\n# If that command returns a process ID, your SSH agent is running! If not, run the command below: \n# eval \"$(ssh-agent -s)\"\n\n# Manually modify the `~/.ssh/config` file to automatically load keys into ssh-agent, like in the example below. For GitHub, the User is **git**, do not write your own. \n\nHost github.com\n  AddKeysToAgent yes\n  IdentityFile ~/.ssh/id_rsa\n\n# Note: if the file does not exit, run `touch ~/.ssh/config` to create it. \n\n# Add the SSH private to the ssh-agent\nssh-add ~/.ssh/id_rsa\n\n# Optional, verify the keys are loaded\nssh-add -l\n\n\n\n\n\n\nOther tips\n\n\n\n\n~/.ssh/config: add several hosts (servers, GitLab, GitHUb, etc.).\n\nEdit the SSH config file Create and use a ~/.ssh/config file on your workstation. E.g., using a password-less ssh key for authentication and the following ~/.ssh/config, user abc123 can login with the ssh xyz command.\nHost xyz\n    Hostname ssh-myserver.dk\n    User abc123\n    ServerAliveInterval 60\n    ForwardX11 yes\n    ForwardX11Trusted yes\n\nUsing paraphrases: store paraphrases in your keychain if you are using one. Please, follow the instructions here.\n\n\n\nStep 3: Transferring the key\nA. Transfer the key to GitHub/GitLab\nCopy your SSH public key ~/.ssh/id_rsa.pub with your favorite editor, go to your GitHub profile (top left side) &gt; Settings &gt; Access &gt; SSH and GPG keys &gt; New/Add SSH key. You just need to fill up the title (e.g.: Personal laptop) and paste the copied key (e.g. ssh-rsa or ecdsa-xxx).\n# Mac/Linux: copy key \ncat ~/.ssh/id_rsa.pub | pbcopy\n# Windows\ncat ~/.ssh/id_rsa.pub | clip\n\n\n\n\n\n\nGitHub Guidelines Links\n\n\n\n\n\n\nGeneral\nLast step, key transfer\n\n\n\n\nB: Transfer the key to a remote server\nTransfer the public part of your key pair to the remote server. This can be done manually, but the easiest way is using ssh-copy-id:\nssh-copy-id -i ~/.ssh/id_rsa.pub &lt;userid&gt;@ssh-myserver.dk\nThis will prompt for a password to authenticate with the remote server in order to copy the file. Once successful though, future logins will use the key pair for authentication, and will not prompt for a password (assuming you login from your workstation, which has the private part of your key pair).\n\n\n\n\n\n\nNote\n\n\n\nssh-copy-id will blindly append the given key file to the remote ~/.ssh/authorized_keys file. If you made a mistake or copied the same key multiple times, you may want to edit this file! Each line in the file corresponds to one key.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nYou are set to start using your SSH key. If you are using UCloud, follow the next section; otherwise, you are done with this tutorial.",
    "crumbs": [
      "HPC Launch",
      "Data management",
      "SSH keys"
    ]
  },
  {
    "objectID": "develop/hpc-ssh_keys.html#ssh-keys",
    "href": "develop/hpc-ssh_keys.html#ssh-keys",
    "title": "HPC Lab",
    "section": "",
    "text": "Using SSH keys for authentication is recommended for accessing servers and version control platforms such as GitLab, GitHub, or BitBucket. SSH keys provide a more convenient and secure method than passwords, allowing you to avoid typing a password every time you log in.\nThis tutorial guides you through configuring SSH keys, using GitHub as an example.\n\n\nAn SSH (Secure Shell) is a network protocol that allows you to execute commands on a remote server securely. SSH keys are cryptographic keys used for authenticating a user or device within the SSH protocol. They come in pairs: the “private” and the “public” key.\n\nPublic Key: This key can be freely shared with anyone and is added to the ~/.ssh/authorized_keys file on the remote server you wish to access.\nPrivate Key: This key should remain on your local machine and be carefully protested, similar to a password. It should never be shared.\n\nThe holder of the private key can authenticate with any remote system that has the corresponding public key. For example, you can use a single public/private key pair to authenticate with the cluster at your research center and your GitHub account.\n\n\n\n\n\n\nHow SSH keys work\n\n\n\nThink of the public key as a complex number x, and the private key as its inverse 1/x. Encrypting a message m with your public key is like multiplying your message by x, making mx completely unreadable. Using the private key, you can decrypt m by multiplying mx by 1/x. This process is secure because you cannot derive 1/x from x. Therefore, you can safely distribute the public key to the machines you want to access while keeping the private key on your machine.\n\n\n\n\n\n\n\nOn your (Mac or Linux) workstation, create the public/private key pair. There are several types of keys and by default you will get an RSA pair. When prompted for a passphrase, just hit enter so that you won’t need a password when logging in.\nWhen prompted, press Enter to store the keys in .ssh/id_rsa (we recommend not to modify this). Press Enter twice again to skip setting a passphrase, which otherwise would require you to enter it each time (e.g. if you set up SSH keys for authentification in GitHub, you will need to enter the passprhase every time you use git push or git pull). This process generates two files: the public key (~/.ssh/id_rsa.pub) and the private key (~/.ssh/id_rsa).\nssh-keygen\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/Users/abc123/.ssh/id_rsa): \nCreated directory '/Users/abc123/.ssh'.\nEnter passphrase (empty for no passphrase): \nEnter same passphrase again: \nYour identification has been saved in /Users/abc123/.ssh/id_rsa\nYour public key has been saved in /Users/abc123/.ssh/id_rsa.pub\nThe key fingerprint is:\nSHA256:w8JIQXafdq6scMiNZefNd8lVz9UmIzm0eiO+pvGOM6A abc123@c9b1d769db6e\nThe key's randomart image is:\n+---[RSA 3072]----+\n|   .+ .     .    |\n|   . o . . . o  .|\n|    .   + . = o =|\n|   . o o o . o =+|\n|    . = S + o   +|\n|   . *.= * o o o |\n|    =.o.= + . +  |\n|    Eo .o+.o .   |\n|      . o*+      |\n+----[SHA256]-----+\ncat `~/.ssh/id_rsa.pub\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCs3nqhRr7FyB8UKxqQ5mA3Gk60oty5lGdzcpVxjL08QmjLz15X6Odo2EotmJQuzCyYbzPw4Tq404N3NOmVqYywrn03i3kmCnZd5MYq8yi72Zsk9qFfgEt7pxpf5WJ+AWSDXwUkUQK2FFby2RWbi4gwTmRHfkB999j8F3F/d5EpB0llQfxmZWIIrX9ampgxfI8+yTIyeG8gpx5028h1oR5qukEFJk6nAoj5on+/vBqHdc0AVPd6jNoNCDZ6ur4xwUPam8iQ6A4NfbnCVUPa95Wqt+QWEF6Mn4UB1WIvly+blDzg5rVXvd9B685bZjQWfUsxSiRPkhtV6X2yImklt+KjV0ufB/Yl1O+x6S8r2+I41WSxwJNr5h5L588l3GH4ehT79uCtrxiu4zas+4s9OW02Ox3auCfLi1/jp/CJ5IsuLwM/jPBXrgzw+HREKM2gtm9d12gU2b9o9bmASZVhEKqeTb0aRPjcTYYdnjOWUl+pqkdVPyB7mJs8NAQemN/shvM= abc123@c9b1d769db6e\nThe public key can be safely displayed cat and shared with any machine where it will function as a personal lock that can be only opened with your private key.\n\n\n\n\n\n\nTip\n\n\n\nTip 1: If you provided a passphrase when generating your keys, you would need to enter this passphrase each time you use your private key. The passphrase adds an extra layer of security in case your private key is compromised. For simplicity, we not always recommend using a passphrase unless you are working in environments where people not part of the project might have access.\nTip 2: DSA keys should be avoided. An RSA key with at least 2048 bits should provide sufficient security. Finally, ecdsa and ed25519 keys are also fine (don’t worry about the number of bits for these keys).\n# Recommended\nssh-keygen -t rsa -b 4096\n# Alternative\nssh-keygen -t ed25519\n\n\nUsing an existing public/private key pair\nIf you already have an SSH key pair, you can use it without generating a new one. To check the details of your existing key, use the ssh-keygen command to display the key’s fingerprint.\nssh-keygen -l -f ~/.ssh/id_rsa\n2048 SHA256:7l0HauYJVRaQhuzmti8XEZImnRbzipu3NKGnE6tDFRk grg@t430s (RSA)\nAlternatively, check the content of the ssh directory:\n\nMac/Unix: ls -la ~/.ssh/\nWindows: ls ~/.ssh/\n\n\n\n\nFollow the steps below if you are using Mac or Linux.\n\n\n\n\n\n\nWarning\n\n\n\n\nWindows users: follow these instructions instead, the process is very similar.\nIf you are getting errors when running the commands below, please read carefully these guidelines as GitHub provides instructions on what to do with common issues users run into for this specific step 2.\nGeneral GitHUbtroubleshooting\n\n\n\n# ONLY if you need to start the ssh agent. First, verify it is not already running: \n# pgrep -u $USER ssh-agent\n# If that command returns a process ID, your SSH agent is running! If not, run the command below: \n# eval \"$(ssh-agent -s)\"\n\n# Manually modify the `~/.ssh/config` file to automatically load keys into ssh-agent, like in the example below. For GitHub, the User is **git**, do not write your own. \n\nHost github.com\n  AddKeysToAgent yes\n  IdentityFile ~/.ssh/id_rsa\n\n# Note: if the file does not exit, run `touch ~/.ssh/config` to create it. \n\n# Add the SSH private to the ssh-agent\nssh-add ~/.ssh/id_rsa\n\n# Optional, verify the keys are loaded\nssh-add -l\n\n\n\n\n\n\nOther tips\n\n\n\n\n~/.ssh/config: add several hosts (servers, GitLab, GitHUb, etc.).\n\nEdit the SSH config file Create and use a ~/.ssh/config file on your workstation. E.g., using a password-less ssh key for authentication and the following ~/.ssh/config, user abc123 can login with the ssh xyz command.\nHost xyz\n    Hostname ssh-myserver.dk\n    User abc123\n    ServerAliveInterval 60\n    ForwardX11 yes\n    ForwardX11Trusted yes\n\nUsing paraphrases: store paraphrases in your keychain if you are using one. Please, follow the instructions here.\n\n\n\nStep 3: Transferring the key\nA. Transfer the key to GitHub/GitLab\nCopy your SSH public key ~/.ssh/id_rsa.pub with your favorite editor, go to your GitHub profile (top left side) &gt; Settings &gt; Access &gt; SSH and GPG keys &gt; New/Add SSH key. You just need to fill up the title (e.g.: Personal laptop) and paste the copied key (e.g. ssh-rsa or ecdsa-xxx).\n# Mac/Linux: copy key \ncat ~/.ssh/id_rsa.pub | pbcopy\n# Windows\ncat ~/.ssh/id_rsa.pub | clip\n\n\n\n\n\n\nGitHub Guidelines Links\n\n\n\n\n\n\nGeneral\nLast step, key transfer\n\n\n\n\nB: Transfer the key to a remote server\nTransfer the public part of your key pair to the remote server. This can be done manually, but the easiest way is using ssh-copy-id:\nssh-copy-id -i ~/.ssh/id_rsa.pub &lt;userid&gt;@ssh-myserver.dk\nThis will prompt for a password to authenticate with the remote server in order to copy the file. Once successful though, future logins will use the key pair for authentication, and will not prompt for a password (assuming you login from your workstation, which has the private part of your key pair).\n\n\n\n\n\n\nNote\n\n\n\nssh-copy-id will blindly append the given key file to the remote ~/.ssh/authorized_keys file. If you made a mistake or copied the same key multiple times, you may want to edit this file! Each line in the file corresponds to one key.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nYou are set to start using your SSH key. If you are using UCloud, follow the next section; otherwise, you are done with this tutorial.",
    "crumbs": [
      "HPC Launch",
      "Data management",
      "SSH keys"
    ]
  },
  {
    "objectID": "develop/hpc-ssh_keys.html#ucloud-users",
    "href": "develop/hpc-ssh_keys.html#ucloud-users",
    "title": "HPC Lab",
    "section": "UCloud Users",
    "text": "UCloud Users\n\nSSH keys setup\nSimilarly to what is explained above, copy your key (manually or using e.g. pbcopy ~/.ssh/id_rsa.pub) and go to Resources (navigation panel on the left) &gt; SSH keys.\n\nPaste the public key here and give a title to your key as in the image below:\n\nWhen you have added the public part of the SSH key pair to your UCloud account, you can access jobs from your local computer if SSH access is possible for the given app and enabled upon job submission. Open Visual Studio Code to test this yourself! You will need to click on “Enable SSH Server” when submitting the job as follows:\n\n\n\nSelect “Enable SSH server”\n\n\nWhen a job with SSH access enabled starts, the command for accessing the job from your local computer via SSH is displayed from the job’s progress view.\n\n\n\nCopy ssh command\n\n\nNow, open a terminal and run the command:\n# the 4 digits will change everytime you start a new job\nssh ucloud@ssh.cloud.sdu.dk -p 2465\nIf it is the first time you do this, this message will prompt, write yes\nThe authenticity of host '[ssh.cloud.sdu.dk]:2465 ([130.225.164.106]:2465)' can't be established.\nED25519 key fingerprint is SHA256:0Q5WMne+hzOwj5bEfssH/gQrxFDz2fvclCGsQbfLLb8.\nThis key is not known by any other names.\nAre you sure you want to continue connecting (yes/no/[fingerprint])?\nYour terminal will look like the image below which means you can starting interacting with the job from the console on your computer. \nThe connection can be closed using the command:\nexit",
    "crumbs": [
      "HPC Launch",
      "Data management",
      "SSH keys"
    ]
  },
  {
    "objectID": "develop/e4-RDM_containers.html",
    "href": "develop/e4-RDM_containers.html",
    "title": "RDM for containers",
    "section": "",
    "text": "Now that you’re familiar with containers, it’s time to focus on making them reproducible and ensuring good Research Data Management (RDM) practices.\nThe current approach that we introduce on the Docker lesson has a significant drawback: it doesn’t ensure a reproducible environment because it depends on external servers and services that frequently update. If you lose your Docker image, you might not be able to rebuild it or even know precisely what was in it. You could save the output of the commands below alongside your Dockerfile. This information will be crucial if you need to rebuild the image.\nHow do we improve reproducibility?",
    "crumbs": [
      "HPC Pipes",
      "Computational environments",
      "RDM for containers"
    ]
  },
  {
    "objectID": "develop/e4-RDM_containers.html#sources",
    "href": "develop/e4-RDM_containers.html#sources",
    "title": "RDM for containers",
    "section": "Sources",
    "text": "Sources\n\nContent adapted from Reproducible Research II: Practices and tools for managing computations and data by members of France Universite Numerique.\nRDM - data analysis, Elixir Europe",
    "crumbs": [
      "HPC Pipes",
      "Computational environments",
      "RDM for containers"
    ]
  },
  {
    "objectID": "develop/p2-smk.html",
    "href": "develop/p2-smk.html",
    "title": "Snakemake",
    "section": "",
    "text": "In this section, we will guide you through transitioning from bash scripts or notebooks to workflows. This approach is particularly useful as computations become more complex.",
    "crumbs": [
      "HPC Pipes",
      "Pipeline languages",
      "Snakemake"
    ]
  },
  {
    "objectID": "develop/p2-smk.html#basics",
    "href": "develop/p2-smk.html#basics",
    "title": "Snakemake",
    "section": "Basics",
    "text": "Basics\nSnakemake is a text-based workflow management tool that uses a Python-based language with domain-specific syntax. Originally designed for scientific workflows in bioinformatics, Snakemake shares operational principles with make, from which it was inspired. Importantly, workflows in Snakemake are structured around data stored in files, and the tool supports parallel computing, cloud storage, and can manage computational environments.\nThe workflow is decomposed into rules that define how to obtain output files from input files. Snakemake infers dependencies and determines the execution order automatically, offering significantly more flexibility than its predecessor, make.\n\nSemantics\nDefine rules, corresponding to different tasks in a pipeline. Each rule can use different programs (e.g. bash to run a shell command, python, R, GNU core utilities)\n\n\nSnakefile\n\nrule dw_metadata:\n  input: path2/filename.tsv\n  output: data/filename.tsv\n  shell: \n    \"wget {input} &gt; {output}\"\n\nrule split_superpop:\n  input: data/filename.tsv\n  output: data/{superpop}.tsv\n  shell: \n    \"python process_pops.py {input} {output}\"\n\nrule avg_gender:\n  input: data/{superpop}.tsv\n  output: data/{superpop}_{gender}.png\n  shell: \n    \"python statsPlot_gender.py {input} {output}\"\n\nThe only mandatory flag in Snakemake is the number of cores (-c) or jobs (-j), which indicates the number of processes that can run in parallel. To run the workflow in Snakemake, you can either:\n\nRequest Snakemake to generate a specific file (using the exact filename as defined in the rule’s output)\n\n# Call the last output (of the pipeline or the one you want to generate)\nsnakemake -c2 data/EUR_female.png\n\nSpecify the name of a rule or task. You can do this when wildcards are not present\n\n# Call the last rule\nsnakemake -c1 dw_metadata\n\nAlternatively, in very common practice, determine what you want to run inside a Snakefile\n\nrule all:\n  input:\n    expand(data/{superpop}_{gender}.png, superpop=[\"EUR\"], gender=[\"female\", \"male\"])\n\nScaling up - rules generalisation using wildcards (e.g.: from one to several datasets) You can refer by index or by name\nDependencies are determined top-down\n\nFor a given target, a rule that can be applied to create it, is determined (a job)\nFor the input files of the rule, go on recursively,\nIf no target is specified, snakemake, tries to apply the first rule\n\nRule all: target rule that collects results\n\n\n\nJob execution\nA job is executed if and only if:\n\notuput file is target and does not exist\noutput file needed by another executed job and does not exist\ninput file newer than output file\ninput file will be updated by other job (eg. changes in rules)\nexecution is force (‘–force-all’)\n\nYou can plot the DAG (directed acyclic graph) of the jobs\n\n\nUseful command line interface\n# dry-run (-n), print shell commands (-p)\nsnakemake -n -p\n# Snakefile named different in another location \nsnakemake --snakefile path/to/file.smoker\n# dry-run (-n), print execution reason for each job\nsnakemake -n -r\n# Visualise DAG of jobs using Graphviz dot command\nsnakemake --dag | dot -Tsvg &gt; dag.svg\n\n\nDefining resources\nrule myrule:\n  resources: mem_mb= 100 #(100MB memory allocation)\n  threads: X\n  shell:\n    \"command {threads}\"\nLet’s say you defined our rule myrule needs 4 works, if we execute the workflow with 8 cores as follows:\nsnakemake --cores 8\nThis means that two myrule jobs, will be executed in parallel.\nThe jobs are schedules to maximize parallelization, high priority jobs will be scheduled first, all while satisfying resource constrains. This means:\nIf we allocate 100MB for the execution of myrule and we call snakemake as follows:\nsnakemake --resources mem_mb=100 --cores 8\nOnly one myrule job can be executed in parallel (you do not provide enough memory resources for 2). The memory resources is useful for jobs that are heavy memory demanding to avoid running out of memory. You will need to benchmark your pipeline to estimate how much memory and time your full workflow will take. We highly recommend doing so, get a subset of your dataset and give it a go! Log files will come very handy for the resource estimation. Of course, the execution of jobs is dependant on the free resources availability (eg. CPU cores).\nrule myrule:\n  log: \"logs/myrule.log\"\n  threads: X\n  shell:\n    \"command {threads}\"\nLog files need to define the same wildcards as the output files, otherwise, you will get an error.\n\n\nConfig files\nYou can also define values for wildcards or parameters in the config file (YAML or JSON). This is recommended when the pipeline might be used several times at different time points, to avoid unwanted modifications to the workflow. parameterization is key for such cases.\nIt can be loaded with: configfile: \"path/to/config.yaml\"\n\n\n\n\n\n\nWarning\n\n\n\nThe given path is interpreted relative to the working directory, not relative to the location of the snakefile that contains the statement.\n\n\n\n\nCluster execution\nWhen working from cluster systems you can execute the workflow using -qsub submission command\nsnakemake --cluster qsub \n\n\nAdditional advanced features\n\nmodularization\nhandling temporary and protected files: very important for intermediate files that filled up our memory and are not used in the long run and can be deleted once the final output is generated. This is automatically done by snakemake if you defined them in your pipeline HTML5 reports\nrule parameters\ntracking tool versions and code changes: will force rerunning older jobs when code and software are modified/updated.\ndata provenance information per file\npython API for embedding snakemake in other tools\n\n\n\nCreate an isolated environment to install dependencies\nBasic file structure\n| - config.yml\n| - requirements.txt (commonly also named environment.txt)\n| - rules/\n|   | - myrules.smk\n| - scripts/\n|   | - script1.py\n| - Snakefile\nCreate conda environment, one per project!\n# create env\nconda create -n myworklow --file requirements.txt\n# activate environment\nsource activate myworkflow\n# then execute snakemake\n\n\nBest Practices recommended by Snakemake\n\nSnakemake (v5.11+) includes a linter to check code quality, helping ensure best practices, readability, and reproducibility. Use the linter before publishing or seeking help. Run it with:\n\nsnakemake --lint\n\nApply snakefmt to format workflows prior to publication.\nAlways include test data when publishing on GitHub and configure GitHub Actions for continuos testing. Predefined actions for testing and linting (here), and formatting (here) are available.\nTo improve discoverability, follow Snakemake’s recommended workflow structure.\nUse config files and, if needed, tabular configuration for metadata and experiment information.\n\nUse command-line arguments for runtime settings like threads, resources or output folders such as --set-threads, --set-resources, --set-default-resources, and --directory. This makes workflows more readable, scalable, and portable.\nKeep filenames short and consistent, but informative. Avoid mixing of too many special characters (e.g. decide whether to use _ or - as a separator and do that consistently throughout the workflow).\nseparate Python code like helper functions from rules (e.g. in a workflow/rules/common.smk file). This approach helps non-experts understand the workflow without having to dig into unnecessary internal details.\nAvoid lambda expressions inside of rules.\nWhere possible, use Snakemake wrappers to simplify recurring tasks.",
    "crumbs": [
      "HPC Pipes",
      "Pipeline languages",
      "Snakemake"
    ]
  },
  {
    "objectID": "develop/p2-smk.html#sources",
    "href": "develop/p2-smk.html#sources",
    "title": "Snakemake",
    "section": "Sources",
    "text": "Sources\n\nSnakemake tutorial\nSnakemake turorial slides by Johannes Koster\nfaircookbook worflows\nFormatter snakefmt\nSnakemake workflow catalog\nRules for inclusion in Snakemake workflow catalog\nSnakemake github template\n\nRecommended reading:\n\nKöster, Johannes and Rahmann, Sven. “Snakemake - A scalable bioinformatics workflow engine”. Bioinformatics 2012.\nKöster, Johannes. “Parallelization, Scalability, and Reproducibility in Next-Generation Sequencing Analysis”, PhD thesis, TU Dortmund 2014.",
    "crumbs": [
      "HPC Pipes",
      "Pipeline languages",
      "Snakemake"
    ]
  },
  {
    "objectID": "develop/p4-workflow_envs.html",
    "href": "develop/p4-workflow_envs.html",
    "title": "Workflows & environments",
    "section": "",
    "text": "Snakemake or Nextflow pipelines are essentially code scripts that require an appropriate computational environment to run properly. Let’s explore the challenges of managing computational environments for workflows.\nYou can use a single common environment for all tasks in a workflow, which is generally recommended unless there are conflicting dependencies (for example, if one task requires a different version of a library than another). Alternatively, you might use separate environments if you’re reusing a task from another workflow and don’t want to alter its existing environment, or if a rarely run task has a large environment. In such cases, creating a dedicated environment for that task can help reduce the overall resource usage of the workflow.\n\n\nSnakemake has built support for tasks environments:\n\nConda\nEnvironment modules\nSingularity\n\nrule ...:\n  conda: \"path/to/env.yml\"\n  shell:\n    \"somecommand {output}\"\nNested environments with Docker for reproducibility Two-level environment:\n\nOuter container\nInner container\n\n\n\n\nEnable conda directives in the pipeline configuration file (e.g. nextflow.config).\nconda.enabled = true\nAlternatively, it can be specified by setting the variableNXF_CONDA_ENABLED=true in your environment or by using the -with-conda command line option.\nprocess foo {\n  conda 'bwa samtools multiqc'  # conda package YourNameSurname\n  conda '/path/to/my-env.yaml'  # conda environment file \n\n  '''\n  your_command --here\n  '''\n}\n\n\n\nEnvironment Manager\nLink\n\n\n\n\nDocker\nNextflow Containers\n\n\nSingularity/Apptainer\nNextflow Containers\n\n\nConda\nNextflow Conda Integration\n\n\n\nIt is recommended to specify environments in a separate configuration profile when possible to allow the execution via command line and enhance portability:\nprofiles {\n  conda {\n    process.conda = 'samtools'\n  }\n\n  docker {\n    process.container = 'biocontainers/samtools'\n    docker.enabled = true\n  }\n}\nThis allows the execution either with Conda or Docker specifying -profile conda or -profile docker when running the workflow script.",
    "crumbs": [
      "HPC Pipes",
      "Pipeline languages",
      "Workflows & environments"
    ]
  },
  {
    "objectID": "develop/p4-workflow_envs.html#integration-between-workflows-and-software-environments",
    "href": "develop/p4-workflow_envs.html#integration-between-workflows-and-software-environments",
    "title": "Workflows & environments",
    "section": "",
    "text": "Snakemake or Nextflow pipelines are essentially code scripts that require an appropriate computational environment to run properly. Let’s explore the challenges of managing computational environments for workflows.\nYou can use a single common environment for all tasks in a workflow, which is generally recommended unless there are conflicting dependencies (for example, if one task requires a different version of a library than another). Alternatively, you might use separate environments if you’re reusing a task from another workflow and don’t want to alter its existing environment, or if a rarely run task has a large environment. In such cases, creating a dedicated environment for that task can help reduce the overall resource usage of the workflow.\n\n\nSnakemake has built support for tasks environments:\n\nConda\nEnvironment modules\nSingularity\n\nrule ...:\n  conda: \"path/to/env.yml\"\n  shell:\n    \"somecommand {output}\"\nNested environments with Docker for reproducibility Two-level environment:\n\nOuter container\nInner container\n\n\n\n\nEnable conda directives in the pipeline configuration file (e.g. nextflow.config).\nconda.enabled = true\nAlternatively, it can be specified by setting the variableNXF_CONDA_ENABLED=true in your environment or by using the -with-conda command line option.\nprocess foo {\n  conda 'bwa samtools multiqc'  # conda package YourNameSurname\n  conda '/path/to/my-env.yaml'  # conda environment file \n\n  '''\n  your_command --here\n  '''\n}\n\n\n\nEnvironment Manager\nLink\n\n\n\n\nDocker\nNextflow Containers\n\n\nSingularity/Apptainer\nNextflow Containers\n\n\nConda\nNextflow Conda Integration\n\n\n\nIt is recommended to specify environments in a separate configuration profile when possible to allow the execution via command line and enhance portability:\nprofiles {\n  conda {\n    process.conda = 'samtools'\n  }\n\n  docker {\n    process.container = 'biocontainers/samtools'\n    docker.enabled = true\n  }\n}\nThis allows the execution either with Conda or Docker specifying -profile conda or -profile docker when running the workflow script.",
    "crumbs": [
      "HPC Pipes",
      "Pipeline languages",
      "Workflows & environments"
    ]
  },
  {
    "objectID": "develop/hpc-data_transfer.html",
    "href": "develop/hpc-data_transfer.html",
    "title": "HPC Lab",
    "section": "",
    "text": "To ensure efficient file transfers on HPC systems, we will go through:\n\nOverview rsync and scp\nBasic commands\n\nWhen transferring files between servers, it’s important to ensure that the files are consistent. Differences in file size can occur due to variations in filesystems, but large differences might indicate an issue. Manually checking file sizes in a terminal (ls -lh or du -sh) to determine if a transfer was successful might not be ideal, as it doesn’t guarantee file integrity. Using checksums provides a more reliable verification method.\nYou can use md5sum to verify that the file contents are identical on both servers. Run the following command on each server:\nmd5sum /path/to/file\nUsing rsync for efficient file transfers rsync is a powerful alternative to scp for transferring files. It only sends data if the file has changed, making it more efficient.\n# Transferring files between local machine-server\nrsync -avz local/path/to/file user@server:/remote/path/to/file\n# Transferring files between servers\nrsync -avz server1:/path/to/my_folder server2:/path/to/destination_folder\nThe -avz flags are commonly used together for efficient file synchronization:\n\n-a (archive): preserves symbolic links, permissions, timestamps, groups, owners, and devices while transferring files\n-v (verbose): display detailed information about the transfer (which files are being copied or updated)\n-z (compress): enables compression during transfer\n\nOther useful flags are:\n\n--progress: display a progress bar (transfer speed, percentage completed, estimated time remaining, …)\n--partial: ensures that partially transferred files are not discarded if the transfer is interrupted, allowing rsync to resume the transfer from where it left off the next time the command is run\n\n\n\n\n\n\n\nNote\n\n\n\nTo transfer files directly between two servers from your local workstation, ensure your SSH setup (configuration, keys, etc.) allows access to both servers. Check this section if you need help setting up your keys (generating, configuring and managing).\n\n\n\n\nrsync is an efficient protocol to compare and copy files between directories or server. It can resume interrupted transfers and compress files on the fly.\n\nChecksum Verification: rsync checks the hashsums of files and only transfers data if the hashes differ. This ensures that only the changed parts of the files are sent (so you can rsync a whole folder, and only the changes files will be send).\nTimestamp Preservation: Using the -a flag with rsync preserves the modified timestamps of files, which is particularly useful for tools like Snakemake.\n\nIf you prefer using SCP (Secure Copy Protocol) for transferring files between a local and remote host, or between two remote hosts, here are some useful commands:\n# copy from local to remote\nscp /home/my_laptop/files.txt username@login.server.dk:/home/username/myproject/\n\n# If you want to copy an entire folder, use the option -r (recursive copy)\nscp -r /home/my_laptop/myfolder username@login.server.dk:/home/username/myproject/\n\n# check other options available \nscp --help \n\n\n\n\n-a: Archive mode, preserves file attributes like timestamps and permissions (important if you are using snakemake).\n-v: Verbose mode, provides detailed information during transfer.\n-z: Compresses data during transfer, reducing the amount of data sent over the network.\n-c: Enables checksum checking, ensuring that files are identical by comparing their contents rather than just their size and modification time.",
    "crumbs": [
      "HPC Launch",
      "Data management",
      "Data transfer"
    ]
  },
  {
    "objectID": "develop/hpc-data_transfer.html#data-transfer",
    "href": "develop/hpc-data_transfer.html#data-transfer",
    "title": "HPC Lab",
    "section": "",
    "text": "To ensure efficient file transfers on HPC systems, we will go through:\n\nOverview rsync and scp\nBasic commands\n\nWhen transferring files between servers, it’s important to ensure that the files are consistent. Differences in file size can occur due to variations in filesystems, but large differences might indicate an issue. Manually checking file sizes in a terminal (ls -lh or du -sh) to determine if a transfer was successful might not be ideal, as it doesn’t guarantee file integrity. Using checksums provides a more reliable verification method.\nYou can use md5sum to verify that the file contents are identical on both servers. Run the following command on each server:\nmd5sum /path/to/file\nUsing rsync for efficient file transfers rsync is a powerful alternative to scp for transferring files. It only sends data if the file has changed, making it more efficient.\n# Transferring files between local machine-server\nrsync -avz local/path/to/file user@server:/remote/path/to/file\n# Transferring files between servers\nrsync -avz server1:/path/to/my_folder server2:/path/to/destination_folder\nThe -avz flags are commonly used together for efficient file synchronization:\n\n-a (archive): preserves symbolic links, permissions, timestamps, groups, owners, and devices while transferring files\n-v (verbose): display detailed information about the transfer (which files are being copied or updated)\n-z (compress): enables compression during transfer\n\nOther useful flags are:\n\n--progress: display a progress bar (transfer speed, percentage completed, estimated time remaining, …)\n--partial: ensures that partially transferred files are not discarded if the transfer is interrupted, allowing rsync to resume the transfer from where it left off the next time the command is run\n\n\n\n\n\n\n\nNote\n\n\n\nTo transfer files directly between two servers from your local workstation, ensure your SSH setup (configuration, keys, etc.) allows access to both servers. Check this section if you need help setting up your keys (generating, configuring and managing).\n\n\n\n\nrsync is an efficient protocol to compare and copy files between directories or server. It can resume interrupted transfers and compress files on the fly.\n\nChecksum Verification: rsync checks the hashsums of files and only transfers data if the hashes differ. This ensures that only the changed parts of the files are sent (so you can rsync a whole folder, and only the changes files will be send).\nTimestamp Preservation: Using the -a flag with rsync preserves the modified timestamps of files, which is particularly useful for tools like Snakemake.\n\nIf you prefer using SCP (Secure Copy Protocol) for transferring files between a local and remote host, or between two remote hosts, here are some useful commands:\n# copy from local to remote\nscp /home/my_laptop/files.txt username@login.server.dk:/home/username/myproject/\n\n# If you want to copy an entire folder, use the option -r (recursive copy)\nscp -r /home/my_laptop/myfolder username@login.server.dk:/home/username/myproject/\n\n# check other options available \nscp --help \n\n\n\n\n-a: Archive mode, preserves file attributes like timestamps and permissions (important if you are using snakemake).\n-v: Verbose mode, provides detailed information during transfer.\n-z: Compresses data during transfer, reducing the amount of data sent over the network.\n-c: Enables checksum checking, ensuring that files are identical by comparing their contents rather than just their size and modification time.",
    "crumbs": [
      "HPC Launch",
      "Data management",
      "Data transfer"
    ]
  },
  {
    "objectID": "develop/hpc-data_transfer.html#interactive-transfer",
    "href": "develop/hpc-data_transfer.html#interactive-transfer",
    "title": "HPC Lab",
    "section": "Interactive transfer",
    "text": "Interactive transfer\nFor users who prefer a graphical interface, tools like Cyberduck and FileZilla can also be used for transferring files between servers.\n\nDownload Filezilla here or Cyberduck here\nOpen the app and configure the access information for your host machine (including password, username, ssh keys (if relevant) and port: 22).\n\nSelect SFTP (SSH File Transfer Protocol) option on Cyberduck\n\nQuick connect\n\nThis will establish a secure connection to your host. You can navigate through your folders and files. Right-click on any file you want to download or preview.\n\nFilezilla: your local files will be display on the left-side of the dashboard. Right-click on them to upload or add it them to a transfer queue. If you have created a queue, this will be shown at the bottom of the window as a list. You can inspect destination folders from there and choose other options such as transfer priority. To start a queue, use CTRL + P, Transfer --&gt; Process Queue or use the toolbar.\n\nCyberduck: you can drag files from your local to the host, choose the directory where you want them located.",
    "crumbs": [
      "HPC Launch",
      "Data management",
      "Data transfer"
    ]
  },
  {
    "objectID": "develop/hpc-data_transfer.html#ucloud-users",
    "href": "develop/hpc-data_transfer.html#ucloud-users",
    "title": "HPC Lab",
    "section": "UCloud Users",
    "text": "UCloud Users\n\nData transfer\nTo transfer files between UCloud and your local machine, you must first configure your SSH keys. If you haven’t done so already, follow the instructions in the SSH keys section.\nNext, open an application on UCloud (e.g., Terminal) which will display the login command and the SSH port for client connections in the job progress view. Note that the SSH port is randomly generated and will be different each time.\n# Run these commands on a terminal in your local machine\n# Files from UCloud to local\nrsync -avP -e \"ssh -i ~/.ssh/id_rsa -p &lt;port&gt;\" ucloud@ssh.cloud.sdu.dk:/work/&lt;path_to_data&gt; ./&lt;path_local_data&gt;\n\n# Files from local to UCloud\nrsync -avP -e \"ssh -i ~/.ssh/id_rsa -p &lt;port&gt;\" ./&lt;path_local_data&gt; ucloud@ssh.cloud.sdu.dk:/work/&lt;path_to_data&gt;\n\n~/.ssh/id_rsa: path to your SSH private key on the remote host (locally)\n&lt;port&gt;: SSH number, which you will find on UCloud\nucloud@ssh.cloud.sdu.dk: default user on the remote server must be ucloud.\n/work/&lt;path_to_data&gt;/: data must be synchronized to a folder within the default working directory work\n\nTo transfer files between two servers, we recommend performing the transfer from a terminal on UCloud. Below is an example of the command for GenomeDK. Please note that this command will prompt you to enter your password, and you may also be asked to complete a two-factor authentication (2FA) process, if enabled.\n# From genomeDK to Ucloud\nrsync -avP &lt;username&gt;@login.genome.au.dk:/home/&lt;username&gt;/data /work/&lt;path_to_data&gt;\n\n/work/&lt;path_to_data&gt;/: data must be synchronized to a folder within the default working directory work\nReverse the order of the command if you need to transfer from UCloud to the GenomeDK server.",
    "crumbs": [
      "HPC Launch",
      "Data management",
      "Data transfer"
    ]
  },
  {
    "objectID": "develop/hpc-intro.html",
    "href": "develop/hpc-intro.html",
    "title": "HPC intro",
    "section": "",
    "text": "Course Overview\n\n\n\n\n⏰ Total Time Estimation: 3 hours\n\n📁 Supporting Materials:\n\n👨‍💻 Target Audience: Ph.D., MSc, anyone interested in HPC systems.\n👩‍🎓 Level: Advanced.\n🔒 License: Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license.\n\n💰 Funding: This project was funded by the Novo Nordisk Fonden (NNF20OC0063268).",
    "crumbs": [
      "HPC Launch",
      "HPC systems",
      "HPC intro"
    ]
  },
  {
    "objectID": "develop/hpc-intro.html#basics-of-high-performance-computing-hpcs",
    "href": "develop/hpc-intro.html#basics-of-high-performance-computing-hpcs",
    "title": "HPC intro",
    "section": "Basics of High-Performance Computing (HPCs)",
    "text": "Basics of High-Performance Computing (HPCs)\nHigh-Performance Computing (HPC) involves connecting a large number of computing hardware components to execute many operations simultaneously. A supercomputer consists of various hardware types, typically organized in this hierarchy:\n\nCPU: The unit that executes a sequence of instructions. A CPU may contain multiple cores, allowing independent execution of several instruction chains.\nNode: A single computer within an HPC system.\nCluster: A group of interconnected nodes that communicate and can work together on a single task.\n\nHPC systems also have a dedicated storage component connected to one or more types of storage hardware, typically referred to as a parallel file system or distributed storage system.\nA node can consist of one or multiple CPUs and RAM memory. The RAM (Random Access Memory) serves as temporary storage that helps manage the data required for running tasks quickly but does not perform computations or persist data after the system shuts down. There are other types of nodes containing different hardware combinations. The most common hardware that can be found in a node beyond RAM and CPUs is:\n\nGPU: A graphics processing unit, originally designed for gaming and graphic software, but now used for its computational power. It is particularly efficient in executing repetitive linear algebra operations across multiple parallel processes. Nvidia and AMD are the primary GPU manufacturers.\nFPGA: A programmable hardware component capable of accelerating specific operations far faster than a CPU. It is often used to optimize processes traditionally handled by CPUs.\n\nSchematic of components of an HPC [IMAGE]\n\nNodes\nThere are two types of nodes on a cluster:\n\nlogin nodes (also known as head or submit nodes).\ncompute nodes (also known as worker nodes).\n\n\n\nJob scheduler\n\n\n\n\n\n\nNote\n\n\n\nSeveral job scheduler programs are available, and SLURM is among the most widely used. In the next section, we’ll explore SLURM in greater detail, along with general best practices for running jobs.\n\n\n\n\nFilesystem\nThe filesystem consists of all the directories and files accessible to a given process.\n\nScratch\nUsers working space\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nI have an omics pipeline that produces a large number of files, resulting in a couple of terabytes of data after processing and analysis. The project will continue for a few more years, and I’ve decided to store the data in the scratch folder. Do you agree with this decision, and why? What factors should be considered when deciding which data to retain and where to store it?\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nTypically, scratch storage is not backed up, so it’s not advisable to rely on it for important data. At a minimum, ensure you back up the raw data and the scripts used for processing. This way, if some processed files are lost, you can replicate the analyses.\nWhen deciding which data to keep on the HPC, back up, or delete, consider the following:\n\nProcessing Time: Evaluate how long each step of the analysis takes to run. There may be significant computational costs associated with re-running heavy data processing steps.\nStorage Management: Use tools like Snakemake to manage intermediate files. You can configure Snakemake to automatically delete intermediate files once the final results are produced, helping you manage storage more efficiently.\n\n\n\n\n\n\n\n\n\n\n\n\n\nKernel\nThe kernel is essential for managing multiple programs on your machine, each of which runs as a process. Even if you write code assuming full control over the CPU and memory, the kernel ensures that multiple processes can run simultaneously without interfering with each other. It does this by scheduling time for each process and translating virtual memory addresses into physical ones, ensuring security and preventing conflicts.\nThe kernel also ensures that processes can’t access each other’s memory or directly modify the hard drive, maintaining system security and stability. For example, when a process needs to write to a file, it asks the kernel to do so through a system call, rather than writing directly.\nIn conclusion, it plays a crucial role in managing the CPU, memory, disk, and software environment. By mediating access to these resources, it maintains process isolation, security, and the smooth operation of your system.\n\n\n\n\n\n\nKernel primary roles:\n\n\n\n\nInterfaces with hardware to facilitate program operations\nManages and schedules the execution of processes\nRegulates and allocates system resources among processes\n\n\n\n\n\nBefore start using an HPC\nHigh-Performance Computing (HPC) systems might be organized differently, but there is typically an HPC administration team you can contact to understand how your specific HPC is structured. Key information you should seek from them includes:\n\nThe types of compute nodes available.\nThe storage options you can access and the amount allocated per user.\nWhether a job scheduler software is in use, and if so, which one. You can also request a sample submission script to help you get started.\nThe policy on who bears the cost of using the HPC resources.\nWhether you can install your own software and create custom environments.\n\n\n\n\n\n\n\nBe nice\n\n\n\nIf your HPC system doesn’t have a job scheduler in place, we recommend using the nice command. This command allows you to adjust and manage the scheduling priority of your processes, giving you the ability to run tasks with lower priority when sharing resources with others. By using nice, you can ensure that your processes do not dominate the CPU, allowing other users’ tasks to run smoothly. This is particularly useful in environments where multiple users are working on the same system without a job scheduler to automatically manage resource allocation.\n\n\n\n\n\n\n\n\nExercise 1: General HPC\n\n\n\n\n\n\n\n\nDescribe how a typical HPC is organised: nodes, job scheduler and filesystem.\nWhat are the roles of a login node and a compute node? how do they differ?\nDescribe the role of a job scheduler\nWhat are the differences between scratch and home storage and when each should be used?\nWhat is a kernel?\n\n\n\n\n\n\n\n\nKey areas of HPC use\n\nQuantum mechanics\nComplex physical simulations (CFD)\nWeather forecasting\nMolecular modeling\nMachine learning with big data\nBioinformatics\n\n\n\nAcademic applications\nHPC systems offer immense computational power, but they are not limited to large-scale projects. You can request anything from a single CPU and some RAM to entire nodes. Danish HPC systems are available for various academic purposes, including:\n\nResearch projects: access to computing power\nCollaboration: Easier data and settings sharing for collaborative projects.\nStudent exercises in classroom teaching: Pre-installed software or package management, saving time on configuration, especially in teaching environments where students may face issues with different operating systems or software versions.\nStudent projects: students are not authorized to request resources independently. It is the responsibility of the lecturer or professor to obtain resources through the front office or facilitator. Once resources are allocated, students can be invited to access the project.\n\nThe Danish HPC ecosystem emphasizes teaching and training new users, so applications for resources related to courses and student projects are strongly encouraged. As a reminder, students cannot request resources themselves. Professors or lecturers are responsible for obtaining resources through the front office, and students can be granted access to the allocated project.\nHere is an overview of the different Danish HPC systems\n\n\n\n\n\n\n\n\n\n\n\nFeature\nComputerome\nGenomeDK\nUCloud\nGefion\nLUMI\n\n\n\n\nCPU Nodes\n692 thin / 55 fat (50k cores)\n52 thin / 60 fat (15k cores)\n6592 vCPUs\n382 / 112 core each\n2048 / 128 core each\n\n\nGPU Nodes\n40 NVIDIA V100s\n2 NVIDIA L40S\n16 NVIDIA H100s\n191 NVIDIA DGX / 8 H100s\n2978 / 4 AMD M250x\n\n\nStorage\n50 PB\n23 PB\n3 PB\n? Provided by DDN\n~120 PB\n\n\nSecurity\nISAE 3000 + ISO 27001\nISAE 3000 + ISO 27001\nISO 27001\nNA\nISAE 3000 + ISO 27001\n\n\nSandbox\nNo\nNo\nYes (1000 core-hours)\nApplication based\nApplication based\n\n\nSensitive Data\nYes, private clouds\nYes, ‘closed zones’\nYes\nNot yet\nNot yet\n\n\nEnv Management\nConda (& Docker?)\nSingularity/Apptainer\nConda\nNA\nSingularity/Apptainer\n\n\nOS\nCentOS 7\nAlmaLinux 8\nUCloud GUI\n? NVIDIA Enterprise\nSUSE LES 15 / CRAY",
    "crumbs": [
      "HPC Launch",
      "HPC systems",
      "HPC intro"
    ]
  },
  {
    "objectID": "develop/hpc-intro.html#hpc-access",
    "href": "develop/hpc-intro.html#hpc-access",
    "title": "HPC intro",
    "section": "HPC access",
    "text": "HPC access\nHPC systems allow multiple users to log in simultaneously and utilize a portion of the system’s resources, usually allocated by an administrator. Exceeding these assigned resources will terminate the running software. In Denmark, there are two ways to access an HPC:\n\nInteractive Interface: You can log in using a user-friendly interface (UCloud Supercomputer and Documentation).\nCommand Line Interface: Requires knowledge of the UNIX shell language (here a good introduction the command-line).\n\nTypically, users first access a login node, which has limited computational resources and is used for tasks like file management and small-scale code testing. Users may be assigned:\n\nA certain number of CPUs (and possibly GPUs or FPGAs)\nA specific amount of RAM\nStorage capacity\nAn amount of total time for using these resources\n\nWe recommend to directly contact the local front office to discuss resource availability.\n\n\n\n\n\n\nWhat can I run from a login node\n\n\n\nA straightforward rule: do not run anything on the login node to prevent potential problems. If the login node crashes, the entire system may need to be rebooted, affecting everyone. Remember, you’re not the only one using the HPC—so be considerate of others. For easy, quick tasks, request an interactive access to one of the compute nodes.\nIn principle, the only activities you should perform on the login node include:\n\nYour active login session.\n[OPTIONAL] A terminal multiplexer, such as TMUX, SCREEN, or similar.\nSubmitting jobs to the queueing system, whether regular or interactive.",
    "crumbs": [
      "HPC Launch",
      "HPC systems",
      "HPC intro"
    ]
  },
  {
    "objectID": "develop/hpc-intro.html#sources",
    "href": "develop/hpc-intro.html#sources",
    "title": "HPC intro",
    "section": "Sources",
    "text": "Sources\nUseful links\n\nAcknowledgements",
    "crumbs": [
      "HPC Launch",
      "HPC systems",
      "HPC intro"
    ]
  },
  {
    "objectID": "develop/p3-nextflow.html",
    "href": "develop/p3-nextflow.html",
    "title": "Nextflow",
    "section": "",
    "text": "Nextflow is a workfow management system that offers scalable and portable NGS data analysis pipelines, facilitating data processing across diverse computing environments. It streamlines and automates various data analysis steps, enabling parallel processing and seamless integration with existing tools.",
    "crumbs": [
      "HPC Pipes",
      "Pipeline languages",
      "Nextflow"
    ]
  },
  {
    "objectID": "develop/p3-nextflow.html#basics",
    "href": "develop/p3-nextflow.html#basics",
    "title": "Nextflow",
    "section": "Basics",
    "text": "Basics\nRead more about the basics here. Let’s talk about the main elements:\n\nprocesses: are the different tasks from a workflow. They are executed independently, are isolated from each other and can be written in any scripting language.\nchannels: for example, input and output. Each process can defined one or more!\nmodules: is a script that contains functions, processes, and workflows.\ninclude { process } from './process_module'\n\nThe interaction between the processes, and ultimately the pipeline execution flow itself, is implicitly defined by these input and output declarations.\n\nJob execution\nnextflow run &lt;pipeline_name&gt; --cpus &lt;n&gt; --mem &lt;n&gt;GB\nIf a job fails, the pipeline will stop. However, there are some processes directives that can help you handle some errors.\n\nerrorStrategy: key to record failures but avoid stopping the pipeline.\n\nprocess ignoreAnyError {\n  errorStrategy 'ignore'\n\n  script:\n  &lt;your command string here&gt;\n}\n\nmaxRetries\n\nprocess retryIfFail {\n  errorStrategy 'retry'\n  maxRetries 2\n  memory { task.attempt * 10.GB}\n\n  script:\n  &lt;your command string here&gt;\n}\n\n\nUseful command line interface\n# dry-run \nnextflow run main.nf -dry-run\n\n# List processes\nnextflow run main.nf -process.list\n\n# Using configuration file \nnextflow run main.nf -c my.config\n\n# Trace execution (logging)\nnextflow run main.nf -trace\n\n# Resume previous run (interrupted)\nnextflow run main.nf -resume\n\n\n\nCluster execution\nWhether you run the pipeline locally or on an HPC, you can find the Nextflow executor compatible with your environment. Executors manage how and where tasks are executed.\nnextflow run &lt;pipeline_name&gt; -profile slurm \n\n\nConfig files\nConfiguration files are used to specify settings, parameters and other configurations for the pipeline. Find Nextflow documentationhere.\nNextflow allows you to define parameters directly within the main.nf file, enabling their use in the workflow logic. Additionally, NextFlow supports the definition of parameters in external configuration files, such as nextflow.config. These parameters can then be accessed and utilized within the .nf file, offering flexibility in managing workflow behavior and ensuring consistency across different runs.\nThe hierarchy of how parameters will be used is as follows:\n\nparameters defined on the command line using --paramname\nparameters defined in the user config file(s) supplied via -c my.config (in the order that they are provided)\nparameters defined in the default config file nextflow.config\nparameters defined within the .nf file\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that if the user specifies -C my.config (capital C) then only that config file will be read, and the nextflow.config file will be ignored.\n\n\n\n\nDefining resources\nprocess {\n    withName: my_task {\n        cpus = 4\n        memory = '8 GB'\n        time = '2h'\n    }\n}\n\n\nBest practices\n\nDocument your pipeline: overview fo what the workflow does, description of the outputs (results), description of the input and other required files.\nMetadata: author, doi, name, version.\nAttach a test dataset so that others can easily run it.\nCreate a --help documentation for all your Nextflow scripts so others can easily use and understand them.\nMake your workflow easy to read and understand: using whitespaces, comments, name output channels\nMake your workflow modular to avoid duplicate code",
    "crumbs": [
      "HPC Pipes",
      "Pipeline languages",
      "Nextflow"
    ]
  },
  {
    "objectID": "develop/p3-nextflow.html#nf-core",
    "href": "develop/p3-nextflow.html#nf-core",
    "title": "Nextflow",
    "section": "nf-core",
    "text": "nf-core\nnf-core is a collaborative platform that provides high-quality, standardized, and peer-reviewed bioinformatics pipelines built using Nextflow. These pipelines are designed to be portable, reproducible, and scalable across various computing environments, from local setups to cloud-based platforms and high-performance computing (HPC) clusters. nf-core also ensures best practices by offering documentation and continuous integration testing for all pipelines, promoting consistency in bioinformatics workflows.\nIf you want to contribute, start by building your pipeline using an nf-core template.",
    "crumbs": [
      "HPC Pipes",
      "Pipeline languages",
      "Nextflow"
    ]
  },
  {
    "objectID": "develop/p3-nextflow.html#sources",
    "href": "develop/p3-nextflow.html#sources",
    "title": "Nextflow",
    "section": "Sources",
    "text": "Sources\n\nNextflow documentation\nfaircookbook worflows\ntraining material",
    "crumbs": [
      "HPC Pipes",
      "Pipeline languages",
      "Nextflow"
    ]
  },
  {
    "objectID": "develop/p1-fair_workflow.html",
    "href": "develop/p1-fair_workflow.html",
    "title": "FAIR computational pipelines",
    "section": "",
    "text": "Data analysis typically involves the use of different tools, algorithms, and scripts. It often requires multiple steps to transform, filter, aggregate, and visualize data. The process can be time-consuming because each tool may demand specific inputs and parameter settings. As analyses become more complex, the importance of reproducible and scalable automated workflow management increases since the key for reproducibility is automation.\n\n\nBash scripts have been widely used for automation in the past and can handle many tasks effectively. Typically, running a bash script requires just one command, which executes all the steps in the script. However, a significant drawback is that it re-runs all steps every time. This can be problematic in certain situations.\n\n\n\n\n\n\nExercise 1.\n\n\n\n\n\n\n\nConsider scenarios where re-running all steps can be an issue (minimum 2).\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nHere are some examples:\n\nChanging input files: if only some parts of the pipeline are affected by the changes.\nCode bugs: issues such as incorrect paths or typos in your code.\nSoftware updates: newer version released.\nParameter updates: test/update parameters in a software tool.\nScript Modifications: for example, if only the plotting section of your script is updated, re-running the entire pipeline could waste significant time and resources.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotebooks might represent an advancement in addressing this issue because they allow you to run individual cells separately. However, this process is manual, and you need to ensure that the cells are executed in the correct order (time-consuming). In practice, it’s easy to make mistakes, and thus, reproducibility is only guaranteed if you run all the cells systematically from top to bottom. This approach can be time-consuming and requires careful management to avoid errors.\n\n\n\n\n\n\nExercise 2: notebooks pros and cons\n\n\n\n\n\n\n\nWhat are the notebooks advantages and disadvantages in the following situations?\n\nIf your pipeline consist of 100 commands\nIf your pipeline is only 4 steps but each takes several weeks of computational time\nBenchmarking and testing parameters of new software\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nMain disadvantage: reproducibility! If you are running block cells by hand, it will be hard to reproduce.\n\n\n\n\nAdvantages:\n\ncan check intermediate outputs and execute incrementally.\nuser-friendly interface for visualizing and debugging.\n\nDisadvantages:\n\nless efficient for large number of commands compared to bash scripts.\ncan become cumbersome and slow intensive code blocks.\nno automation features and lack of advanced version control.\n\n\n\n\n\n\nAdvantages:\n\nsimplified interface for quick execution and visualization.\neasier to manage and understand fewer steps.\ngreat for prototyping and testing small workflows.\n\nDisadvantages:\n\nless straightforward automation.\n\n\n\n\n\n\nAdvantages:\n\nfacilitates experimentation and visualizes results instantly.\neasy documentation.\nenables step-by-step execution and modification of parameters.\n\nDisadvantages:\n\nnot efficient for extensive benchmarking.\ntracking and managing multiple parameters can become complex.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorkflows propose a third option, which is very attractive for computations that are too big to be handled conveniently by scripts or notebooks. A workflow manager, is a suitable computer program that decides which task and when is run. Workflow management encompasses tasks such as parallelization, resumption, logging, and data provenance.\nOne single command should trigger all necessary steps, ensuring they run in the correct sequence (simple and easy!). Workflows are divided into separate tasks, with each task reading an input and writing an output. A given task needs to be re-run only when its code or input data have changed. Using workflow managers, you ensure:\n\nautomation\nconvenience\nportability\nreproducibility\nscalability\nreadable\n\nPopular workflow management systems such as Snakemake, Nextflow, and Galaxy can be scaled effortlessly across server, cluster, and cloud environments without altering the workflow definition. They also allow for specifying the necessary software, ensuring the workflows can be deployed in any setting. It’s important to select a workflow manager that best fits each research project. Therefore, we will provide two sections: one on Snakemake and one on Nextflow, so you can make the best selection for your needs.\n\n\nA key service offered by workflow managers is resource management, essential for executing large computations. They handle parallel execution, running tasks simultaneously when they do not depend on each other and can execute a single parallelized task on multiple nodes in a computing cluster. Workflow managers also handle data storage, managing local files, shared filesystems (storage system within a server), and cloud storage. Additionally, they can manage software environments by interfacing with package managers and running tasks in dedicated containers.\nDuring this module, you will learn about:\n\nSyntax: understand the syntax of two workflow languages.\nDefining steps: how to define a step in each of the language (rule in Snakemake, process in Nextflow), including specifying input, outputs and execution statements.\nGeneralizing steps: explore how to generalise steps and create a chain of dependency across multiple steps using wildcards (Snakemake) or parameters and channel operators (Nextflow).\nAdvanced Customisation: gain knowledge of advanced pipeline customisation using configuration files and custom-made functions.\nScaling workflows: understand how to scale workflows to compute servers and clusters while adapting to hardware-specific constraints.With multiple CPUs available, you can leverage parallel execution for groups of tasks that are independent of each other. In this context, tasks do not rely on the outputs of other tasks in the same group. Since data is transferred between tasks using files, it is easy to see how tasks depend on each other (dependencies).\n\nTo put this into practice, you will start by scaling up a data analysis from one dataset to a large number of datasets and incorporating additional analysis steps at the aggregate level. However, before you can scale up, the first step involves converting a notebook into a shell script that chains several computational tasks (Exercise 3). This script will serve as an intermediate stage before you move on to using Snakemake and Nextflow, and completing it will help you understand how different tasks correspond and are split in these tools.\n\n\n\n\n\n\nHow to define a good task?\n\n\n\nIn a workflow, a task is executed in its entirety or not at all. Long-running tasks can undermine one of the primary advantages of workflows, which is to execute only the necessary computations. Conversely, there is overhead associated with starting a task, reading input files, and writing output files. If the computational work performed by a task is too minimal, this overhead can become disproportionately large. With this in mind, we should also assess the effects of data input/output (I/O) on disk storage and code complexity. When a task involves minimal computation but is heavily dominated by I/O operations, it becomes difficult to understand and modify. Furthermore, a task that requires large amounts of disk storage for its execution can lead to significant costs.\n\n\n\n\n\n\n\n\nExercise 3: notebook -&gt; shell script\n\n\n\n\n\n\n\nConsider each task as a Python script containing the code from one or more cells of a Jupyter Notebook. The key aspect of these tasks is that all data exchange occurs via files. Ideally, a task should be neither too lengthy nor too brief.\nWe will use the classic Iris dataset for this exercise. Convert this Jupyter notebook to a shell script.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\n\nReuse as much as possible from the code above but remove code for displaying tables or plotting figures.\n\nLeave only comments specifically for the code, create a README.md for the description of the dataset and the objective of the project.\nSave the python code to a file (e.g. process_iris.py).\nWrite a shell script and run the python script once for one of the species (e.g.”setosa” | “versicolor” | “virginica”).\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nHere is one possible approach. You may choose to split the notebook into more or fewer tasks as needed. Remember to make your script executable by running chmod +x run_iris_analysis.sh so that you can run it by simply ./run_iris_analysis.sh.\nThe reason for splitting the tasks is based on their functionality. We handle plotting and data manipulation or preprocessing in one script, as these tasks are typically performed together. The summary statistics, however, might need to be generated multiple times or for different species. Therefore, we created a separate script specifically for summarizing the data. This separation allows us to run the summary script as needed, either once, multiple times, or for different species, while keeping the data processing and plotting tasks contained in their own script.\n#!/bin/bash\n\npython process_iris.py\n\nSPECIES_NAME=\"setosa\"\npython summaryStats_species.py $SPECIES_NAME",
    "crumbs": [
      "HPC Pipes",
      "Pipeline languages",
      "FAIR computational pipelines"
    ]
  },
  {
    "objectID": "develop/p1-fair_workflow.html#introduction",
    "href": "develop/p1-fair_workflow.html#introduction",
    "title": "FAIR computational pipelines",
    "section": "",
    "text": "Data analysis typically involves the use of different tools, algorithms, and scripts. It often requires multiple steps to transform, filter, aggregate, and visualize data. The process can be time-consuming because each tool may demand specific inputs and parameter settings. As analyses become more complex, the importance of reproducible and scalable automated workflow management increases since the key for reproducibility is automation.\n\n\nBash scripts have been widely used for automation in the past and can handle many tasks effectively. Typically, running a bash script requires just one command, which executes all the steps in the script. However, a significant drawback is that it re-runs all steps every time. This can be problematic in certain situations.\n\n\n\n\n\n\nExercise 1.\n\n\n\n\n\n\n\nConsider scenarios where re-running all steps can be an issue (minimum 2).\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nHere are some examples:\n\nChanging input files: if only some parts of the pipeline are affected by the changes.\nCode bugs: issues such as incorrect paths or typos in your code.\nSoftware updates: newer version released.\nParameter updates: test/update parameters in a software tool.\nScript Modifications: for example, if only the plotting section of your script is updated, re-running the entire pipeline could waste significant time and resources.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotebooks might represent an advancement in addressing this issue because they allow you to run individual cells separately. However, this process is manual, and you need to ensure that the cells are executed in the correct order (time-consuming). In practice, it’s easy to make mistakes, and thus, reproducibility is only guaranteed if you run all the cells systematically from top to bottom. This approach can be time-consuming and requires careful management to avoid errors.\n\n\n\n\n\n\nExercise 2: notebooks pros and cons\n\n\n\n\n\n\n\nWhat are the notebooks advantages and disadvantages in the following situations?\n\nIf your pipeline consist of 100 commands\nIf your pipeline is only 4 steps but each takes several weeks of computational time\nBenchmarking and testing parameters of new software\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nMain disadvantage: reproducibility! If you are running block cells by hand, it will be hard to reproduce.\n\n\n\n\nAdvantages:\n\ncan check intermediate outputs and execute incrementally.\nuser-friendly interface for visualizing and debugging.\n\nDisadvantages:\n\nless efficient for large number of commands compared to bash scripts.\ncan become cumbersome and slow intensive code blocks.\nno automation features and lack of advanced version control.\n\n\n\n\n\n\nAdvantages:\n\nsimplified interface for quick execution and visualization.\neasier to manage and understand fewer steps.\ngreat for prototyping and testing small workflows.\n\nDisadvantages:\n\nless straightforward automation.\n\n\n\n\n\n\nAdvantages:\n\nfacilitates experimentation and visualizes results instantly.\neasy documentation.\nenables step-by-step execution and modification of parameters.\n\nDisadvantages:\n\nnot efficient for extensive benchmarking.\ntracking and managing multiple parameters can become complex.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorkflows propose a third option, which is very attractive for computations that are too big to be handled conveniently by scripts or notebooks. A workflow manager, is a suitable computer program that decides which task and when is run. Workflow management encompasses tasks such as parallelization, resumption, logging, and data provenance.\nOne single command should trigger all necessary steps, ensuring they run in the correct sequence (simple and easy!). Workflows are divided into separate tasks, with each task reading an input and writing an output. A given task needs to be re-run only when its code or input data have changed. Using workflow managers, you ensure:\n\nautomation\nconvenience\nportability\nreproducibility\nscalability\nreadable\n\nPopular workflow management systems such as Snakemake, Nextflow, and Galaxy can be scaled effortlessly across server, cluster, and cloud environments without altering the workflow definition. They also allow for specifying the necessary software, ensuring the workflows can be deployed in any setting. It’s important to select a workflow manager that best fits each research project. Therefore, we will provide two sections: one on Snakemake and one on Nextflow, so you can make the best selection for your needs.\n\n\nA key service offered by workflow managers is resource management, essential for executing large computations. They handle parallel execution, running tasks simultaneously when they do not depend on each other and can execute a single parallelized task on multiple nodes in a computing cluster. Workflow managers also handle data storage, managing local files, shared filesystems (storage system within a server), and cloud storage. Additionally, they can manage software environments by interfacing with package managers and running tasks in dedicated containers.\nDuring this module, you will learn about:\n\nSyntax: understand the syntax of two workflow languages.\nDefining steps: how to define a step in each of the language (rule in Snakemake, process in Nextflow), including specifying input, outputs and execution statements.\nGeneralizing steps: explore how to generalise steps and create a chain of dependency across multiple steps using wildcards (Snakemake) or parameters and channel operators (Nextflow).\nAdvanced Customisation: gain knowledge of advanced pipeline customisation using configuration files and custom-made functions.\nScaling workflows: understand how to scale workflows to compute servers and clusters while adapting to hardware-specific constraints.With multiple CPUs available, you can leverage parallel execution for groups of tasks that are independent of each other. In this context, tasks do not rely on the outputs of other tasks in the same group. Since data is transferred between tasks using files, it is easy to see how tasks depend on each other (dependencies).\n\nTo put this into practice, you will start by scaling up a data analysis from one dataset to a large number of datasets and incorporating additional analysis steps at the aggregate level. However, before you can scale up, the first step involves converting a notebook into a shell script that chains several computational tasks (Exercise 3). This script will serve as an intermediate stage before you move on to using Snakemake and Nextflow, and completing it will help you understand how different tasks correspond and are split in these tools.\n\n\n\n\n\n\nHow to define a good task?\n\n\n\nIn a workflow, a task is executed in its entirety or not at all. Long-running tasks can undermine one of the primary advantages of workflows, which is to execute only the necessary computations. Conversely, there is overhead associated with starting a task, reading input files, and writing output files. If the computational work performed by a task is too minimal, this overhead can become disproportionately large. With this in mind, we should also assess the effects of data input/output (I/O) on disk storage and code complexity. When a task involves minimal computation but is heavily dominated by I/O operations, it becomes difficult to understand and modify. Furthermore, a task that requires large amounts of disk storage for its execution can lead to significant costs.\n\n\n\n\n\n\n\n\nExercise 3: notebook -&gt; shell script\n\n\n\n\n\n\n\nConsider each task as a Python script containing the code from one or more cells of a Jupyter Notebook. The key aspect of these tasks is that all data exchange occurs via files. Ideally, a task should be neither too lengthy nor too brief.\nWe will use the classic Iris dataset for this exercise. Convert this Jupyter notebook to a shell script.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\n\nReuse as much as possible from the code above but remove code for displaying tables or plotting figures.\n\nLeave only comments specifically for the code, create a README.md for the description of the dataset and the objective of the project.\nSave the python code to a file (e.g. process_iris.py).\nWrite a shell script and run the python script once for one of the species (e.g.”setosa” | “versicolor” | “virginica”).\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nHere is one possible approach. You may choose to split the notebook into more or fewer tasks as needed. Remember to make your script executable by running chmod +x run_iris_analysis.sh so that you can run it by simply ./run_iris_analysis.sh.\nThe reason for splitting the tasks is based on their functionality. We handle plotting and data manipulation or preprocessing in one script, as these tasks are typically performed together. The summary statistics, however, might need to be generated multiple times or for different species. Therefore, we created a separate script specifically for summarizing the data. This separation allows us to run the summary script as needed, either once, multiple times, or for different species, while keeping the data processing and plotting tasks contained in their own script.\n#!/bin/bash\n\npython process_iris.py\n\nSPECIES_NAME=\"setosa\"\npython summaryStats_species.py $SPECIES_NAME",
    "crumbs": [
      "HPC Pipes",
      "Pipeline languages",
      "FAIR computational pipelines"
    ]
  },
  {
    "objectID": "develop/p1-fair_workflow.html#good-practices",
    "href": "develop/p1-fair_workflow.html#good-practices",
    "title": "FAIR computational pipelines",
    "section": "Good practices",
    "text": "Good practices\nIf you develop your own software make sure you follow FAIR principles. We highly endorse following these FAIR recommendations.\n\nRemember to create portable code and use relative paths to ensure transferability between users.\nUse git repositories to save your projects and pipelines.\nRegister and publish your scientific computational workflow on WorkflowHub.",
    "crumbs": [
      "HPC Pipes",
      "Pipeline languages",
      "FAIR computational pipelines"
    ]
  },
  {
    "objectID": "develop/p1-fair_workflow.html#sources",
    "href": "develop/p1-fair_workflow.html#sources",
    "title": "FAIR computational pipelines",
    "section": "Sources",
    "text": "Sources\n\nSnakemake tutorial\nSnakemake tutorial slides by Johannes Koster\nbioconda\n\nRDM best practices for computations\n\nThe turing way - reproducible research\nfaircookbook worflows\nRDM - data analysis, Elixir\n\nParts of the content are inspired by Reproducible Research II: Practices and Tools for Managing Computations and Data by members of France Université Numérique. Enroll here.\nScientific articles\n\nWratten, Laura, Andreas Wilm, and Jonathan Göke. “Reproducible, scalable, and shareable analysis pipelines with bioinformatics workflow managers.” Nature methods 2021.\nKöster, Johannes and Rahmann, Sven. “Snakemake - A scalable bioinformatics workflow engine”. Bioinformatics 2012.\nKöster, Johannes. “Parallelization, Scalability, and Reproducibility in Next-Generation Sequencing Analysis”, PhD thesis, TU Dortmund 2014.",
    "crumbs": [
      "HPC Pipes",
      "Pipeline languages",
      "FAIR computational pipelines"
    ]
  },
  {
    "objectID": "develop/cheat_sheet.html",
    "href": "develop/cheat_sheet.html",
    "title": "Cheat sheet",
    "section": "",
    "text": "Collection of useful commands for package and environment management.",
    "crumbs": [
      "HPC Pipes",
      "Computational environments",
      "Cheat sheet"
    ]
  },
  {
    "objectID": "develop/cheat_sheet.html#environment-descriptors",
    "href": "develop/cheat_sheet.html#environment-descriptors",
    "title": "Cheat sheet",
    "section": "Environment descriptors",
    "text": "Environment descriptors\n\nGit: git log -1 and git status -u. In python, use the following command for a specific module version: .version.git_revision) or .__git_version__.\nR: sessionInfo() or devtools::session_info(): In R, these functions provide detailed information about the current session and list all dependencies.\nPython: the best way seems to import sys, and then, __version__, __file__ which will display package versions and their location without having to list all packages using pip. It’s essential to load the package first and then use the following code to print all its dependencies:",
    "crumbs": [
      "HPC Pipes",
      "Computational environments",
      "Cheat sheet"
    ]
  },
  {
    "objectID": "develop/cheat_sheet.html#shell",
    "href": "develop/cheat_sheet.html#shell",
    "title": "Cheat sheet",
    "section": "SHELL",
    "text": "SHELL\n# finds the location of a command by searching through the PATH environment variable\nwhich &lt;program&gt;\n\n# lists all occurrences of a command found in the PATH\nwhich -a &lt;program&gt; \n\n# shows the shared libraries required by a specific program\nldd &lt;program&gt; \n\n# provides detailed system information, including machine name and kernel version\nuname -a \n\n# displays operating system identification data (such as Debian, Ubuntu, etc.)\ncat /etc/os-release \n\n# provides operating system identification data and is recommended if available. This command might not be installed by default but is part of the lsb-release package on Debian-based systems\nlsb_release -a \n\n# Environmental variables for locations \n$HOME # home directory\n$PYTHONPATH # empty by default\n$PYTHONHOME # python libraries\n$RHOME # R libraries\n$LD_LIBRARY_PATH # dynamic loader for shared libraries when running a program\nUnderstanding PYTHONPATH\n\nPYTHONPATH: you can set the $PYTHONPATH environment variable to include additional directories where Python will look for modules and packages. This allows you to extend the search path beyond the default locations and load packages that has been installed in a different directory. It is highly discouraged to mix different versions of libraries and interpreters! as some libraries are complex packages with dependencies.\n\nexport PYTHONPATH=/path/to/packages/\n# unset the variable \nunset PYTHONPATH",
    "crumbs": [
      "HPC Pipes",
      "Computational environments",
      "Cheat sheet"
    ]
  },
  {
    "objectID": "develop/cheat_sheet.html#pip",
    "href": "develop/cheat_sheet.html#pip",
    "title": "Cheat sheet",
    "section": "PIP",
    "text": "PIP\npip3 show &lt;module&gt;\npip3 install &lt;module&gt;      # install the latest version (and dependencies)\npip3 uninstall &lt;module&gt;    # remove\npip3 freeze                # output installed packages in requirements.txt format (similar to pip3 list) which can conveniently be used with: pip3 install -r requirements.txt\nWe highly recommend to avoid using pip and start using Python virtualenv management tools, pipenv.\n\nAdvantages: it will generates and checks file hashes for locked dependencies when installing from Pipfile.lock and it creates a virtualenv in a standard customizable location.",
    "crumbs": [
      "HPC Pipes",
      "Computational environments",
      "Cheat sheet"
    ]
  },
  {
    "objectID": "develop/cheat_sheet.html#python",
    "href": "develop/cheat_sheet.html#python",
    "title": "Cheat sheet",
    "section": "PYTHON",
    "text": "PYTHON\n# version info on installed packages\n\ndef print_imported_modules():\n  import sys\n  for name, val in sorted(sys.modules.items()):\n      if(hasattr(val, '__version__')): \n          print(val.__name__, val.__version__)\n      else:\n          print(val.__name__, \"(unknown version)\")\n\nprint(\"==== Package list after loading pandas ====\");\nimport &lt;module&gt;\nprint_imported_modules()",
    "crumbs": [
      "HPC Pipes",
      "Computational environments",
      "Cheat sheet"
    ]
  },
  {
    "objectID": "develop/cheat_sheet.html#r",
    "href": "develop/cheat_sheet.html#r",
    "title": "Cheat sheet",
    "section": "R",
    "text": "R\ninstall.packages()     # install a given package\nlibrary()              # loads a given package\nremove.packages()      # install a given package\nWe highly recommend using renv.",
    "crumbs": [
      "HPC Pipes",
      "Computational environments",
      "Cheat sheet"
    ]
  },
  {
    "objectID": "develop/cheat_sheet.html#conda",
    "href": "develop/cheat_sheet.html#conda",
    "title": "Cheat sheet",
    "section": "CONDA",
    "text": "CONDA\nAgnostic package manager\nconda info # conda installed version \nconda list # conda installed packages\nconda env list # list all environments\nconda create --name &lt;envname&gt; # create environment\nconda activate &lt;envname&gt; # activate environment\nconda deactivate \nconda config --show-sources # channel sources \nconda config --add channels &lt;channelname&gt;\nconda search &lt;pkgname&gt; --info # search for package\nconda install -c &lt;channelname&gt; pkg1 pkg2=1.2 # install packages from channel  \nconda uninstall pkg1 # uninstall package \nconda remove -n &lt;envname&gt; # conda remove \nconda env export --from-history &gt; &lt;myenv&gt;.yml # cross-platform compatinle export of env file\nconda env create -n &lt;envname&gt; --file &lt;myenv&gt;.yml",
    "crumbs": [
      "HPC Pipes",
      "Computational environments",
      "Cheat sheet"
    ]
  },
  {
    "objectID": "develop/cheat_sheet.html#dpkg",
    "href": "develop/cheat_sheet.html#dpkg",
    "title": "Cheat sheet",
    "section": "DPKG",
    "text": "DPKG\nLow-level tool for package management on Debian-based systems\ndpkg -S package_name # Seach   \ndpkg -I package.deb  # --info\ndpkg -L package_name # --list files installed by a package\ndpkg -i package.deb  # --install   [requires sudo]\ndpkg -r   # --remove:  Remove debian_package        [requires sudo]\ndpkg --get-selections    # List all the packages known by dpkg and whether they are installed or not\ndpkg --set-selections    # Set which package should be installed     [requires sudo]\ndpkg-query -W -f='${Package} == ${Version}\\n' # package version",
    "crumbs": [
      "HPC Pipes",
      "Computational environments",
      "Cheat sheet"
    ]
  },
  {
    "objectID": "develop/cheat_sheet.html#apt",
    "href": "develop/cheat_sheet.html#apt",
    "title": "Cheat sheet",
    "section": "APT",
    "text": "APT\nHigh-level tool for package management on Debian-based systems. It also handles package dependencies and repositories.\napt\napt update                      # Update the package list                  [sudo]\napt search &lt;package_name&gt;\napt show &lt;package_name&gt;\napt install &lt;package_name&gt;      # Install a debian package                 [sudo]\napt upgrade                     # Upgrade all your packages                [sudo]\napt clean                       # Delete all the .deb you've downloaded (save some space)\napt remove --purge &lt;package_name&gt; # Remove a debian package                  [sudo]\napt-cache search keyword # Search for packages containing a keyword.",
    "crumbs": [
      "HPC Pipes",
      "Computational environments",
      "Cheat sheet"
    ]
  },
  {
    "objectID": "develop/cheat_sheet.html#docker-commandline",
    "href": "develop/cheat_sheet.html#docker-commandline",
    "title": "Cheat sheet",
    "section": "Docker",
    "text": "Docker\nUseful commands to build and deploy a Docker image.\ndocker pull         # download image from a registry e.g. Docker Hub \ndocker images       # list all Docker images on your local machine\ndocker run -it &lt;image_name&gt;  # creates and starts a new container from the specified Docker image, -it flag means interactive virtual machine which is very useful during the development-phase for testing the container \ndocker build -t &lt;my-app&gt;  # build a Docker image form a Dockerfile and a context \ndocker tag &lt;my-app&gt; &lt;myrepo/my-app:v1.0&gt; # creates an new alias for an existing Docker image. Useful for versioning\ndocker push         # upload image from local machine to Docker registry \ndocker login        # logs you into a Docker register (after pull and push), username and pw needed\nOther common commands use in Dockerfiles to clean up the image and reduce its size:\n\n\nDockerfile\n\nRUN apt-get -y autoclean && \\\n    apt-get -y autoremove && \\\n    rm -rf /var/lib/apt/lists/* && \\\n    rm -rf /var/cache/apt/archives/*deb",
    "crumbs": [
      "HPC Pipes",
      "Computational environments",
      "Cheat sheet"
    ]
  },
  {
    "objectID": "workshop/pipes-ex-smk.html",
    "href": "workshop/pipes-ex-smk.html",
    "title": "Day 1 - Part 2",
    "section": "",
    "text": "Put your learning to the test with what you’ve covered so far.",
    "crumbs": [
      "Workshop",
      "HPC Pipes",
      "Day 1",
      "Day 1 - Part 2"
    ]
  },
  {
    "objectID": "workshop/pipes-ex-smk.html#a.-general-knowledge",
    "href": "workshop/pipes-ex-smk.html#a.-general-knowledge",
    "title": "Day 1 - Part 2",
    "section": "A. General knowledge",
    "text": "A. General knowledge\n\n\n\n\n\n\nI - General knowledge\n\n\n\n\n\n\n\nG.1. What role does a workflow manager play in computational research?\n\n Automating the execution of complex computational processes To manually execute each step of a computation Offering real-time and intuitive data analysis To minimize the reliance on specific software environments\n\nG.2.What is the primary drawback of using shell scripts for automating computations?\n\n Limited error handling and debugging capabilities Difficulty in integrating with notebooks Not compatible with all operating systems They re-run all the steps every time Insufficient support for parallel processing Complex and extensive coding for simple tasks\n\nG.3. What are the key features of workflow manager in computational research? (Several possible solutions)\n\n Executing tasks only when required Managing task dependencies Overseeing storage and resource allocation Providing intuitive graphical interfaces\n\nG.4. Workflow managers can run tasks (different) concurrently if there are no dependencies (True or False) TRUEFALSE\nG.5. A workflow manager can execute a single parallelized task on multiple nodes in a computing cluster (True or False) TRUEFALSE",
    "crumbs": [
      "Workshop",
      "HPC Pipes",
      "Day 1",
      "Day 1 - Part 2"
    ]
  },
  {
    "objectID": "workshop/pipes-ex-smk.html#b.-snakemake",
    "href": "workshop/pipes-ex-smk.html#b.-snakemake",
    "title": "Day 1 - Part 2",
    "section": "B. Snakemake",
    "text": "B. Snakemake\nIn this section, we will be working with a tabulated metadata file, samples_1kgp_test, which contains information about the samples from the 1000 Genomes Project. Each row represents a sample, and the columns contain various attributes for that sample (ID, population, super population, and sex). The file is designed to be simple and small, making it easy to work with and manipulate across tasks. However, the specifics of the data aren’t the primary focus.\n\n\n\n\n\n\nHow to run snakemake on UCloud\n\n\n\nFirst, mount the following two drives, use the setup.sh initialization file, and ask for 2 CPUs so we can run things in parallel:\n\nYourNameSurname#xxxx: save your results/files here.\n\nhpclab-workshop: this contains input files and scripts. You can read-only access from this directory (no write permissions).\n\nNext, activate snakemake environment.\nconda deactivate \n# make sure no env is active!\nconda activate snakemake \n\n\n\n\n\n\n\n\nAre you running the exercises locally?\n\n\n\nDownload the Snakefile and data required for this exercise using the links below if you are running things locally.\n\n Download data input \n Download Snakefile \n\n\n\nWe strongly recommend keeping the Snakemake documentation open for reference and guidance.\n\n\n\n\n\n\nII - Exploring rule invocation in Snakemake\n\n\n\n\n\n\n\nIn this exercise, we will explore how rules are invoked in a Snakemake workflow. The snakemake file is located at: /work/HPCLab_workshop/rules/process_1kgp.smk. Now follow these steps and answer the questions:\n\nOpen the snakefile, named process_1kgp.smk and try to understand every single line. If you request Snakemake to generate the file results/all_female.txt, what tasks will be executed and in what sequence?\nOpen a terminal and navigate to your personal drive cd /work/YourNameSurname#xxxx. Create a project directory called, for example, hpclab and make it your working directory. You should save all the results here!\nDry-run the workflow: Check the number of jobs that will be executed.\nQ.1. How many jobs will Snakemake run? \nRun the workflow from the directory hpclab (the one you just created on your personal drive). Use the name flag --snakefile &lt;/path/to/snakefile&gt;.smk --cores 1, or the abbreviated format -s &lt;/path/to/snakefile&gt;.smk -c 1.\nPlease verify that the output has been successfully generated and saved in your working directory (navigate through the project).\nQ.2. Has Snakemake created a subdirectory that didn’t previously exist? What is its name? \nQ.3. How many files with the extension *.tsv can you find in that subdirectory? \nDry-run the workflow again (from hpclab).\nQ.4. Would Snakemake run any jobs based on the results of the dry-run?\n\n No. There is nothing to be done. All requested files are present and up to date Yes. Reasons: input files updated by another job (all, combine, split_by_superpop) and output files have to be generated (combine, preprocess, split_by_superpop)\n\nRemove files starting with E in your results folder (“EAS.tsv” and “EUR.tsv”) and all_female.txt. Then, dry-run once more.\nQ.5. How many jobs will Snakemake run? \nUnder your working directory, create a folder named rules and copy the snakefile (process_1kgp.smk) to that folder so you can edit it! Then, open the file and remove lines 13-15. How else can you run the workflow but generate instead all_male.txt using only the command line?\n\n\nprocess_1kgp.smk\n\n13 rule all:\n14    input:\n15       expand(\"results/all_{sex}.txt\", sex=[\"female\"])\n\nQ.6. Tip: what is missing at the end of the command ( e.g. what should be added to ensure all_male.txt is generated)? snakemake -s process_1kgp.smk -c1 \nLet’s add a new rule that concatenates the two files you have generated (all_female.txt and all_male.txt) and save it into concatenated.txt. Remember, all files should be saved into the results subdir. Hint: cat file1.txt file2.txt &gt; output.txt\nRun the pipeline with your own version of the process_1kgp.smk file.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nTasks will be executed in this order preprocess (1), split_by_superpop (5), and combine (1).\n\n# 2. Create subdir \ncd /work/AlbaRefoyoMartínez#0753/\nmkdir hpclab\ncd hpclab\n# 3. dry run \nsnakemake -s /work/HPCLab_workshop/rules/process_1kgp.smk -n \n# 4. run the workflow \nsnakemake -s HPCLab_workshop/rules/process_1kgp.smk -c1 \n# 5. verify output \nls results/*\n# 6. dry run \nsnakemake -s /work/HPCLab_workshop/rules/process_1kgp.smk -n \n# 7. remove file(s) starting with E and the all_female.txt\nrm results/E*.tsv results/all_female.txt\n# 8. make a copy of the snakefile and remove the lines \nmkdir rules \ncp /work/HPCLab_workshop/rules/process_1kgp.smk rules/\n# 8. S5. rerun again with the &lt;name_output&gt;\nsnakemake -s rules/process_1kgp.smk -c1 results/all_male.txt \n# 9. create rule \nrule concat: \n   input: \n      \"results/all_female.txt\",\n      \"results/all_male.txt\"\n   output:\n      \"results/concatenated.txt\"\n   shell:\n      \"cat {input} &gt; {output}\"\n# 10. Run again \nsnakemake -s rules/process_1kgp.smk -c1 results/concatenated.txt",
    "crumbs": [
      "Workshop",
      "HPC Pipes",
      "Day 1",
      "Day 1 - Part 2"
    ]
  },
  {
    "objectID": "workshop/pipes-ex-envs.html",
    "href": "workshop/pipes-ex-envs.html",
    "title": "Day 1 - Part 1",
    "section": "",
    "text": "Put your learning to the test with what you’ve covered so far.",
    "crumbs": [
      "Workshop",
      "HPC Pipes",
      "Day 1",
      "Day 1 - Part 1"
    ]
  },
  {
    "objectID": "workshop/pipes-ex-envs.html#a.-general-environment",
    "href": "workshop/pipes-ex-envs.html#a.-general-environment",
    "title": "Day 1 - Part 1",
    "section": "A. General environment",
    "text": "A. General environment\n\n\n\n\n\n\nI - General knowledge\n\n\n\n\n\n\n\nG.1. What role does a workflow manager play in computational research??\n\n Automating the execution of complex computational processes To manually execute each step of a computation Offering real-time and intuitive data analysis To minimize the reliance on specific software environments",
    "crumbs": [
      "Workshop",
      "HPC Pipes",
      "Day 1",
      "Day 1 - Part 1"
    ]
  },
  {
    "objectID": "workshop/pipes-ex-envs.html#b.-conda",
    "href": "workshop/pipes-ex-envs.html#b.-conda",
    "title": "Day 1 - Part 1",
    "section": "B. Conda",
    "text": "B. Conda\nFor these exercises, make sure you have submitted a job to UCloud (resources set to 1 CPU and 1 hour). Unsure which command to use? Refer to the official Conda cheat sheet for guidance!\n\n\n\n\n\n\nWarning\n\n\n\n\nIs the hpc-lab environment not automatically activated? Restart the job following the instructions.\nDouble-check that your personal drive is properly mounted (e.g. ls). Can you see a directory with your name listed (e.g. NmeSurname#XXXX)? Restart the job following the instructions.\nIf your session gets disconnected while completing exercise: Part 2 - Build your conda environment, you’ll need to (re)deactivate hpclab-env, activate your own environment, and navigate back to your working directory!\n\n\n\n\n\n\n\n\n\nII - Understanding an existing environment\n\n\n\n\n\n\n\nType the answers with no white spaces!\nC.1. What is the version of conda \nC.2. List all environments available to you. What is the name of the active environment \nC.3. Active hpclab-env environment\nC.4. What is the version of the package cookiecutter (use a conda command) \nC.5. How many packages are available?  \nC.6. Export the environment specifications and save it to your personal drive (e.g.&lt;yourname-hpclab&gt;.yml)\nC.7. Deactivate the environment\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nconda info\nconda env list ; conda info --envs\nconda activate \nconda list cookiecutter\nconda env export \n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nC.1. conda info, v=24.9.2\nC.2. conda env list, the active one will be indicated with an asterisk (*).\nC.3. conda activate hpclab-env\nC.4. conda list cookiecutter, v=2.6.0\n# packages in environment at /work/HPCLab_workshop/miniconda3/envs/hpclab-env:\n#\n# Name                    Version                   Build  Channel\ncookiecutter              2.6.0              pyhca7485f_0    conda-forge\nC.5. conda list |grep -v '#' | wc -l, 72 packages\nC.6. conda env export --from-history &gt; &lt;yourname-hpclab&gt;.yml\nC.7. conda deactivate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nII - Bonus exercise\n\n\n\n\n\nInspect numpy version installed in the hpclab-env. Get the version and the full revision id (git_version). Hint: you can find this information in the version.py file! Check the installed libraries (lib) and look inside Python packages.\n/work/HPCLab_workshop/miniconda3/envs/hpclab-env/lib/python3.12/site-packages/numpy/version.py\n\nBonus.1. Numpy version \nBonus.2. Full ID version \n\n\n\n\n\ncat /work/HPCLab_workshop/miniconda3/envs/hpclab-env/lib/python3.12/site-packages/numpy/version.py\n\"\"\"\nModule to expose more detailed version info for the installed `numpy`\n\"\"\"\nversion = \"1.26.4\"\n__version__ = version\nfull_version = version\n\ngit_revision = \"9815c16f449e12915ef35a8255329ba26dacd5c0\"\nrelease = 'dev' not in version and '+' not in version\nshort_version = version.split(\"+\")[0]\n\n\n\n\n\n\n\n\n\n\nIII - Build your conda environment\n\n\n\n\n\n\n\nLet’s prepare to build our own environment. First, make sure to deactivate the hpclab-env environment (if you haven’t yet). Next, create a new directory by running mkdir envs in your mounted personal drive (e.g. AlbaRefoyoMartínez#0753). This directory will be used to save your environments.\n# Deactivate hpclab-env \nconda deactivate \n\n# Navigate to your personal drive and create a new environment directory with the command mkdir envs\ncd &lt;yourpersonal-drive&gt; # AlbaRefoyoMartínez#0753\nmkdir envs \nIs the full path of your envs directory /work/&lt;NameSurname#xxxx&gt;/envs? TRUEFALSE\nSince Miniconda is already pre-installed, you’re ready to create your first environment. Just follow these steps:\n\nCreate a new environment using --prefix PATH (for example, /work/&lt;YourNameSurname#xxxx&gt;/envs/&lt;name-env&gt;) and Proceed yes (y or enter). N.B. You can either name or prefix your environment. We will be using the prefix as miniconda is installed in a directory where you do not have writing privileges.\nCheck the available environments. How many environments do you see? \nActivate the environment\nCheck which python executable is being used (Hint: which python)? Does this executable belong to your conda environment?? TRUEFALSE\nSearch for snakemake https://anaconda.org/. Copy the installation command and run it in your activated conda env. This might take some time!\nExecute the help command: snakemake -h. Seems like snakemake is now installed in our environment!\nDoes the Python executable now belong to your conda environment?? TRUEFALSE\nIs your python version &gt;= 3.12 and packaged by conda-forge? TRUEFALSE. IF NOT, use one of the following commands: conda update -c conda-forge python or conda install -c conda-forge python=3.12.7\nLet’s search for bowtie2. Do you get an error? which channel is needed (Hint: search for bowtie2 in https://anaconda.org/)? \nLet’s add the missing channel: conda config --add channels xxxx. Hint: it is a repo that offers a wide range of bioinformatics software and tools.\nLet’s search for bowtie2 again. Is bowtie2 now available? TRUEFALSE\nExport the conda you have created and save the yml. Did you use --from-history command? TRUEFALSE\n\nDeactivate the environment\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nHere are some of the commands you need for the exercise.\n# use conda or mamba commands \nconda create --prefix /work/envs/&lt;myenv&gt;\nconda install --channel &lt;CHANNEL-NAME&gt; --name &lt;ENV-NAME&gt;\nconda env list # OR conda info --envs\nconda update -c conda-forge python\nconda activate &lt;ENV-NAME&gt;\nconda config --add channels bioconda \nconda search &lt;PKG-NAME&gt;\nconda deactivate \nconda env export &gt; env.yml\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nThe syntax to create a new environment is: conda create --prefix /work/&lt;NameSurname#xxxx&gt;/envs/&lt;test-env&gt;\nThere are 4 environments available: base, hpclab-env, snakemake, and the one you just created.\nconda activate /work/&lt;NameSurname#xxxx&gt;/envs/\nThe executable is located at /usr/bin/python in the system\nconda install bioconda::snakemake\nsnakemake is working!\nYes, now is in the bin of our env: /work/&lt;NameSurname#xxxx&gt;/envs/&lt;test-env&gt;/bin/python\nIt should be!\nconda search bowtie2. Yes. Go to anaconda.org and search for “bowtie2” to confirm it is available through conda and which software channel it is provided from. You will find that it is available via the “bioconda” channel: https://anaconda.org/bioconda/bowtie2.\nconda config --add channels bioconda. Use add, so that it has a higher priority. The syntax to install packages is: conda install --channel &lt;CHANNELNAME&gt; --name &lt;ENVNAME&gt; &lt;SOFTWARE&gt;\nYes!\nWhen using --from history flag, conda only exports explicitly installed packages (without the automatically resolved dependencies). This results in a simpler and smaller file which is more flexible but might lead to slightly different environments on different systems: conda env export --from-history &gt; environment.yml. Not using --from-history would ensure replicability but will also introduce packages that may not be compatible across platforms.\nconda deactivate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIV - Adapt a colleague’s Conda environment to fit your needs.\n\n\n\n\n\n\n\nIf you are running this exercise locally, download the environment YAML file first:\n\n Download environment file \n\n\nCreate a Conda environment using the provided ‘test-env.yml’ file (/work/HPCLab_workshop/data/test-env.yml). Please, use the flag --prefix for this purpose.\nOnce the environment is set up, activate it (do not forget to deactivate active environments)\nVerify the installed packages (you will also see other dependencies installed). Hint: check the test-env.yml to see which dependencies are included in this conda environment.\nUninstall the ‘cookiecutter’ package from your environment. Then, check again the list of all installed packages. Did you remove cookiecutter successfully? TRUEFALSE\nUpdate Python to the latest compatible version. Which version is that? Hint: check the prompt from the conda update command \nTry to install the latest numpy version using conda install numpy=2.1, could you successfully install it? TRUEFALSE\n\nOnce you are done, delete the environment.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nThe solution includes commands with my paths. Please modify them accordingly with the name of your personal drive.\n# 1. \nconda env create --file /work/HPCLab_workshop/data/test-env.yml --prefix /work/AlbaRefoyoMartínez#0753/envs/test-env\n# 2. \nconda deactivate; conda activate /work/AlbaRefoyoMartínez#0753/envs/test-env \n# 3.\nconda list \n# 4. \nconda remove cookiecutter ; conda list \n# 5. Check prompt from running this command \nconda update python # 3.7.16\n# OR conda list | grep python \n; conda list python \n# 6. \nconda env remove --prefix /work/AlbaRefoyoMartínez#0753/envs/test-env",
    "crumbs": [
      "Workshop",
      "HPC Pipes",
      "Day 1",
      "Day 1 - Part 1"
    ]
  },
  {
    "objectID": "workshop/pipes-ex-envs.html#c.-containers",
    "href": "workshop/pipes-ex-envs.html#c.-containers",
    "title": "Day 1 - Part 1",
    "section": "C. Containers",
    "text": "C. Containers\nApptainer/Singularity was specifically designed for HPC platforms; however, the UCloud server does not support it for regular users. As a result, container exercises will need to be run locally. We will be using Docker locally, as it is commonly employed for developing images. Please remember to install Docker Desktop, as noted on the Welcome page. For further guidance, refer to the official Docker cheat sheet.\n\n\n\n\n\n\nV - Running a docker image\n\n\n\n\n\n\n\nIn this exercise, we will utilize the fastmixture Docker image which is available on DockerHub, the repository for docker images. The aim is to understand how to run containerized software. To enhance the learning experience, we have chosen a simple genomics analysis, an efficient software tool, and a sample dataset. Focus on executing the commands, ensuring that this approach is easily adaptable to your own projects and software needs.\nFastmixture is a software designed to estimate ancestry proportions in unrelated individuals. It analyses genetic data to determine the percentage of various ancestral backgrounds present in a given individual. This tool is essential for understanding demographic histories and modeling population structure. You can view the results of running such analyses in the figure below.\nHere are some optional resources you might typically review before running the software (though not required for this exercise):\n\nSantander, Cindy G., Alba Refoyo Martinez, and Jonas Meisner. “Faster model-based estimation of ancestry proportions.” bioRxiv (2024): 2024-07 link to Preprint.\nSoftware GitHub repository link.\n\n\n\n\nAdmixture proportions\n\n\n\n Download toy data \n\n\nPull fastmixture image from DockerHUb using the following command:\n\n\nTerminal\n\ndocker pull albarema/fastmixture\n\nMake sure to pull the latest version.\nDownload and unzip the toy data (you may move the files to any preferred folder on your laptop)\nWhat is the command you need to run the container? Do not forget to mount the data (using the flag -v /path/toy:/path/mnt). Before executing the software, verify that the data has been correctly mounted (e.g. running the ls command inside the container).\nRun fastmixture software using the command below. We will set K to 3 because there are three populations (clusters) in our PCA analysis (exploratory analysis). Both --bfile and --out require the prefix of a filename, so do not include the file extension. If you have checked the toy folder, you will find the files named toy.data.*; therefore, use --bfile toy.data.\nIn fastmixture, the main arguments used in this exercise are:\n\n--K: Specifies the number of ancestral components, representing the sources in the mixture model.\n--seed: Sets the random seed to ensure reproducibility of the analysis across different runs.\n--bfile: prefix for PLINK files (.bed, .bim, .fam).\n--out: Prefix output name.\n\nfastmixture --bfile &lt;input.prefix&gt; --K 3 --threads 4 --seed 1 --out &lt;output.prefix&gt;\nDo you have the results in the folder on your local system? You should look for files named toy.fast.K3.s1.{ext}, where {ext}=[\"Q\", \"P\", \"log\"].\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nDocker\n\ndocker pull albarema/fastmixture  # Pull \n\ndocker run --cpus=8 -v `pwd`/toy/:/data/ albarema/fastmixture\n\nfastmixture --bfile data/toy.data --K 3 --out data/toy.fast --threads 8 # run the command \n\nApptainer on HPC / local machine: on your local machine you will need to modify lima.yml to make the current directory (pwd) writable. Alternatively, write the data out to /tmp/lima!\n\napptainer pull docker://albarema/fastmixture\napptainer run fastmixture_latest.sif fastmixture --version\n\n# on local machine (using LIMA)\ncd toy # from data folder \napptainer pull /tmp/lima/fastmixture_latest.sif docker:/albarema/fastmixture\napptainer run /tmp/lima/fastmixture_latest.sif fastmixture --bfile toy.data --K 3 --out toy.fast --threads 8\n\n\n\n\n\n\n\n\n\n\n\nContainers bonus exercises\nThere are several repositories where you can find containerised bioinformatics tools:\n\nDockerHub\nbiocontainers\nCloud sylabs\nSingularityHub\nGalaxy-singularity\n\nIf you want to run the first bonus exercise locally, use Docker. However, if you have access to an HPC platform, check the documentation to see if Singularity/Apptainer is installed; now would be a great time to give it a try. Don’t hesitate to ask for help if needed!\nIn the first bonus exercise, you will get to test other containerised tools:\n\nBLAST (local alignment search tool)\nBOWTIE2 (sequencing reads aligner to reference)\n\nAlternatively, explore one of the container image repositories and select a tool that you use regularly. Once you have pulled an image, we recommend starting by running the --help command, as all software have one. This command displays the help documentation of the program, verifying that our image is functioning correctly and includes the intended software.\n\n\n\n\n\n\nMounting is key!\n\n\n\nMake sure to mount a directory when running a container. This ensures that any data generated will be saved to your host system. If you do not mount a directory and use the --rm command, all generated data will be lost once the container stops.\n\nUse --rm flag to remove automatically the container once it stops running to avoid cluttering your system with stopped containers.\nUse --volume to mount data into the container (e.g. /data), for example, your working directory if you are already located in a project-specific dir.\n\n\n\n\n\n\n\n\n\nV - Bonus 1: Running other containers\n\n\n\n\n\n\nBLAST - Build a BLAST protein database from zebrafish protein sequences.\nZebrafish is a widely used model organism in genetics. This small dataset will facilitate quick results, allowing us to focus on how to run different bioinformatics tools so that you can easily adapt the commands in future projects.\nDocker: follow the steps in Running BLAST: https://biocontainers-edu.readthedocs.io/en/latest/running_example.html.\ndocker pull biocontainers/blast:2.2.31\ndocker run biocontainers/blast:2.2.31 blastp -help\nmkdir zebrafish-ref\ndocker run -v `pwd`/zebrafish-ref/:/data/ biocontainers/blast:2.2.31 curl -O ftp://ftp.ncbi.nih.gov/refseq/D_rerio/mRNA_Prot/zebrafish.1.protein.faa.gz\ndocker run -v `pwd`/zebrafish-ref/:/data/ biocontainers/blast:2.2.31 gunzip zebrafish.1.protein.faa.gz\ndocker run -v `pwd`/zebrafish-ref/:/data/ biocontainers/blast:2.2.31 makeblastdb -in zebrafish.1.protein.faa -dbtype prot\nApptainer: you might need to mount data folders if you test the container on your own computer! Ask if you an in doubt!\napptainer pull docker://biocontainers/blast:2.2.31\napptainer run blast_2.2.31.sif blastp -help\nmkdir zebrafish-ref\napptainer run blast_2.2.31.sif curl -O ftp://ftp.ncbi.nih.gov/refseq/D_rerio/mRNA_Prot/zebrafish.1.protein.faa.gz\napptainer run gunzip zebrafish.1.protein.faa.gz\napptainer run blast_2.2.31.sif makeblastdb -in zebrafish.1.protein.faa -dbtype prot\n\n\nBOWTIE2\nSingularity:\n# download the image\nsingularity pull images/bowtie2-2.5.4.sif https://depot.galaxyproject.org/singularity/bowtie2%3A2.5.4--he20e202_2\n\n# run the image: singularity run &lt;PATH-TO-IMAGE&gt; &lt;YOUR COMMANDS&gt;\nsingularity run images/bowtie2-2.5.4.sif bowtie2 --help\n\n\n\n\nAre you ready to build your own docker image? Let’s get started by building a Jupyter Notebook container! We’ll share several helpful tips to guide you through the process effectively. You might not be familiar with all the concepts, but google them if you’re uncertain.\n\n\n\n\n\n\nV - Bonus 2: Building a docker image and running your own container\n\n\n\n\n\n\nCreate a Dockerfile in a project-specific dir (e.g.: sandbox-debian-jupyter). We will add a command to clean up the package after installation which is a common practice to reduce the image size.\n\n\n\nDockerfile\n\nFROM debian:stable \n\nLABEL maintainer=\"Name Surname &lt;abd123@ku.dk&gt;\"\n\n# Update package list and install necessary packages\nRUN apt update \\\n    && apt install -y jupyter-notebook \\\n                      python3-matplotlib \\\n                      python3-pandas \\\n                      python3-numpy \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/* # cleanup tmp files created by apt\n\n# You may consider adding a working directory\nWORKDIR /notebooks\n\n\nFrom the project-specific dir, build the Docker image using, for example, docker build -t sandbox-debian-jupyter:1.0 .\nTesting the custom image. Let’s verify if the custom image functions as expected, by running the following command:\n\n\n\nTerminal\n\ndocker run --rm -p 8888:8888 --volume=$(pwd):/root sandbox-debian-jupyter:1.0 jupyter-notebook\n\nJupyter typically refuses to run as root or accept network connections by default. To address this, you need to either add --ip=0.0.0.0 --allow-root when starting Jupyter to the command above or uncomment the last line in the Dockerfile above (CMD [\"jupyter-notebook\", \"--ip=0.0.0.0\", \"--allow-root\"]). Test this before moving on!\nAlternatively, you can run the container with the flag --user=$(id -u):$(id -g) to ensure that files created in the container have matching user and group ownership with those on the host machine, preventing permission issues. However, this restricts the container from performing root-level operations.\nFor broader usability and security, it is advisable to create a non-root user (e.g. jovyan) within the Docker image by adding user setup commands to the Dockerfile (see below). This approach makes the image more user-friendly and avoids file ownership conflicts.\n\n\nDockerfile2\n\n##\n## ----- ADD CONTENT FROM Dockerfile HERE ----- \n## \n\n# Creating a group & user\nRUN addgroup --gid 1000 user && \\\n    adduser --uid 1000 --gid 1000 --gecos \"\" --disabled-password jovyan\n\n# Setting active user \nUSER jovyan\n\n# setting working directory \nWORKDIR /home/jovyan\n\n# let' automatically start Jupyter Notebook\nCMD [\"jupyter-notebook\", \"--ip=0.0.0.0\"]\n\n\n\n\n\n\n\nTip\n\n\n\n\nUse --rm flag to remove automatically the container once it stops running\nUse --volume to mount data into the container (e.g. /home/jovyan)\nUse --file flag to test two dockerfile versions (default:“PATH/Dockerfile”)\n\ndocker build -t sandbox-debian-jupyter:2.0 sandbox-debian-jupyter -f sandbox-debian-jupyter/Dockerfile2\n\n\nNow that we have fixed that problem, we will test A. using a port to launch a Jupyter Notebook (or Rstudio server) and B. starting a bash shell interactively.\n# Option A. Start jupyter-notebook or on the server \ndocker run --rm -p 8888:8888 --volume=$(pwd):/home/jovyan sandbox-debian-jupyter:2.0 \n\n# Option B. Start an interactive shells instead \ndocker run -it --rm --volume=$(pwd):/home/jovyan sandbox-debian-jupyter:2.0 /bin/bash\nIf you make changes to the container (incl. installing software), you need to commit the changes to a new image (docker commit).",
    "crumbs": [
      "Workshop",
      "HPC Pipes",
      "Day 1",
      "Day 1 - Part 1"
    ]
  },
  {
    "objectID": "workshop/pipes-ex-envs.html#blast---build-a-blast-protein-database-from-zebrafish-protein-sequences.",
    "href": "workshop/pipes-ex-envs.html#blast---build-a-blast-protein-database-from-zebrafish-protein-sequences.",
    "title": "Day 1 - Part 1",
    "section": "BLAST - Build a BLAST protein database from zebrafish protein sequences.",
    "text": "BLAST - Build a BLAST protein database from zebrafish protein sequences.\nZebrafish is a widely used model organism in genetics. This small dataset will facilitate quick results, allowing us to focus on how to run different bioinformatics tools so that you can easily adapt the commands in future projects.\nDocker: follow the steps in Running BLAST: https://biocontainers-edu.readthedocs.io/en/latest/running_example.html.\ndocker pull biocontainers/blast:2.2.31\ndocker run biocontainers/blast:2.2.31 blastp -help\nmkdir zebrafish-ref\ndocker run -v `pwd`/zebrafish-ref/:/data/ biocontainers/blast:2.2.31 curl -O ftp://ftp.ncbi.nih.gov/refseq/D_rerio/mRNA_Prot/zebrafish.1.protein.faa.gz\ndocker run -v `pwd`/zebrafish-ref/:/data/ biocontainers/blast:2.2.31 gunzip zebrafish.1.protein.faa.gz\ndocker run -v `pwd`/zebrafish-ref/:/data/ biocontainers/blast:2.2.31 makeblastdb -in zebrafish.1.protein.faa -dbtype prot\nApptainer: you might need to mount data folders if you test the container on your own computer! Ask if you an in doubt!\napptainer pull docker://biocontainers/blast:2.2.31\napptainer run blast_2.2.31.sif blastp -help\nmkdir zebrafish-ref\napptainer run blast_2.2.31.sif curl -O ftp://ftp.ncbi.nih.gov/refseq/D_rerio/mRNA_Prot/zebrafish.1.protein.faa.gz\napptainer run gunzip zebrafish.1.protein.faa.gz\napptainer run blast_2.2.31.sif makeblastdb -in zebrafish.1.protein.faa -dbtype prot",
    "crumbs": [
      "Workshop",
      "HPC Pipes",
      "Day 1",
      "Day 1 - Part 1"
    ]
  },
  {
    "objectID": "workshop/pipes-ex-envs.html#bowtie2",
    "href": "workshop/pipes-ex-envs.html#bowtie2",
    "title": "Day 1 - Part 1",
    "section": "BOWTIE2",
    "text": "BOWTIE2\nSingularity:\n# download the image\nsingularity pull images/bowtie2-2.5.4.sif https://depot.galaxyproject.org/singularity/bowtie2%3A2.5.4--he20e202_2\n\n# run the image: singularity run &lt;PATH-TO-IMAGE&gt; &lt;YOUR COMMANDS&gt;\nsingularity run images/bowtie2-2.5.4.sif bowtie2 --help",
    "crumbs": [
      "Workshop",
      "HPC Pipes",
      "Day 1",
      "Day 1 - Part 1"
    ]
  },
  {
    "objectID": "workshop/UCloud.html",
    "href": "workshop/UCloud.html",
    "title": "HPC Lab",
    "section": "",
    "text": "UCloud is a relatively new HPC platform accessible to all researchers and students at Danish universities (via a WAYF university login). It features a user-friendly graphical interface that simplifies project, user, and resource management. UCloud offers access to numerous tools via selectable apps and a variety of flexible compute resources. Check out UCloud’s extensive user docs here.\nIf you’d like a more detailed explanation and guide on UCloud, including how to navigate and understand the dashboard better, feel free to check our guidelines on how to access our sandbox app and get started: Sandbox guidelines.\n\n\nLog onto UCloud at the address http://cloud.sdu.dk using university credentials.\n\n\n\nWhen logged in, choose the project from the dashboard (top-right side) from which you would like to utilize compute resources. Every user has their personal workspace (My workspace). You can also provision your own project (check with your local DeiC office if you’re new to UCloud) or you can be invited to someone else’s project. If you’ve previously selected a project, it will be launched by default. If it’s your first time, you’ll be in your workspace.\n \nFor our workshops, you need to select Sandbox Workshop (see image below, top-right corner). This will allow us to provide a pre-configured environment with everything you need installed, along with access to our resources.\n\n\n\nDashboard\n\n\n\n\n\nSearch for the Coder application (you might be familiar with its original name, Visual Studio Code, shown in the image above) using the search bar. Make this app your favorite. You can see all available apps on the cloud by clicking on ‘Apps’ in the left panel, marked by the bag symbol.\nClick on the app button to get into the settings window. You will have to configure the settings as shown below before submitting a job. Follow these steps:\n\nName and version of the app to run (Coder - 1.93.1)\nJob settings: enter a job name (descriptive of the task), select the time (in hours) we want to use a node for (it can be modified afterward!), and the machine type (selecting a 1 CPU standard node with 6GB memory)\nAdd folders to access while in this job (select your own drive!)\n\n\nYou need access to our shared folder so that you can get the correct software environment and other material for the exercises\nYou need to select your own drive, to save any output from the exercise! You won’t have writing permissions on our shared drive.\n\n\nChoose the file shared/HPCLab_workshop/setup.sh. The initialization file,setup.sh, is on a different drive than yours.\nClick on the Submit button (and wait!)\n\n\n\n\nJob submission\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\nStep 2 sets up our computing resources for the period we want to work and can be customized as needed. However, only the time can be modified after submitting the job. You can always add extra time\nStep 4, you will only have access to the folders added during the job setup, and this cannot be modified after submission! Do not use our shared drive to create new files or for any other purpose! Please, make sure you save output files in your own drive (Member Files: YourName##1234)\nRefresh the website if the app doesn’t launch automatically.\n\n\n\n\n\n\n\n\n\nWhy is it necessary to mount two drives for the exercises?\n\n\n\n\nYourNameSurname#xxxx: save your results here or any other file.\nHPCLab_workshop: this is where the input and environment files are located; you can read from this directory but do not have write permissions.\n\n\n\n\nNow, open the interface and a new terminal. Then, check all conda environments available and activate hpclab-env as shown below.\n\n\n\n\nConda environment\n\n\n\n\n\n\n\n\nWhy do we use an Integrated Development Environment (IDE)?\n\n\n\n\nUser-friendly interface​\nSupport multiple languages (flexibility)​ and notebooks! ​\nIntegrated package management (seamlessly)​\nSupported version control integration and formatter tools​\nFile management and navigation ​\nDebugging tools (access to command-line tools)​\nTerminal integration ​",
    "crumbs": [
      "Workshop",
      "UCloud setup",
      "UCloud project workspace"
    ]
  },
  {
    "objectID": "workshop/UCloud.html#using-ucloud",
    "href": "workshop/UCloud.html#using-ucloud",
    "title": "HPC Lab",
    "section": "",
    "text": "UCloud is a relatively new HPC platform accessible to all researchers and students at Danish universities (via a WAYF university login). It features a user-friendly graphical interface that simplifies project, user, and resource management. UCloud offers access to numerous tools via selectable apps and a variety of flexible compute resources. Check out UCloud’s extensive user docs here.\nIf you’d like a more detailed explanation and guide on UCloud, including how to navigate and understand the dashboard better, feel free to check our guidelines on how to access our sandbox app and get started: Sandbox guidelines.\n\n\nLog onto UCloud at the address http://cloud.sdu.dk using university credentials.\n\n\n\nWhen logged in, choose the project from the dashboard (top-right side) from which you would like to utilize compute resources. Every user has their personal workspace (My workspace). You can also provision your own project (check with your local DeiC office if you’re new to UCloud) or you can be invited to someone else’s project. If you’ve previously selected a project, it will be launched by default. If it’s your first time, you’ll be in your workspace.\n \nFor our workshops, you need to select Sandbox Workshop (see image below, top-right corner). This will allow us to provide a pre-configured environment with everything you need installed, along with access to our resources.\n\n\n\nDashboard\n\n\n\n\n\nSearch for the Coder application (you might be familiar with its original name, Visual Studio Code, shown in the image above) using the search bar. Make this app your favorite. You can see all available apps on the cloud by clicking on ‘Apps’ in the left panel, marked by the bag symbol.\nClick on the app button to get into the settings window. You will have to configure the settings as shown below before submitting a job. Follow these steps:\n\nName and version of the app to run (Coder - 1.93.1)\nJob settings: enter a job name (descriptive of the task), select the time (in hours) we want to use a node for (it can be modified afterward!), and the machine type (selecting a 1 CPU standard node with 6GB memory)\nAdd folders to access while in this job (select your own drive!)\n\n\nYou need access to our shared folder so that you can get the correct software environment and other material for the exercises\nYou need to select your own drive, to save any output from the exercise! You won’t have writing permissions on our shared drive.\n\n\nChoose the file shared/HPCLab_workshop/setup.sh. The initialization file,setup.sh, is on a different drive than yours.\nClick on the Submit button (and wait!)\n\n\n\n\nJob submission\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\nStep 2 sets up our computing resources for the period we want to work and can be customized as needed. However, only the time can be modified after submitting the job. You can always add extra time\nStep 4, you will only have access to the folders added during the job setup, and this cannot be modified after submission! Do not use our shared drive to create new files or for any other purpose! Please, make sure you save output files in your own drive (Member Files: YourName##1234)\nRefresh the website if the app doesn’t launch automatically.\n\n\n\n\n\n\n\n\n\nWhy is it necessary to mount two drives for the exercises?\n\n\n\n\nYourNameSurname#xxxx: save your results here or any other file.\nHPCLab_workshop: this is where the input and environment files are located; you can read from this directory but do not have write permissions.\n\n\n\n\nNow, open the interface and a new terminal. Then, check all conda environments available and activate hpclab-env as shown below.\n\n\n\n\nConda environment\n\n\n\n\n\n\n\n\nWhy do we use an Integrated Development Environment (IDE)?\n\n\n\n\nUser-friendly interface​\nSupport multiple languages (flexibility)​ and notebooks! ​\nIntegrated package management (seamlessly)​\nSupported version control integration and formatter tools​\nFile management and navigation ​\nDebugging tools (access to command-line tools)​\nTerminal integration ​",
    "crumbs": [
      "Workshop",
      "UCloud setup",
      "UCloud project workspace"
    ]
  },
  {
    "objectID": "workshop/pipes-ex-smk-self.html",
    "href": "workshop/pipes-ex-smk-self.html",
    "title": "Day 2 - Part 3",
    "section": "",
    "text": "UCloud\n\n\n\n\nLocal Setup\n\n\n\n\n\n\nFollow these steps to run the exercises on UCloud. First, mount the following two drives, use the setup.sh initialization file, and ask for 2 CPUs so we can run things in parallel:\n\nYourNameSurname#xxxx: save your results/files here.\n\nhpclab-workshop: this contains input files and scripts. You can read-only access from this directory (no write permissions).\n\nNext, activate snakemake environment.\nconda deactivate \n# make sure no env is active!\nconda activate snakemake \nFinally, navigate to your personal drive and use the project directory you created yesterday to save all the output files from the exercises!\n\n\nYou can choose to run your tasks either locally or on an accessible HPC. Refer to the Welcome page for the software you need to install. Next, create a new environment using the YAML file, activate the conda environment, and download the data.\n Download environment file   Download data",
    "crumbs": [
      "Workshop",
      "HPC Pipes",
      "Day 2",
      "Day 2 - Part 3"
    ]
  },
  {
    "objectID": "workshop/pipes-ex-smk-self.html#a.-snakemake",
    "href": "workshop/pipes-ex-smk-self.html#a.-snakemake",
    "title": "Day 2 - Part 3",
    "section": "A. Snakemake",
    "text": "A. Snakemake\nIn this section, we will be working with a tabulated metadata file, samples_EB.tsv, which documents samples from a project on bearded seals (sp. Erignathus barbatus). Each row represents a sample, and the columns contain various attributes for that sample. The file is designed to be simple and small, making it easy to work with and manipulate across tasks. However, the specifics of the data aren’t the primary focus.\n\n\n\n\n\n\nWhat information is available in the metadata columns?\n\n\n\n\n\n\nID: Unique identifier for each sample.\ngenomic_species: Species classification based on genomic data.\nsubspecies: Specific subspecies classification.\nplot_subspecies: Visualization label for subspecies.\ncountry: Country of origin.\nlocation: Specific location of the sample within the country.\nAbbrevMap: Abbreviated mapping of location details.\nlat and long: Latitude and longitude coordinates.\nlon.utm and lat.utm: UTM (Universal Transverse Mercator) coordinates.\nsex: Sample gender.\nyear: Year the sample was collected.\ntissue: Tissue type used for sample collection.\ncoverage and samtools.coverage: Coverage metrics for sample analysis.\nnuDNA coverage and nuDNA coverage st.dev: Nuclear DNA coverage and its standard deviation.\ntotal_sites: Total genomic sites assessed.\nhomozygous_sites: Count of homozygous sites.\nproportion_heterozygous: Proportion of heterozygous sites.\n\n\n\n\n\n\n\n\n\n\nI - Build your pipeline from scratch\n\n\n\n\n\n\n\nYou will be working and running the workflow from hpclab, so make sure it’s set as your working directory. Launch the Coder application (by selecting Open interface).\n\n\n\nMy Coder app: ignore the additional subdirectories and files—they are from future exercises.\n\n\n\nPreparation step\n\nDirectory setup: Inside hpclab, create two subdirectories, scripts and rules.\n\nscripts: save all python/r external scripts.\nrules: save all your snakefiles here!\n\nFile creation: you will need to create two files for this exercise:\n\nOpen a new file named step1_wf.smk in the rules dir and include the following two lines of code:\n#!/usr/bin/env python \n  # -*- coding: utf-8 -*-\n\n\n\n\n\n\nwhat are those lines for?\n\n\n\n\n\n\n\n\n!/usr/bin/env python: tells the system to use the Python interpreter to execute the script\n-*- coding: utf-8 -*-: specifies the character encoding for the file as UTF-8\n\n\n\n\n\n\nFinally, create a Python file named preprocess.py in the scripts folder of your project directory. Add this code:\n\n\npreprocess.py\n\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport sys\nimport pandas as pd\n\ndef preprocess(input_file, output_file):\n    df = pd.read_csv(input_file, sep='\\t')  # Read the input TSV file\n    df_cleaned = df.dropna()  # Remove rows with missing values\n    df_cleaned.to_csv(output_file, sep='\\t', index=False)  # Save the cleaned data to output file\n\nif __name__ == \"__main__\":\n    input_file = sys.argv[1]  # First argument: input file\n    output_file = sys.argv[2]  # Second argument: output file\n    preprocess(input_file, output_file)\n\n\n\nYour directory layout should look like this:\n# Project structure  \nmypersonaldrive/\n│\n├── envs/\n│   └── [environment files here] \n│\n└── hpclab/\n    ├── results/\n    │   ├── EAS.txt \n    │   ...\n    ├── rules/\n    │   ├── step1_wf.smk \n    └── scripts \n        └── preprocess.py\n\n\nPart 1: Writing the first rule\nLet’s define a very simple workflow of one task (S.1). With step1_wf.smk open, follow these steps to create a rule called preprocess:\n\nName of the rule: preprocess\n\nAvoid using wildcards for now\nDefine inputs and outputs:\n\nInput: metadata file (‘/work/HPCLab_workshop/data/samples_EB.tsv’)\nOutput: preprocessed file (‘results/samples_EB_cleaned.tsv’)\n\nUse the shell directive to run a Python script that removes samples with missing values (NA)\n\n\n\nstep1_wf.smk\n\nrule firstrule:\n  input: \"/path/to/input\"\n  output: \"/path/to/output\"\n  shell: \n    \"somecommand {output}\"\n\nYour rule should replicate the behavior of the bash script shown below.\n\n\nTerminal\n\nINFILE=\"/work/HPCLab_workshop/data/samples_EB.tsv\"\nOUTPUT=\"/work/&lt;nameSurname#xxx&gt;/hpclab/results_eb/samples_EB_cleaned.tsv\"\n\n# This is what the pipeline's task is:\npython scripts/preprocess.py $INFILE $OUTFILE\n\n\nRunning the workflow\nAll set! You’re ready to run the workflow now. As a best practice, we recommend starting with a dry-run to ensure everything works correctly before executing the full workflow:\nsnakemake -s rules/step1_wf.smk -n\nIf there are no errors, go ahead and run it!. Since this workflow contains only one rule, defining the target (e.g. through rule all or using the name of the rule) is unnecessary—snakemake will execute the preprocess rule directly with the following command:\nsnakemake -s rules/step1_wf.smk -c1\nDid the pipeline create the expected output file in the results folder? TRUEFALSE\n\n\n\n\n\n\nSolution S.1\n\n\n\n\n\n\n\n\n\nstep1_wf.smk\n\nrule preprocess:\n  input:\n    \"/work/HPCLab_workshop/data/samples_EB.tsv\"\n  output:\n    \"results/samples_EB_cleaned.tsv\"\n  shell:\n    \"\"\"\n    python scripts/preprocess.py {input} {output}\n    \"\"\"\n\n\n\n\n\n\n\n\n\nPart 2: Generalising the rule\nWhat if we wanted to run this same rule for the dataset used in the previous Snakemake exercise? How could we modify the rule (and so, the workflow) to accommodate other datasets?\n\n\n\n\n\n\nDetails about the previous dataset\n\n\n\n\n\n\n\n\nInput: “/work/HPCLab_workshop/data/samples_1kgp.tsv”\nOutput: “results/samples_1kgp_cleaned.tsv”\n\n\n\n\n\n\nYou can leverage Snakemake’s customization by introducing variables through either params, wildcards, or other mechanisms. By using these variables, you can dynamically adjust input files, tool settings, or even paths, making it easier to switch datasets or configurations without manually editing the rule each time.\n\nWildcards\nWildcards are dynamic placeholders in rule definitions that allow for flexible input or output file names. You define them in your rules, and their actual values are determined when the workflow is executed, enabling versatile file handling across different workflow runs.\nCould you modify the previous rule so that you can run the workflow for different datasets? Not sure how?\n\nDefine a wildcard named dataset which represents the variable parts of your input or output (e.g. EB in this case)\nDry-run, is it working snakemake -s rules/step1_wf.smk -np results/samples_EB_cleaned.tsv\n\nDoes Snakemake’s dry-run command return the message “Nothing to be done”? TRUEFALSE\nLet’s explore other commands! You can force a run in Snakemake even if the output file already exists by using the --force option in your command. Try it out!\nDid Snakemake rerun the workflow and successfully produce the output file again? TRUEFALSE\n\n\n\n\n\n\nSolution S.2\n\n\n\n\n\n\n\n\n\nstep1_wf.smk\n\n\nrule preprocess:\n  input:\n    \"/work/HPCLab_workshop/data/samples_{dataset}.tsv\"\n  output:\n    tsv=\"results/samples_{dataset}_cleaned.tsv\"\n  shell:\n    \"\"\"\n    python scripts/preprocess.py {input} {output}\n    \"\"\"\n\n\n\nTerminal\n\n# dry-run \nsnakemake -s rules/step1_wf.smk -n results/samples_EB_cleaned.tsv\n# force re-running preprocess step \nsnakemake -s rules/step1_wf.smk --force -c1 results/samples_EB_cleaned.tsv\n\n\n\n\n\n\n\n\nPart 3: Helper functions and target rules\nBy default, Snakemake always wants to execute the first rule in the Snakefile. This gives rise to “pseudo-rules” at the beginning of the file, which are used to define targets. Typically, this rule is named all. Let’s look at an example:\n\n\noption1.smk\n\nrule all:\n    input:\n        \"ds1.file.A.gz\",\n        \"ds2.file.A.gz\",\n        \"ds3.file.A.gz\"\n\nrule file_conversion:\n    input:\n        \"{dataset}/file.{group}.txt\"\n    output:\n        \"{dataset}/file.{group}.gz\"\n    shell:\n        \"somecommand {input} &gt; {output}\"\n\nAlternatively, you can use a Python list to define the inputs:\n\n\noption2.smk\n\nDATASETS = [\"ds1\", \"ds2\", \"ds3\"]\n\nrule all:\n    input:\n        [f\"{dataset}/file.A.gz\" for dataset in DATASETS]\n\nBut how does it look if we use a helper function, e.g. expand():\n\n\noption3.smk\n\nDATASETS=[\"ds1\", \"ds2\", \"ds3\"]\n\nrule all:\n    input:\n        expand(\"{dataset}/file.A.gz\", dataset=DATASETS)\n\nIn all cases, for each dataset in the Python list DATASETS, the file {dataset}/file.A.gz is requested. Snakemake, will recognise that these files can be created by multiple applications (executions) of the rule file_conversion.\nIt’s time for you to define the target rule rule all within the Snakefile and utilize helper functions. Instead of using a Python list comprehension to specify input files please use expand() function!\n\nDeclare the variable DATASETS at the top\nDefine rule all and use the expand() function to specify the input files (our targets, corresponding to the last output)\nIf you need it, have a look at Snakemake documentation\n\nDoes your Snakefile resemble the one in the solution above? TRUEFALSE\nDo you get any error if you do a dry-run? TRUEFALSE\n\n\n\n\n\n\nSolution S.3\n\n\n\n\n\n\n\n\n\nstep1_wf.smk\n\n\nDATASETS = [\"EB\", \"1kgp\"]\n\nrule all: \n    input:\n        expand(\"results/samples_{dataset}_cleaned.tsv\", dataset=DATASETS)\n\nrule preprocess:\n    input:\n        \"/work/HPCLab_workshop/data/samples_{dataset}.tsv\"\n    output:\n        tsv=\"results/samples_{dataset}_cleaned.tsv\"\n    shell:\n        \"\"\"\n        python scripts/preprocess.py {input} {output}\n        \"\"\"\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s assume that step1_wf.smk includes all the necessary steps for preprocessing our data. Now, let’s add other components to the workflow to further process the data. The aim of the next exercise is to modularise our workflow and decorate the workflow using some advanced functionality.\n\n\n\n\n\n\nII - Extend your pipeline\n\n\n\n\n\n\n\n\nPart 4: Modularization\nLet’s walk through an example of modularization. In the example below, the statement module step1_workflow registers the external workflow as a module by specifying the path to its snakefile. The second statement declares that all rules of those modules are available in the current workflow, except for ruleExtra. The as step1_* at the end renames all those rules with a common prefix (e.g. rule preprocess would become rule step1_preprocess). This renaming is optional but very useful for avoiding rule name conflicts (e.g. we could have another rule on this new snakefile named preprocess). Want to learn more? Click here.\n\n\nexample.smk\n\nmodule step1_workflow:\n    snakefile:\n        # you can use plain paths, URL or host code providers (e.g. GitHub)\n        \"path/to/snakefile\" # path relative to current snakefile \n\nuse rule * from step1_workflow exclude ruleExtra as step1_*\n\nNow it’s time to implement it following these steps:\n\nFile creation: open a new file named step2_wf.smk and define the external workflow step1_wf.smk as a module, making all rules available by explicitly importing them and renaming them as step1_*. Hint: the example above.\nDelete the output file results/samples_EB_cleaned.tsv to test the newly implemented feature.\nDry-run step2_wf.smk using the -p command (--printshellcmds)\n\n\n\n\n\n\n\nSolution S.4\n\n\n\n\n\n\n\n\n\nstep2_wf.smk\n\n#!/usr/bin/env python \n# -*- coding: utf-8 -*-\n\nmodule step1_wf:\n  snakefile:\n    \"step1_wf.smk\"\n\nuse rule * from step1_wf as step1_*\n\nDid you run these commands:\n\n\nTerminal\n\n# rm file\nrm results/samples_EB_cleaned.tsv\n# dry-run \nsnakemake -s rules/step2_wf.smk -np\n\nBuilding DAG of jobs...\nJob stats:\njob                 count\n----------------  -------\nstep1_all               1\nstep1_preprocess        1\ntotal                   2\n\nrule step1_preprocess:\n    input: /work/HPCLab_workshop/data/samples_EB.tsv\n    output: results/samples_EB_cleaned.tsv\n    jobid: 1\n    reason: Missing output files: results/samples_EB_cleaned.tsv\n    wildcards: dataset=EB\n    resources: tmpdir=&lt;TBD&gt;\n\n        python scripts/preprocess.py /work/HPCLab_workshop/data/samples_EB.tsv results/samples_EB_cleaned.tsv\n        \n\nrule step1_all:\n    input: results/samples_EB_cleaned.tsv, results/samples_1kgp_cleaned.tsv\n    jobid: 0\n    reason: Input files updated by another job: results/samples_EB_cleaned.tsv\n    resources: tmpdir=&lt;TBD&gt;\n\n\n\n\n\nShell commands can sometimes involve more than just input and output files along with static flags. In particular, there may be situations where additional parameters must be configured based on the wildcard values specific to a job. To facilitate this, Snakemake provides the params directive, enabling you to define custom parameters for your rules.\nMany software tools accept arguments to set specific thresholds. For instance, one such parameter could be the p-value threshold (e.g. default value of 0.05). However, you might need to change the significance level depending on the analysis, such as when correcting for multiple testing or when a stricter criterion is needed. You can see more examples in the documentation.\n\n\nParams\nLet’s add a new rule named filter_year in step2_wf.smk that replicates the behavior of the bash script below:\n\n\nfilter_year.sh\n\nINFILE=\"results/samples_EB_cleaned.tsv\"\nOUTFILE=\"results/samples_EB_filtered.tsv\"\nCUTOFF=2000\n\n# Python script filters the metadata based on the year the samples were collected \npython scripts/filter_year.py -i $INFILE -o $OUTFILE -y $CUTOFF\n\nPlease, use named inputs (e.g. meta for the input and fi for the output). Named inputs are particularly useful when multiple files are needed for running a software tool.\nYou can find the Python file on UCloud: PATH=\"/work/HPCLab_workshop/scripts/filter_year.py\". Copy it to your scripts directory.\n\n\n\n\n\n\nYou want to have a look at the Python code?\n\n\n\n\n\n\n\n\n\nfilter_year.py\n\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport pandas as pd\nimport numpy as np\nimport argparse\n\ndef filter_year(input_file, output_file, year):\n    # Read the input file into a DataFrame\n    df = pd.read_csv(input_file, sep='\\t')\n\n    # Filter the DataFrame based on the 'year' column\n    df_filtered = df[df['year'] &gt;= year]\n\n    # Save the filtered DataFrame to the output file\n    df_filtered.to_csv(output_file, sep='\\t', index=False)\n\nif __name__ == \"__main__\":\n    # Set up argument parsing\n    parser = argparse.ArgumentParser(description=\"Filter input data by year.\")\n    parser.add_argument('-i', '--input', required=True, help='Input file path')\n    parser.add_argument('-o', '--output', required=True, help='Output file path')\n    parser.add_argument('-y', '--year', type=int, required=True, help='Year to filter by')\n\n    # Parse the command-line arguments\n    args = parser.parse_args()\n\n    # Call the filter function with the parsed arguments\n    filter_year(args.input, args.output, args.year)\n\n\n\n\n\n\nNow, run step2_wf.smk. How many samples got filtered out? Compared the *_cleaned.tsv vs the *_filtered.tsv. \n\n\n\n\n\n\nSolution S.5\n\n\n\n\n\n\n\nrule filter_year:\n    input:\n        meta=\"results/samples_EB_cleaned.tsv\"\n    output:\n        fi=\"results/samples_EB_filtered.tsv\"\n    params: \n        cutoff=2000\n    shell:\n        \"\"\"\n        python scripts/filter_year.py -i {input.meta} -o {output.fi} -y {params.cutoff}\n        \"\"\"\n\n\n\n\n\nAlmost done! We want our workflow to be as customizable as possible so that it can easily be adapted to new data. For this purpose, snakemake provides a config file mechanism.\n\n\nConfig files\nModify your Snakemake workflow to include a YAML configuration file and use it within the filter_year rule. Configuration files are typically located at the top level of the project directory, e.g. hpclab (often alongside the Snakefile). While you usually reuse a pipeline, it’s advisable to create a new config file for each new dataset or project. This approach helps isolate project-specific configurations and avoids modifying the original pipeline.\n\n\n\n\n\n\nNot familiar with YAML format? Read this!\n\n\n\n\n\n\n\nWhitespace is a key part of YAML’s formatting. Unless otherwise indicated, newlines indicate the end of a field. You structure a YAML document with indentation. The indentation level can be one or more spaces. The specification forbids tabs because tools treat them differently.\n\n\n\n\n\n\nDefine the YAML config file\n\n\n\nconfig_step2.yml\n\nyear:\n  cutoff: 2000\n\n\nDefine the config file at the top of your Snakefile\n\n\n\nstep2_wf.smk\n\nconfigfile:\"/path/to/config_step2.yml\"\n\n\nModify the filter_year rule\n\n\n\nstep2_wf.smk\n\nrule ...:\n  params: \n    cutoff=config['year']['cutoff']\n\nVerify that the workflow functions correctly by testing with various cutoff values. Use the --forceall flag.\n\n\n\n\n\n\n\n\n\n\n\n\nBonus exercise\n\n\n\n\n\n\nAdd a log directive to the filter_year rule to collect stderr output.\nHint: 2&gt; {log}. It is best practice to store all log files in a subdirectory logs/, prefixed by the rule or tool name.\nRerun the workflow and see how the log files are created.\nDo the same using the benchmark directive.",
    "crumbs": [
      "Workshop",
      "HPC Pipes",
      "Day 2",
      "Day 2 - Part 3"
    ]
  },
  {
    "objectID": "resources/slides/slides_git.html#generate-ssh-key-pair",
    "href": "resources/slides/slides_git.html#generate-ssh-key-pair",
    "title": "Git and Github",
    "section": "Generate SSH Key Pair",
    "text": "Generate SSH Key Pair\nNavigate to the location where all SSH keys are stored to generate a new one. Use a file name that describes what the key is for, and do not enter a passphrase for “id_UCloud”.\n# For Mac/Linux\ncd ~/.ssh \n# For Windows\ncd C:\\Users\\&lt;YourUsername&gt;\\.ssh\n\nssh-keygen -t ed25519 \nGenerating public/private ed25519 key pair.\nEnter file in which to save the key (/Users/gsd818/.ssh/id_ed25519): id_UCloud\nYour identification has been saved in id_UCloud\nYour public key has been saved in id_UCloud.pub\n...\nAdd the key to your system:\nssh-add id_UCloud\nentity added: id_UCloud (gsd818@SUN1029429)",
    "crumbs": [
      "Workshop",
      "UCloud setup",
      "Git/GitHub on UCloud"
    ]
  },
  {
    "objectID": "resources/slides/slides_git.html#connecting-git-and-github",
    "href": "resources/slides/slides_git.html#connecting-git-and-github",
    "title": "Git and Github",
    "section": "Connecting Git and GitHub",
    "text": "Connecting Git and GitHub\nFirst, we need to create a classic token. Go to your profile and on the left-side menu select Settings &gt; Developer settings (at the bottom) &gt; Personal access tokens &gt; Tokens (classic) &gt; Generate new token. Do not set an expiration date and name it however you like (e.g., ucloud hpc). Please, remember to save the generated token somewhere locally and safely. You won’t be able to see it again.\nThen, we will generate an SSH key on UCloud following these steps 1. Submit a new job and mount your personal drive 2. Change directory to your personal drive (e.g., cd /work/myNameXXX/) 3. Create a new folder named ssh 4. Generate a new key (no need for paraphrase) ssh-keygen -t ed25519 and save it as id_repo1 5. Copy the public key (id_repo1.pub) 6. Add the key to the repository you have forked from us on GitHub 7. Modify the initialization bash script to include this code\neval `ssh-agent -s`\nssh-add /work/ssh/id_repo1\ngit config --global user.name \"add here your name\"\ngit config --global user.email \"add here your email\"\nbash -i \nYou can now git commit and push using the token as the password to save all changes to your repository.",
    "crumbs": [
      "Workshop",
      "UCloud setup",
      "Git/GitHub on UCloud"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to HPC-Lab Hub",
    "section": "",
    "text": "Welcome to HPC-Lab Hub\n\n\nNote: Actively being developed\nHigh Performance Computing (HPC) plays a crucial role for researchers by offering the computational speed and power needed to manage large and complex data sets, perform simulations, and address intricate problems that would be impractical or too time-consuming with standard computing methods.\nKnowing which HPC resources are accessible and how to use them efficiently is essential for researchers. Making the most of these resources can significantly expedite research and drive innovation. By becoming proficient with the available tools and technologies, researchers can address complex challenges, analyze extensive data sets, and execute advanced simulations with increased speed and accuracy. This module provides essential knowledge on HPC resources and best practices for their utilization.\nThis module offers content for three distinct courses:\n\nHPC Launch: Foundations on HPC and essential knowledge on national HPC resources\nHPC Pipes: Best practices for using workflow management systems and computational environments with HPC\nHPC ML (Machine Learning): Insights into applying HPC for machine learning tasks, including model training, data analysis, and optimization techniques.\n\nBy the end of all the modules, you will gain practical skills in promoting reproducibility through comprehensive training in HPC resource management, workflow pipelines, and computing environments.\n\n\n\n\n\n\nGeneral Course Goals\n\n\n\nBy the end of this workshop, you should be able to apply the following concepts in the context of Next Generation Sequencing data:\n\nUnderstand the Importance of Research Data Management (RDM)\nMake your data analysis and workflows reproducible and FAIR\nMake FAIR environment using conda or Docker\n\n\n\n\n\n\n\n\n\nHPC best practices\n\n\n\nWe offer in-person workshops, keep an eye on the upcoming events on the Sandbox website.\n\n\n\nAcknowledgements\nOur interactive exercises are developed using the R packaged developed by Barr and DeBruine (2023).\n\n\n\n\n\n\nReferences\n\nBarr, Dale, and Lisa DeBruine. 2023. Webexercises: Create Interactive Web Exercises in r Markdown (Formerly Webex). https://github.com/psyteachr/webexercises.\n\n\nWagner, Adina S, Laura K Waite, Małgorzata Wierzba, Felix Hoffstaedter, Alexander Q Waite, Benjamin Poldrack, Simon B Eickhoff, and Michael Hanke. 2022. “FAIRly Big: A Framework for Computationally Reproducible Processing of Large-Scale Data.” Scientific Data 9 (1): 80.\n\n\nWratten, Laura, Andreas Wilm, and Jonathan Göke. 2021. “Reproducible, Scalable, and Shareable Analysis Pipelines with Bioinformatics Workflow Managers.” Nature Methods 18 (10): 1161–68.\n\nCopyrightCC-BY-SA 4.0 license"
  },
  {
    "objectID": "resources/slides/slides_rsync_tmux.html#syncronizations-downloads-multiple-terminals",
    "href": "resources/slides/slides_rsync_tmux.html#syncronizations-downloads-multiple-terminals",
    "title": "Use GenomeDK",
    "section": "Syncronizations, downloads, multiple terminals",
    "text": "Syncronizations, downloads, multiple terminals\n\nHow to download/update incrementally using rsync\nUse rsync to create backups and versioning\nCreate and navigate multiple sessions with tmux\nLaunch parallel background downloads with tmux"
  },
  {
    "objectID": "resources/slides/slides_rsync_tmux.html#transfer-and-sync-with-rsync",
    "href": "resources/slides/slides_rsync_tmux.html#transfer-and-sync-with-rsync",
    "title": "Use GenomeDK",
    "section": "transfer and sync with rsync",
    "text": "transfer and sync with rsync\ntmux is a very versatile tool for\n\ntransfering from remote to local host (and viceversa)\ncopying from local to local host (e.g. data backups/sync)\ntransfering only files which has changed from last copy (incremental copy)\n\n\n\n\n\n\n\nWarning\n\n\nrsync cannot make a transfer between two remote hosts, e.g. running from your PC to transfer data between GenomeDK and Computerome.\n\n\n\nLots of options you can find in the manual (would require a workshop only for that)\n\n rsync manual"
  },
  {
    "objectID": "resources/slides/slides_rsync_tmux.html#exercise",
    "href": "resources/slides/slides_rsync_tmux.html#exercise",
    "title": "Use GenomeDK",
    "section": "Exercise",
    "text": "Exercise\nLog into UCloud. Create anywhere you prefere a folder called sandboxWS containing rsync/data\nmkdir -p sandboxWS/rsync/data\ncd sandboxWS/rsync\nCreate 100 files with extensions fastq and log in the data folder\ntouch data/file{1..100}.fastq data/file{1..100}.log"
  },
  {
    "objectID": "resources/slides/slides_rsync_tmux.html#multiple-terminals-with-tmux",
    "href": "resources/slides/slides_rsync_tmux.html#multiple-terminals-with-tmux",
    "title": "Use GenomeDK",
    "section": "multiple terminals with tmux",
    "text": "multiple terminals with tmux\nWith tmux you can\n\nstart a server with multiple sessions\neach session containing one or more windows with multiple terminals (panes)\neach terminal run simultaneously and be accessed (attached) or exited from (detached)\nthe tmux server keeps runninng without a logged user"
  },
  {
    "objectID": "resources/slides/slides_rsync_tmux.html#exercise-1",
    "href": "resources/slides/slides_rsync_tmux.html#exercise-1",
    "title": "Use GenomeDK",
    "section": "Exercise",
    "text": "Exercise\ntmux was a keyboard-only software. But you can set it up also to change windows and panes with the mouse. Simply write this setting on the configuration file:\necho \"set -g mouse\" &gt;&gt; ~/.tmux.conf\nYou can start a tmux session anywhere. It is easier to navigate sessions giving them a name. For example start a session called example1:\ntmux new -s example1"
  },
  {
    "objectID": "resources/slides/slides_conda.html#sandbox-project-workspace",
    "href": "resources/slides/slides_conda.html#sandbox-project-workspace",
    "title": "Package managers",
    "section": "Sandbox project workspace",
    "text": "Sandbox project workspace\nClick the invite link below to accept our the invitation to the Sandbox workspace:\n \n\n Ucloud unvite to Sandbox workspace",
    "crumbs": [
      "Workshop",
      "UCloud setup",
      "Conda on UCloud"
    ]
  },
  {
    "objectID": "resources/slides/slides_conda.html#software-management-on-ucloud",
    "href": "resources/slides/slides_conda.html#software-management-on-ucloud",
    "title": "Package managers",
    "section": "Software management on UCloud",
    "text": "Software management on UCloud\n\nEvery app comes with its pre-defined installed software on UCloud\nThe Terminal app, has no preinstalled software\nYou can install and manage your software and its dependencies using virtual environments\n\nVirtual environments\nEach project needs specific software versions dependent on each other for reproducibility - without interferring with other projects.\n\n\n\n\n\n\n\nDefinition\n\n\nA virtual environment keeps project-specific softwares and their dependencies separated\nA package manager is a software that can retrieve, download, install, upgrade packages easily and reliably",
    "crumbs": [
      "Workshop",
      "UCloud setup",
      "Conda on UCloud"
    ]
  },
  {
    "objectID": "resources/slides/slides_conda.html#conda",
    "href": "resources/slides/slides_conda.html#conda",
    "title": "Package managers",
    "section": "Conda",
    "text": "Conda\nConda is both a virtual environment and a package manager.\n\neasy to use and understand\ncan handle quite big environments\nenvironments are easily shareable\na large archive (Anaconda) of packages\nactive community of people archiving their packages on Anaconda",
    "crumbs": [
      "Workshop",
      "UCloud setup",
      "Conda on UCloud"
    ]
  },
  {
    "objectID": "workshop/pipes-ex-smk-fast.html",
    "href": "workshop/pipes-ex-smk-fast.html",
    "title": "Day 2 - Part 4",
    "section": "",
    "text": "UCloud\n\n\n\n\nLocal Setup\n\n\n\n\n\n\nFollow these steps to run the exercises on UCloud. First, mount the following two drives, use the setup.sh initialization file, and ask for 2 CPUs so we can run things in parallel:\n\nYourNameSurname#xxxx: save your results/files here.\nhpclab-workshop: this contains input files and scripts. You can read-only access from this directory (no write permissions).\n\nNext, activate snakemake environment.\n# make sure no other env is active!\nconda activate snakemake \nFinally, navigate to your personal drive and use the project directory you created yesterday hpclab to create a new subdir fastmixture and save here all files from this exercise (Snakefile, config.yml, output file, etc.).\n\n\nYou can choose to run your tasks either locally or on an accessible HPC. Refer to the Welcome page for the software you need to install. Next, create a new environment using the YAML file, activate the conda environment, and download the data.\n Download environment file   Download data \n\n\n\n\n\nDay 2 - Part 4\nCreate workflows that are both reproducible and easy to distribute. Use the config.yml file to define all variable values, and rely on separate environment YAML files for tasks that use software outside of the provided Conda environment.\n\n\n\n\n\n\nII - Admixture analysis pipeline\n\n\n\n\n\n\n\nIn this exercise, you will run the software fastmixture as a Snakemake pipeline by converting the following bash script. Pay attention to the prefix for input and output files and the specified number of cores.\n\nFile paths for UCloud users:\n\nRscript: /work/HPCLab_workshop/scripts/plotAdmixture.R\nInput PLINK files: /work/HPCLab_workshop/data/plink_sim/sim.small.*\n\n\nfastmixture is not pre-installed in the snakemake environment and we don’t want to include it for the purpose of this exercise. You will have to git clone the GitHub repository. There are two options on how to proceed:\n\nUse the provided environment.yml file to create a new environment, which you are going to name fastmixture. Then, use the full path to the fastmixture env in the workflow. Is this the best approach? The workflow will rely on this environment being present on a new system where the workflow is executed. Referring to an existing environment can however be useful during development, e.g. when a certain software package is developed in parallel to a workflow that uses it.\nProvide the path to environment.yml within the workflow. Snakemake will create the env for you which takes some time but will only do this once. This is recommended and preferred for reproducibility reasons.\n\nEither way, use the conda directive!\nHave a look at the fastmixture.sh script below. How many different wildcards would you need? Recommended: Specify the values of the wildcard variables in a separate config file.\n\n\nfastmixture.sh\n\n### Run fastmixture for K=2,3 and three different seeds\nFPREFIX=\"sim.small\"\nTHREADS=4 \n\nfor k in {2,3}\ndo\n    for s in {1..3}\n    do\n        fastmixture --bfile $FPREFIX --K $k --threads $THREADS --seed $s --out $FPREFIX\n    done\ndone\n# Saves three files for each run (.Q, .P and .log)\n\n### Plot results and save (using R)\nfor k in {2,3}\ndo\n    for s in {1..3}\n    do\n        Rscript plotAdmixture.R $FPREFIX.K${k}.s${s}.Q\n    done\ndone\n\nIn fastmixture, the main arguments used in this exercise are:\n\n--K: Specifies the number of ancestral components, representing the sources in the mixture model.\n--seed: Sets the random seed to ensure reproducibility of the analysis across different runs.\n--bfile: prefix for PLINK files (.bed, .bim, .fam)\n--out: Prefix output name\n\nYou are set to write your own snakemake pipeline. Need more suggestions?\n\nAdd a log directive to every rule. It is best practice to store all log files in a subdirectory logs/, prefixed by the rule or tool name.\nAdd a benchmark directive as well. Applying the same best practice.\nMemory estimated based on the size of the file\nAfter running the workflow for the set of K and s values shown in the bash script. Use the command line to provide a new seed value: --config PARAM=&lt;value&gt;\nMake use of the multiext function to define a set of output or input that only differ by their extension\n\nHow to run this workflow? See the picture below:\nsnakemake -c1 --sdm conda\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nFirst, create a config.yaml file to specify the values for the wildcards k and s, as well as other parameters (such as the number of threads, and the prefix).\nThen, create the Snakemake Workflow, defining two rules for running fastmixture and plotting with R.\nfastmixture is not pre-installed, check the instructions fastmixture GitHub. Need more tips?\n# Git clone on your working dir\ngit clone https://github.com/Rosemeis/fastmixture.git\n\n# OPTION 1: Use the env.yml (recommended)\n# You just need to locate the file and use the the path to the environment file under the conda directive\nrule fast: \n  input: ...\n  output: ...\n  conda: /work/&lt;NameSurname#xxx&gt;/fastmixture/environment.yml\n\n# OPTION 2: Create the env and use it (useful if you and your colleague are using the same HPC and won't get the pipeline published)\ncd /work/&lt;NameSurname#xxx&gt;/fastmixture\nconda env create -f environment.yml --prefix /work/&lt;NameSurname#xxx&gt;/envs/fastmixture\n# Then use the env path \nrule fast: \n  input: ...\n  output: ...\n  conda: /work/&lt;NameSurname#xxx&gt;/envs/fastmixture\nMake use of useful directives:\nrule ...:\n  output: \"results/{wd2}.tsv\"\n  conda: \"/path/to/env.yml\"\n  log: \"logs/{wd1}.log\"\n  benchmark: \"logs/{wd1}.txt\"\n  shell: \"somecomamnd {output}\"\n\n\n\n\n\n\n\n\n\n\n\nMy solution\n\n\n\n\n\n\n\nThis is one possible solution, how does yours look?\n\n\nconfig.yml\n\nPLINKPATH: \"/work/HPCLab_workshop/data/plink_sim\"\nFPREFIX: \"sim.small\"\nTHREADS_FAST: 2  # Threads for fastmixture\nTHREADS_R: 1            # Threads for R plotting\nK_VALUES: [2,3]        # Values of K\nSEEDS:\n  - 1\n  - 2\n  - 3    # Seed values\nPATH_SCRIPT: \"/work/HPCLab_workshop/scripts\"\n\n\n\nSnakefile\n\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n__author__ = \"Alba Refoyo Martinez\"\n__copyright__ = \"Copyright 2024, University of Copenhagen\"\n__email__ = \"gsd818@@ku.dk\"\n__license__ = \"MIT\"\nconfigfile: \"config.yml\"\n\nrule all:\n    input:\n        expand(\"{FPREFIX}.K{k}.s{s}.png\", FPREFIX=config[\"FPREFIX\"], k=config[\"K_VALUES\"], s=config[\"SEEDS\"])\n\nrule fastmixture:\n    output:\n        q=\"{prefix}.K{k}.s{s}.Q\",\n        p=\"{prefix}.K{k}.s{s}.P\",\n        log=\"{prefix}.K{k}.s{s}.log\"\n    params:\n        prefix=config[\"FPREFIX\"],\n        pathIn=config[\"PLINKPATH\"]\n    threads: config[\"THREADS_FAST\"]\n    log:\n        \"logs/fastmixture_{prefix}.K{k}.s{s}.log\"\n    benchmark:\n        \"benchmarks/fastmixture_{prefix}.K{k}.s{s}.txt\"\n    conda: \n        \"/work/AlbaRefoyoMartínez#0753/fastmixture/environment.yml\"\n    shell:\n        \"\"\"\n        fastmixture --bfile {params.pathIn}/{params.prefix} --K {wildcards.k} --threads {threads} --seed {wildcards.s} --out {params.prefix} 2&gt; {log}\n        \"\"\"\n\nrule plot_results:\n    input:\n        q=\"{prefix}.K{k}.s{s}.Q\"\n    output:\n        plot=\"{prefix}.K{k}.s{s}.png\"\n    params:\n        plot_scripts=config[\"PATH_SCRIPT\"]\n    shell:\n        \"\"\"\n        Rscript {params.plot_scripts}/plotAdmixture.R {input.q}\n        \"\"\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCopyrightCC-BY-SA 4.0 license",
    "crumbs": [
      "Workshop",
      "HPC Pipes",
      "Day 2",
      "Day 2 - Part 4"
    ]
  },
  {
    "objectID": "workshop/cookiecutter.html",
    "href": "workshop/cookiecutter.html",
    "title": "Managing data",
    "section": "",
    "text": "Let’s start with some practical exercises focused on implementing tools that will help you with the collect & document data life cycle phase.",
    "crumbs": [
      "Workshop",
      "HPC Launch",
      "Managing data"
    ]
  },
  {
    "objectID": "workshop/cookiecutter.html#data-structure-with-cookiecutter",
    "href": "workshop/cookiecutter.html#data-structure-with-cookiecutter",
    "title": "Managing data",
    "section": "Data structure with cookiecutter",
    "text": "Data structure with cookiecutter\nEstablishing a consistent file structure and naming conventions will help you efficiently manage your data. We will classify your data and data analyses into two distinct types of folders to ensure the data can be used and shared by many lab members while preventing modifications by any individual:\n\nData folders: store raw and processed datasets, the workflow/pipeline used, data provenance, and quality control reports. These folders should be locked and read-only to prevent changes, with MD5 checksums used to verify data integrity.\nProject folders: contain all necessary files for a specific research project (scripts, results, intermediate files, etc.)\n\n\nSetting up folder templates\nCreating a folder template is straightforward with cookiecutter a command-line tool that generates projects from templates (called cookiecutters). You can do it from scratch (see Bonus) or opt for one of our pre-made templates available as a Github repository (recommended for this workshop).\nLet’s give it a try!\n\n\n\n\n\n\nExercise 1: Get familiar with Cookiecutter\n\n\n\n\n\n\n\n\nUse our cookiecuter-template and fill-up the variables\n\n\n\nTerminal\n\ncookiecutter https://github.com/hds-sandbox/cookiecutter-template\n\n\nExplore the project structure (e.g., ls -l)\n\n\n\n\n\n\nYou’re ready to customize your own template! Explore the following folder structure and the types of files you might encounter. How does it compare to your own setup?\n\n\nProject folder structure\n\n&lt;project&gt;_&lt;keyword&gt;_YYYYMMDD\n├── data                    # symlinks or shortcuts to the actual data files \n│  └── &lt;ID&gt;_&lt;keyword&gt;_YYYYMMDD\n├── documents               # docs and files relevant to the project \n│  └── research_project_template.docx\n├── metadata.yml            # variables or key descriptors of the project or data\n├── notebooks               # notebooks containing the data analysis\n│  └── 01_data_processing.rmd\n│  └── 02_data_analysis.rmd\n│  └── 03_data_visualization.rmd\n├── README.md               # detailed description of the project\n├── reports                 # notebooks rendered as HTML/PDF for sharing \n│  └── 01_data_processing.html\n│  └── 02_data_analysis.html\n│  ├── 03_data_visualization.html\n│  │  └── figures\n│  │  └── tables\n├── requirements.txt // env.yaml # file listing necessary software, libs and deps\n├── results                 # output from analyses, figs and tables\n│  ├── figures\n│  │  └── 02_data_analysis/\n│  │    └── heatmap_sampleCor_20230102.png\n│  ├── tables\n│  │  └── 02_data_analysis/\n│  │    └── DEA_treat-control_LFC1_p01.tsv\n│  │    └── SumStats_sampleCor_20230102.tsv\n├── pipeline                # pipeline scripts \n│  ├── rules // processes \n│  │  └── step1_data_processing.smk\n│  └── pipeline.md\n├── scratch                 # temporary files or workspace for dev \n└── scripts                 # other scripts \n\n\n\n\n\n\n\nExercise 2 for non-GitHub users\n\n\n\n\n\nIf you haven’t created a GitHub account or are not comfortable using it yet, you can skip step 1 in Exercise 2 (below). In step 2, use the sandbox URL instead of your owned forked repo by running the following command:\n\n\nTerminal\n\ngit clone https://github.com/hds-sandbox/cookiecutter-template\n\nIf you have a GitHub Desktop, click Add and select Clone repository from the options.\n\n\n\n\n\n\n\n\n\nExercise 2: Use and adapt the Sandbox template\n\n\n\n\n\n\n\nYou will first fork our Sandbox repository, and then clone it to UCloud. This allows you to customize the template to fit your specific needs, rather than strictly following our example, and save the changes back to your repository.\n\nGo to our Cookicutter template and click on the Fork button at the top-right corner of the repository page to create a copy of the repository on your own GitHub account.\n\n\n\nfork_repo_example\n\n\nOpen the Coder app on UCloud (locally, if you don’t have access), copy the URL of your fork and clone the repository to your personal drive on UCloud (the URL should look something like https://github.com/your_username/cookiecutter-template):\ngit clone &lt;your URL to the template&gt;\nAccess the cloned repository (cd cookiecutter-template) and navigate through the different directories.\nThe Cookiecutter template you just cloned is missing the ‘reports’ directory and the ‘requirements.txt’ file. Create these, along with a subdirectory named ‘reports/figures’.\n│...\n├── results/\n│   ├── figures/\n├── requirements.txt\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nHere’s an example of how to do it. Open your terminal and navigate to your template directory\ncd \\{\\{\\ cookiecutter.project_name\\ \\}\\}/  \nmkdir reports \ntouch requirements.txt\n\n\n\n\n\nFeel free to customize the template further to fit your project’s requirements. You can change files, add new ones, remove existing ones or adjust the folder structure. For inspiration, review the data structure under ‘Project folder structure’ above.\n\nUtilize the template\n\n\n\nTerminal\n\ncookiecutter cookiecutter-template\n\nThe command cookiecutter cookiecutter-template will initiate an interactive prompt. Fill up the variables and verify that the new structure (and folders) looks like you would expect. Have any new folders been added, or have some been removed?\n\n\nExample bash commands\n\n# Assuming the name of the project is 'myproject_sep24'\nls -l myproject_sep24 \n\nOptional (for those already familiar with git and GitHub)\nThe following steps enable version control and make it easy to share the structure with other members of your lab.\n\nCommit and push changes when you are done with your modifications.\n\n\nStage the changes with git add.\nCommit the changes with a meaningful commit message git commit -m \"update cookicutter template\".\nPush the changes to your forked repository on Github git push origin main (or the appropriate branch name).\n\n\nUse cookiecutter on the new template! cookiecutter &lt;URL to your GitHub repository \"cookicutter-template\"&gt;\n\n\n\n\n\n\nIf you’ve completed the tasks quickly and have time left, feel free to tackle the optional final exercise.\n\n\n\n\n\n\nBonus exercise 2\n\n\n\n\n\nCreate a template from scratch using this tutorial scratch. Your template can be as basic as the example provided or include a data folder structure with directories for raw data, processed data, and the pipeline used for preprocessing.\nmy_template/\n|-- {{cookiecutter.project_name}}\n|   |-- main.py\n|-- tests\n|   |-- test_{{cookiecutter.project_name}}.py\n|-- README.md\n\nStep 1: Create a directory for the template (like the one above).\nStep 2: Write a cookiecutter.json file with variables such as project_name and author.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\n{\n  \"project_name\": \"MyProject\",\n  \"author_name\": \"Your Name\",\n  \"description\": \"A short description of your project\"\n}\n\n\n\n\n\n\nStep 3: Set up the folder structure by creating subdirectories and files as needed.\nStep 4: Incorporate cookiecutter variables in the names of files (test_{{cookiecutter.project_name}}.py).\nStep 5: Use cookiecutter variables within scripts opr metadata files (e.g., such as printing a message that includes the project name or the metadata file gets automatically populated with the cookiecutter variables),",
    "crumbs": [
      "Workshop",
      "HPC Launch",
      "Managing data"
    ]
  },
  {
    "objectID": "workshop/cookiecutter.html#naming-conventions",
    "href": "workshop/cookiecutter.html#naming-conventions",
    "title": "Managing data",
    "section": "Naming conventions",
    "text": "Naming conventions\nA well-structured naming system keeps files organized, easy to search, compatible across different systems, and useful for collaboration.\n\n\n\n\n\n\n\nExercise 3\n\n\n\n\n\n\n\nQ1. Which naming conventions should be used and why?\n\n a. Grant proposal final.doc b. differential_expression_results_clara.csv c. sequence_alignment$v1.py\n\n\n d. scripts/data_processing_carlo's.py e. data/raw_sequences_V#20241111.fasta f. data/gene_annotations_20201107.gff\n\n\n g. alpha~1.0/beta~2.0/reg_2024-05-98.tsv h. alpha=1.0/beta=2.0/reg_2024-05-98.tsv i. run_pipeline:20241203.sh\n\nQ2. Which file name is more readable?\n\n 1a. forecast2000122420240724.tsv 1b. forecast_2000-12-24_2024-07-24.tsv 1c. forecast_2000_12_24_2024_07_24.tsv\n\n\n 2a. 01_data_preprocessing.R 2b. 1_data_preProcessing.R 2c. 01_d4t4_pr3processing.R\n\n\n 3a. B1_2024-12-12_cond~pH7_temp~37C.fastq 3b. B1.20241212.pH7.37C.fastq 3c. b1_2024-12-12_c0nd~pH7_t3mp~37C.fastq\n\n\n\n\n\n\nRegular expressions are an incredibly powerful tool for string manipulation. We recommend checking out RegexOne to learn how to create smart file names that will help you parse them more efficiently.\n\n\n\n\n\n\nBonus exercise 3\n\n\n\nWhich of the regexps below match ONLY the filenames shown in bold?\n\nrna_seq/2021/03/results/Sample_A123_gene_expression.tsv\nproteomics/2020/11/Sample_B234_protein_abundance.tsv\nrna_seq/2021/03/results/Sample_C345_normalized_counts.tsv\nrna_seq/2021/03/results/Sample_D456_quality_report.log\nmetabolomics/2019/05/Sample_E567_metabolite_levels.tsv\nrna_seq/2019/12/Sample_F678_raw_reads.fastq\nrna_seq/2021/03/results/Sample_G789_transcript_counts.tsv\nproteomics/2021/02/Sample_H890_protein_quantification.TSV\n\nrna_seq.*\\.tsv TRUEFALSE\n.*\\.csv TRUEFALSE\n.*/2021/03/.*\\.tsv TRUEFALSE\n.*Sample_.*_gene_expression.tsv TRUEFALSE\nrna_seq/2021/03/results/Sample_.*_.*\\.tsv TRUEFALSE",
    "crumbs": [
      "Workshop",
      "HPC Launch",
      "Managing data"
    ]
  },
  {
    "objectID": "workshop/cookiecutter.html#collect-share-using-checksums",
    "href": "workshop/cookiecutter.html#collect-share-using-checksums",
    "title": "Managing data",
    "section": "Collect & share using checksums",
    "text": "Collect & share using checksums\nWe recommend using md5sum to verify data integrity, particularly when downloading large datasets, as it is a widely used tool. All data and files archived on Zenodo include an MD5 hash for this purpose. Let’s have a look at the content of a newly developed software fastmixture that estimates individual ancestry proportions from genotype data.\n\n\n\n\n\n\nExercise 4\n\n\n\n\n\n\n\n\nOpen this Zenodo link\nEnter the DOI of the repo (for all versions): \nZenodo offers an API at https://zenodo.org/api/, which functions similarly to the DOI API. This allows you to retrieve a BibTeX-formatted reference for a specific record (e.g., records/14106454) using either curl or wget.\n\n\n\nTerminal\n\n# ------curl-------\ncurl -LH 'Accept: application/x-bibtex' https://zenodo.org/api/records/14106454 \\\n     --output meisner_2024.bib\n\n# ------wget-------\nwget --header=\"Accept: application/x-bibtex\" -q \\\n     https://zenodo.org/api/records/12683372 -O meisner_2024.bib\n\nDoes the content of your *.bib file look like this?\n@misc{meisner_2024_14106454,\n  author       = {Meisner, Jonas},\n  title        = {Supplemental data for reproducing \"Faster model-\n                   based estimation of ancestry proportions\"},\n  month        = nov,\n  year         = 2024,\n  publisher    = {Zenodo},\n  version      = {v0.93.4},\n  doi          = {10.5281/zenodo.14106454},\n  url          = {https://doi.org/10.5281/zenodo.14106454},\n}\n\nScroll down to files and download the software zip file (fastmixture-0.93.4.zip)\n\n\n\nTerminal\n\ncurl https://zenodo.org/records/14106454/files/fastmixture-0.93.4.zip \\\n--output fastmixture.zip \n\n\nCompute md5 hash and enter the value (no white-spaces) \nIs your value tha same as the one shown on zenodo TRUEFALSE\nFinally, compute the sha256 digest (with program sha256)\n\nsha256sum\nand enter the value \n\n\n\n\n\n\n\n\n\n\n\nBonus exercise 4\n\n\n\n\n\nWe will be using the HLA database for this exercise. Click on this link or google IMGT HLA&gt; Download. Important: go through the README before downloading! Check if a checksums file is included.\n\nDownload and open the md5checksum.txt (HLA FTP Directory)\nLook for the hash of the file hla_prot.fasta\nCreate a bash script to download the target files (named “dw_resources.sh” in your current directory).\n\n#!/bin/bash\nmd5file=\"md5checksum.txt\"\n\n# Define the URL of the files to download\nurl=\"ftp://ftp.ebi.ac.uk/pub/databases/ipd/imgt/hla/hla_prot.fasta\"\n\n# (Optional 1) Save the original file name: filename=$(basename \"$url\")\n# (Optional 2) Define a different filename to save the downloaded file (`wget -O $out_filename`)\n# out_filename = \"imgt_hla_prot.fasta\"\n\n# Download the file\nwget $url --output $out_filename && \\\nmd5sum --quiet --ignore-missing --check $md5file\nWe recommend using the argument --quiet as part of your pipeline so that it only prints the errors (it doesn’t print output when success). The --ignore-missing argument is useful because it allows us to use the raw checksums file while skipping files we may not want to download.\nDid you get any error?\n\nGenerate the md5 hash & compare to the one from the original md5checksum.txt.",
    "crumbs": [
      "Workshop",
      "HPC Launch",
      "Managing data"
    ]
  },
  {
    "objectID": "workshop/cookiecutter.html#documentation",
    "href": "workshop/cookiecutter.html#documentation",
    "title": "Managing data",
    "section": "Documentation",
    "text": "Documentation\nExplore the examples below and consider how effectively the README files communicate key information about the project. Some links point to README files describing databases, while others cover software and tools.\n\n1000 Genomes Project\nHomo sapiens, GRCh38\nIPD-IMGT/HLA database\nPandas package\nDanish registers\n\nHow does your documentation compare to these?\nDone for today!",
    "crumbs": [
      "Workshop",
      "HPC Launch",
      "Managing data"
    ]
  },
  {
    "objectID": "workshop/launch-requirements.html#agenda",
    "href": "workshop/launch-requirements.html#agenda",
    "title": "Welcome to the HPC-Launch workshop",
    "section": "Agenda",
    "text": "Agenda\n\n\n\n\n\n\n\n\n\nTime\nActivity\nTime\nActivity\n\n\n\n\n8:45\nMorning coffee (optional)\n\n\n\n\n9:00\nIntroduction to the Sandbox project\n12:00\nLunch break\n\n\n9:15\nIntroduction to HPC: the basics\n13:00\nRDM Step-by-step I\n\n\n10:15\nCoffee break\n14:15\nCoffee break\n\n\n10:30\nHPC workflow\n14:30\nRDM Step-by-step II\n\n\n11:15\nIntro to RDM\n15:00\nDK HPC solutions & resources",
    "crumbs": [
      "Workshop",
      "HPC Launch",
      "Welcome to the HPC-Launch workshop"
    ]
  },
  {
    "objectID": "workshop/launch-requirements.html#discussion-and-feedback",
    "href": "workshop/launch-requirements.html#discussion-and-feedback",
    "title": "Welcome to the HPC-Launch workshop",
    "section": "Discussion and feedback",
    "text": "Discussion and feedback\nWe hope you enjoyed the workshop. As data scientists, we also would be really happy for some quantifiable info and feedback - we want to build things that the Danish health data science community is excited to use. Please, fill up the feedback form before you head out for the day 3.\n \n\n\n\n\n\n\n\nNice meeting you and we hope to see you again!\n\n\n\n\n\nAbout the National Sandbox project\nThe Health Data Science Sandbox aims to be a training resource for bioinformaticians, data scientists, and those generally curious about how to investigate large biomedical datasets. We are an active and developing project seeking interested users (both trainees and educators). All of our open-source materials are available on our Github page and can be used on a computing cluster! We work with both UCloud, GenomeDK and Computerome, the major Danish academic supercomputers.",
    "crumbs": [
      "Workshop",
      "HPC Launch",
      "Welcome to the HPC-Launch workshop"
    ]
  },
  {
    "objectID": "workshop/launch-requirements.html#footnotes",
    "href": "workshop/launch-requirements.html#footnotes",
    "title": "Welcome to the HPC-Launch workshop",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOther institutions (e.g. hospitals, libraries, …) can log on through WAYF. See all institutions here.↩︎\nTo use Sandbox materials outside of the workshop: remember that each new user has hundreds of hours of free computing credit and around 50GB of free storage, which can be used to run any UCloud software. If you run out of credit (which takes a long time) you’ll need to check with the local DeiC office at your university about how to request compute hours on UCloud. Contact us at the Sandbox if you need help or want more information.↩︎\nlink activated on the day of the workshop.↩︎",
    "crumbs": [
      "Workshop",
      "HPC Launch",
      "Welcome to the HPC-Launch workshop"
    ]
  },
  {
    "objectID": "workshop/pipes-requirements.html#course-requirements",
    "href": "workshop/pipes-requirements.html#course-requirements",
    "title": "Welcome to the HPC-Pipes workshop",
    "section": "Course requirements",
    "text": "Course requirements\n\n\n\n\n\n\nRequired preparation\n\n\n\nYou are expected to complete the required setup, including tool installation (Docker) and account creation (UCloud).\n\nDocker - click on Download Docker Desktop\n\nAs for other software, we will provide access to a Danish HPC platform, UCloud, with all the necessary software pre-installed. Please read Using UCloud for exercises carefully.\nIf you prefer to run the exercises on your personal laptop or a different server, please ensure you have the following software installed:\n\nconda - miniconda or miniforge recommended.\nsnakemake use conda for this!\nnextflow\nApptainer, formerly known as Singularity.\n\n\n\n\nUsing UCloud for exercises\n\n\n\n\n\n\nWarning\n\n\n\nFollow the instructions below if you have an account at a Danish university. You will need your institutional email to proceed. Unfortunately, this will not work for those without a university email.\n\n\n\nCreate an account on UCloud with your institution’s credentials\nUse the link below to join our workspace where you will find a setup environment1\n\n \n\n Invite link to UCloud workspace \n\n \n\nYou’re all set! You will receive instructions on how to navigate through UCloud during the course.\n\n\n\nReading material (optional)\n\nThe Turing way. It offers comprehensive guidance on reproducible research practices, including setting up computational environments and managing reproducible workflows.\nMölder, Felix, et al. “Sustainable data analysis with Snakemake.” F1000Research 10 (2021). Link to article. Best practices using Snakemake to develop your pipelines.\nCheck our content on HPC pipes.",
    "crumbs": [
      "Workshop",
      "HPC Pipes",
      "Welcome to the HPC-Pipes workshop"
    ]
  },
  {
    "objectID": "workshop/pipes-requirements.html#agenda",
    "href": "workshop/pipes-requirements.html#agenda",
    "title": "Welcome to the HPC-Pipes workshop",
    "section": "Agenda",
    "text": "Agenda\n\nDay 1\n\n\n\nTime\nActivity\nTime\nActivity\n\n\n\n\n8:45\nMorning coffee (optional)\n\n\n\n\n9:00\nIntro to HPC & onboarding\n12:00\nLunch break\n\n\n9:45\nHPC resources\n13:00\nExercise - software mgmt\n\n\n10:15\nCoffee break\n14:15\nCoffee break\n\n\n10:30\nSoftware mgmt I\n14:30\nComputations mgmt I\n\n\n11:15\nSoftware mgmt II\n16:00\nDiscussions & Wrap-up\n\n\n\n\nEnvironments exercises Snakemake I exercises\n\n\n\nDay 2\n\n\n\n\n\n\n\n\n\nTime\nActivity\nTime\nActivity\n\n\n\n\n8:45\nMorning coffee (optional)\n\n\n\n\n9:00\nComputations mgmt II\n12:00\nLunch break\n\n\n9:45\nExercise - integration\n13:00\nNextflow, nf-core example\n\n\n10:15\nCoffee break\n14:15\nCoffee break\n\n\n10:30\nExercise - implementation\n14:30\nBuild your own pipeline\n\n\n11:15\nComputations mgmt III\n16:00\nDiscussions & Wrap-up\n\n\n\n\nSnakemake II exercises Snakemake III exercises",
    "crumbs": [
      "Workshop",
      "HPC Pipes",
      "Welcome to the HPC-Pipes workshop"
    ]
  },
  {
    "objectID": "workshop/pipes-requirements.html#discussion-and-feedback",
    "href": "workshop/pipes-requirements.html#discussion-and-feedback",
    "title": "Welcome to the HPC-Pipes workshop",
    "section": "Discussion and feedback",
    "text": "Discussion and feedback\nWe hope you enjoyed the workshop. As data scientists, we also would be really happy for some quantifiable info and feedback - we want to build things that the Danish health data science community is excited to use. Please, fill up the feedback form before you head out for the day 2.\n \n\n\n\n\n\n\n\nNice meeting you and we hope to see you again!\n\n\n\n\n\nAbout the National Sandbox project\nThe Health Data Science Sandbox aims to be a training resource for bioinformaticians, data scientists, and those generally curious about how to investigate large biomedical datasets. We are an active and developing project seeking interested users (both trainees and educators). All of our open-source materials are available on our Github page and can be used on a computing cluster! We work with both UCloud, GenomeDK and Computerome, the major Danish academic supercomputers.",
    "crumbs": [
      "Workshop",
      "HPC Pipes",
      "Welcome to the HPC-Pipes workshop"
    ]
  },
  {
    "objectID": "workshop/pipes-requirements.html#footnotes",
    "href": "workshop/pipes-requirements.html#footnotes",
    "title": "Welcome to the HPC-Pipes workshop",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nlink activated a week before the workshop.↩︎\nlink activated on the day of the workshop.↩︎",
    "crumbs": [
      "Workshop",
      "HPC Pipes",
      "Welcome to the HPC-Pipes workshop"
    ]
  },
  {
    "objectID": "develop/exercise_pipes.html",
    "href": "develop/exercise_pipes.html",
    "title": "Exercises",
    "section": "",
    "text": "Put your learning to the test with what you’ve covered so far.",
    "crumbs": [
      "HPC Pipes",
      "Pipeline languages",
      "Exercises"
    ]
  },
  {
    "objectID": "develop/exercise_pipes.html#general-hpc-pipes",
    "href": "develop/exercise_pipes.html#general-hpc-pipes",
    "title": "Exercises",
    "section": "General HPC pipes",
    "text": "General HPC pipes\n1. What role does a workflow manager play in computational research??\n\n Automating the execution of complex computational processes To manually execute each step of a computation Offering real-time and intuitive data analysis To minimize the reliance on specific software environments\n\n2.What is the primary drawback of using shell scripts for automating computations?\n\n Limited error handling and debugging capabilities Difficulty in integrating with notebooks Not compatible with all operating systems They re-run all the steps every time Insufficient support for parallel processing Complex and extensive coding for simple tasks\n\n3. What are the key features of workflow manager in computational research? (Several possible solutions)\n\n Executing tasks only when required Managing task dependencies Overseeing storage and resource allocation Providing intuitive graphical interfaces\n\n4. Workflow managers can run tasks (different) concurrently if there are no dependencies (True or False) TRUEFALSE\n5. A workflow manager can execute a single parallelized task on multiple nodes in a computing cluster (True or False) TRUEFALSE",
    "crumbs": [
      "HPC Pipes",
      "Pipeline languages",
      "Exercises"
    ]
  },
  {
    "objectID": "develop/exercise_pipes.html#snakemake",
    "href": "develop/exercise_pipes.html#snakemake",
    "title": "Exercises",
    "section": "Snakemake",
    "text": "Snakemake\n\n\n\n\n\n\nExercise 1S: Exploring Rule Invocation in Snakemake\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this exercise, we will explore how rules are invoked in a Snakemake workflow. Download the Snakefile and data required for this exercise using the links below.\n Download data input   Download Snakefile \nNow follow these steps and answer the questions:\n\nOpen the snakefile, named process_1kgp.smk and try to understand every single line. If you request Snakemake to generate the file results/all_female.txt, what commands will be executed and in what sequence?\nDry run the workflow: Check the number of jobs that will be executed.\n6. How many jobs will Snakemake run? \nRun the workflow: Use the name flag --snakefile | -s follow by the name of the file.\nVerify output: Ensure that the output files are in your working directory.\nClean Up: remove all files starting with EUR in your results folder.\nRerun the workflow: Execute the Snakefile again.\n7. How many jobs did Snakemake run in this last execution? \nRemove lines 4-6 in the process_1kgp.smk. How else can you run the workflow but to generate instead all_male.txt using only the command-line?\nrule all:\n   input:\n      expand(\"results/all_{gender}.txt\", gender=[\"female\"])\n8. Tip: what is missing at the end of the command ( e.g. what should be added to ensure all_male.txt is generated)? snakemake -s process_1kgp.smk -c1 \n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\n# dry run \nsnakemake -s process_1kgp.smk -n \n# run the workflow \nsnakemake -s process_1kgp.smk-c1 &lt;name_rule|name_output&gt;\n# verify output \nls &lt;name_output&gt;\n# remove file belonging to european individuals \nrm results/EUR.tsv results/all_female.txt\n# rerun again \nsnakemake -s process_1kgp.smk -c1 &lt;name_rule|name_output&gt;",
    "crumbs": [
      "HPC Pipes",
      "Pipeline languages",
      "Exercises"
    ]
  },
  {
    "objectID": "develop/p0-intro.html",
    "href": "develop/p0-intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Course Overview\n\n\n\n\n👨‍💻 Target Audience: Anyone interested in workflow management systems and software environments.\n👩‍🎓 Level: Advanced.\n🔒 License: Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license.\n\n💰 Funding: This project was funded by the Novo Nordisk Fonden (NNF20OC0063268).\n\n\n\nThe course “HPC pipes” is designed to provide participants with foundational knowledge and practical skills in writing reproducible pipelines. As part of effective data management, it is crucial that researchers create reproducible analyses that enable others to validate and build upon their work. We will explore essential elements of reproducibility and efficiency in computational research, highlighting techniques and tools for creating robust and transparent coding and workflows. By prioritizing reproducibility and replicability, researchers can enhance the credibility and impact of their findings while fostering collaboration and knowledge dissemination within the scientific community. T, ensuring that you can handle your projects with ease. For more details on practical RDM strategies, explore our on practical RDM for biodata for more details.\nHPC-pipes is divided into two main sections:\n\nComputational environments\nPipeline languages\n\n\n\n\n\n\n\nCourse Requirements\n\n\n\nIt is important to be familiar with unix and python. Follow the tutorials in the links below if you need a refresher.\n\nCommand Line experience (Software Carprentry Shell)\nProgramming experience (Python)\n\n\n\n\n\n\n\n\n\nModule Goals\n\n\n\n\nUnderstand the rol of scientific pipelines\nRun existing pipelines\nImplement and modify pipelines\nSpecify software and computational resource needs\nCustomise your pipeline to accept user-defined configurations (params)\nCreate reproducible analyses that can be adapted to new data with little effort\nIntegrate workflows with software environments\n\n\n\n\n\n\nCopyrightCC-BY-SA 4.0 license",
    "crumbs": [
      "HPC Pipes",
      "Introduction"
    ]
  },
  {
    "objectID": "develop/e5-data_compression.html#data-organization",
    "href": "develop/e5-data_compression.html#data-organization",
    "title": "HPC Lab",
    "section": "Data organization",
    "text": "Data organization\n\nregular expression\ncookiecutter"
  },
  {
    "objectID": "develop/e3-singularity.html",
    "href": "develop/e3-singularity.html",
    "title": "Singularity",
    "section": "",
    "text": "Requirements\n\n\n\n\nInstall singularity\nRead the documentation",
    "crumbs": [
      "HPC Pipes",
      "Computational environments",
      "Singularity"
    ]
  },
  {
    "objectID": "develop/e3-singularity.html#building-sif-images",
    "href": "develop/e3-singularity.html#building-sif-images",
    "title": "Singularity",
    "section": "Building SIF images",
    "text": "Building SIF images\nThe equivalent to a Dockerfile for singularity is Singularity definition file where the instructions for the image are specified.\nBootstrap: docker\nFrom: debian:stable\nStage: build\n\n%post\n    apt-get update && apt-get install -y jupyter-notebook python3-matplotlib python3-pandas python3-numpy\n%labels\n    Author Name Surname &lt;abc123@ku.dk&gt;\n    Version v1.0\n# Build the image\nsingularity build --fakeroot  &lt;my-app&gt;.sif &lt;my-app&gt;.def\n# Run the container \nsingularity run &lt;my-app&gt;.sif jupyter-notebook \n\n\n\n\n\n\nTips\n\n\n\nLimitations\n\nKeep in mind that Singularity always operates using your user ID, meaning you cannot switch to the root user inside a Singularity container. This will cause troubles when installing package managers like apt.\nSIF images are not writable by default.\n\nSolution\n\n--fakeroot option is used to create a container image with root-like permissions without requiring actual root access on the host system. This is particularly useful for users who need to build or modify containers (installing software) in environments where they do not have superuser privileges.",
    "crumbs": [
      "HPC Pipes",
      "Computational environments",
      "Singularity"
    ]
  },
  {
    "objectID": "develop/e3-singularity.html#using-docker-images",
    "href": "develop/e3-singularity.html#using-docker-images",
    "title": "Singularity",
    "section": "Using Docker images",
    "text": "Using Docker images\nLet’s use the same docker image as in the Docker section.\nsingularity pull docker://debian@sha256:2171c87301e5ea528cee5c2308a4589f8f2ac819d4d434b679b7cb1dc8de6912\n\n# creates a sif \nsingularity shell debian@sha256_2171c87301e5ea528cee5c2308a4589f8f2ac819d4d434b679b7cb1dc8de6912.sif\n\n# Let's check a couple commands on the host machine  (as we did in the previous module)\n\ngzip --version # same as docker\nhostname # vagrant (macß) different\nwhoami # vagrant (docker is root)\n\n# deploy the container  \nsingularity shell debian@sha256_2171c87301e5ea528cee5c2308a4589f8f2ac819d4d434b679b7cb1dc8de6912.sif\n\n# Install filter in the container \napt-get install filter\n\n# Alternative command to install filter\nsingularity exec docker://debian@sha256:2171c87301e5ea528cee5c2308a4589f8f2ac819d4d434b679b7cb1dc8de6912 bash -c \"apt install filter 2&gt;&1 || true \"\n\n\n\n\n\n\nTip\n\n\n\nThe docker tag (docker://) step is required as singularity has sometimes trouble handling the sha256, image description.\nOther important information to consider:\n\nSingularity Hub is no longer maintained. Alternatives: git-annex for hosting images.\nSIF images are much smaller than Docker images.",
    "crumbs": [
      "HPC Pipes",
      "Computational environments",
      "Singularity"
    ]
  },
  {
    "objectID": "develop/e2-docker.html",
    "href": "develop/e2-docker.html",
    "title": "Docker",
    "section": "",
    "text": "Content\n\n\n\nThis section is divided into two main parts:\n\nUsing Docker images\nBuilding custom Docker images\n\nRefer to the Docker commands overview for a handy checklist or quick reference.\nDocker enables developers build, share, run, and verify applicationsseamlessly across different environments, eliminating the need for complex environment configuration and management. Before diving into hands-on activities, ensure you understand these three key concepts:",
    "crumbs": [
      "HPC Pipes",
      "Computational environments",
      "Docker"
    ]
  },
  {
    "objectID": "develop/e2-docker.html#using-docker-images",
    "href": "develop/e2-docker.html#using-docker-images",
    "title": "Docker",
    "section": "Using Docker images",
    "text": "Using Docker images\nFirst, we’ll start by using basic commands with existing Docker images from Docker Hub. This will help you become familiar with Docker’s functionality before we move on to creating our own custom images. While people commonly use shorthand Docker commands like docker pull and docker run, the full versions (e.g., docker image pull, docker container run) provide clearer insights into their underlying concepts and specific functions.\nDocker Hub\n\nSearch for images docker search &lt;name&gt; (e.g. docker search debian)\nDownload an image docker pull &lt;image-name&gt;\n\nLocal Docker Daemon\n\nDisplay all docker images currently stored on your local Docker daemon docker images (alias for docker image ls)\nInspect docker image docker inspect &lt;image_name&gt; (alias for docker image inspect)\nRun a command (cmd) in a container docker run &lt;image_name&gt; cmd (alias for docker container run  &lt;image_name&gt; cmd)\nStart an interactive bash shell docker run -it &lt;image_name&gt; bash. Add other flags like:\n\n-p 8888:8888 to access your interactive shell through ‘localhost:8888’ on your host.\n-rm to automatically delete the container once it stops, keeping your system clean (including its filesystem changes, logs and metadata). If you don’t run this flag, a container will automatically be created and information about tje processes will be kept. Check all containers in your Docker daemon docker container ls -a\n--user=$(id -u):$(id -g) useful if you are using sharing volumes and need appropriate permissions on the host to manipulate files.\n\nShare the current directory with the container docker run -it --volume=$(pwd):/directory/in/container image_name bash, the output of pwd will be mounted to the /directory/in/container (e.g. data, shared, etc.)\nManage your containers using pause or stop\n\nTag images with a new name docker image tag image_name:tag new_name:tag\ndocker logs &lt;container_id&gt;\nRemove images and clean up your hard drive docker rmi &lt;image_name&gt;\nRemove containers docker container rm &lt;container_name&gt;. Alternatively, remove all dead containers: docker container prune\n\nAll Docker containers have a digest which is thesha256 hash of the image. It allows to uniquely identify a docker image and it is great for reproducibility.\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nIn this exercise, we will use a Debian stable image (sha256-540ebf19fb0bbc243e1314edac26b9fe7445e9c203357f27968711a45ea9f1d4) as an example for pulling and inspecting Docker images. This image offers a reliable, minimal base with essential tools, including fundamental utilities like bash for shell scripting and apt for package management. It’s an excellent starting point for developing and testing pipelines or configuring server environments, providing a stable foundation that can be customized with additional packages as needed.\n1. Get a container image\n\nPull docker image (using tag stable otherwise, latest will be pulled by default)\ndocker pull debian:stable@sha256:2171c87301e5ea528cee5c2308a4589f8f2ac819d4d434b679b7cb1dc8de6912\n2. Run commands in a container\nList the content of the container\ndocker run -it -rm debian@sha256:2171c87301e5ea528cee5c2308a4589f8f2ac819d4d434b679b7cb1dc8de6912 ls\nCheck is there is python or perl in this container:\ndocker run -rm debian@sha256:2171c87301e5ea528cee5c2308a4589f8f2ac819d4d434b679b7cb1dc8de6912 which -a python perl\n\n3. Use docker interactively\n\nEnter the container interactively with a Bash shell\ndocker run -it -rm debian@sha256:2171c87301e5ea528cee5c2308a4589f8f2ac819d4d434b679b7cb1dc8de6912 /bin/bash\nNow, collect info about the packages installed in the environment.\n\n\nInteractive Docker container\n\nhostname\nwhoami\nls -la ~/\npython  \necho \"Hello world!\" &gt; ~/myfile.txt\nls -la ~/\n\n4. Exit and check the container\nExit the container\nexit\n\nNow, rerun commands from step 3 under “Interactive Docker container”. Does the output look any different?\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nYou will notice that you are the root but the name of the machine has now changed and the file that you had created has disappeared.\n\n\n\n\n\n5. Inspect the docker image\ndocker image inspect debian:stable@sha256:2171c87301e5ea528cee5c2308a4589f8f2ac819d4d434b679b7cb1dc8de6912\n6. Inspect Docker Image Details Identify the date of creation, the name of the field with the digest of the image and command run by default when entering this container.\n7. Remove container\ndocker image docker rmi debian:stable@sha256:2171c87301e5ea528cee5c2308a4589f8f2ac819d4d434b679b7cb1dc8de6912\n\n\n\n\n\n\nWhen you exit a container, any changes made are lost because each new container starts in a clean state. Containers are isolated environments that don’t retain memory of past interactions, allowing you to experiment without affecting your system. This immutability ensures that your program will run under the same conditions in the future, even months later. So, how can you retrieve the results of computations performed inside a container?\nTo preserve your work, it is crucial to use Docker’s volume mounting feature. When running a Docker container with the -v (volume) option, it’s best to execute the command from the temporary directory (e.g. /tmp/ ) or a project-specific directory rather than your home directory. This practice helps maintain a clean and isolated environment within the container, while ensuring your results are saved in a designated location outside the container’s ephemeral filesystem.\n\nNamed volumes\nNow, we will be using named volumes or shared directories. When you make changes in these directories within the container, the updates are immediately reflected in the corresponding directory on the host machine. This setup enables seamless synchronization between the container and the host file system.\n# -v: mounting only your project-specific dir \ndocker run -it --volumes &lt;project_dir&gt;\n# Named volumes: are managed by Docker and are isolated from your host file system\ndocker volume create &lt;my_project_volume&gt;\ndocker run -it --volumes &lt;my_project_volume&gt;\n\n\n\n\n\n\nWhy not mounting your home directory\n\n\n\nMounting your current working directory (${PWD}) into the container at e.g. /home/rstudio can compromise the benefits of container isolation. If you run the command from your home directory, any local packages or configurations (such as R or Python packages installed with install.packages or pip) would be accessible inside the container. This inadvertently includes your personal setup and packages, potentially undermining the container’s intended clean and isolated environment. To maintain effective isolation, it’s better to use a temporary or project-specific directory for mounting.\n\n\nAlternatively, you can add a volume to a project, by modifying the compose.yaml (also named docker-compose.yml) file. There are two types of volumes:\n\nService-level name: specify how volumes are mounted inside the container. In this case, dataset volume (defined at the top-level) will be mounted to the /path/in/container/ (e.g. data, results or logs directory) inside the myApp container.\nTop-level volume: volumes shared across multiple services in the compose.yaml file. The volume dataset can be referenced by any service and will be created if it doesn’t exit. If the host path is not specified, Docker will automatically create and manage the volume in a default location on the host machine. However, if you need the volume to be located in a specific directory, you can specify the host path directly (option 2).\n\n\n\ncompose.yaml\n\n# Service-level name\nmyApp:\n    # ...\n    volumes:\n      - dataset:/path/in/container/\n    # Option 2 \n    # - /my/local/path:/path/in/container/\n                      \n# Top-level volume\nvolumes:\n  dataset:\n\nIn this case, a volume named mydata will be mounted to the /data/ directory inside the container running the todo-databse service.\nLet’s not forget to track changes to container images for reproducibility by using version control. Store your images in a shared storage area, such as with Git or Git Annex, to manage versions and facilitate collaboration.\n\n\nTransfer and backup Docker images\nSaving a Docker image as a tar file is useful for transferring the image to another system or operating system without requiring access to the original Docker registry. The tar file contains several key components:\n\nmetadata: JSON files essential to reconstruct the image\n\nlayer information with each layer associated with metadata that includes which commands are used to create the later.\n\n\nDockerfile\n\nFROM ubuntu:20.04 # Layer 1 - base image \nRUN apt-get update && apt-get install -y python3 # Layer 2 - the result of running the command \nCOPY . /app # Layer 3 - add application files to the images\n\ntags pointers to specific image digests or versions.\nhistory of the image and instructions from the DOckerfile that were used to build the image.\nmanifest which ties together the layers and the overall image structure.\n\nFilesystem: the actual content, files and directories that make up the image.\n\n# save to tar file \ndocker image save --output=image.tar image_name \n# load tar file\ndocker image load --input=image.tar  \nIn some situations, you may need to access the filesystem content of a Docker image for purposes such as debugging, backup, or repurposing. To do this, you should create a container from the image and then export the container’s root filesystem to a tar file. Similarly, you can create a new Docker image from this tar file if needed.\ndocker container create --name=temp_container image_name\ndocker container export --output=image.tar temp_container\ndocker container rm temp_container\n# new image importing tar file \ndocker image import --input image.tar image_name",
    "crumbs": [
      "HPC Pipes",
      "Computational environments",
      "Docker"
    ]
  },
  {
    "objectID": "develop/e2-docker.html#building-docker-images",
    "href": "develop/e2-docker.html#building-docker-images",
    "title": "Docker",
    "section": "Building Docker images",
    "text": "Building Docker images\nWe’re now ready to build custom Docker images. We’ll start with a minimal pre-built image, add the necessary software, and then upload the final image to Docker Hub.\nA common practice is to start with a minimal image from Docker Hub. There are many base images you can use to start building your customized Docker image, and the key difference between them lies in what they already include. For example:\n\nDebian: This is a very minimal base image, including only the most essential utilities and libraries required to run Debian, such as the core Linux utilities and the package manager (apt). It offers a high degree of customization, giving you full control over what you install. However, you’ll need to manually add all the necessary software, making the setup process longer. This image is relatively small in size, making it a good starting point if you want to build your environment from scratch.\nData Science-Specific Images: These images come pre-configured with a wide range of tools pre-installed, reducing the amount of customization needed. This allows you to get started much faster, though it also means less flexibility. These images tend to be larger in size due to the pre-installed software. For example:\n\ntensorflow/tensorflow: This image includes TensorFlow and often other machine learning libraries, making it ideal for deep learning projects.\njupyter/scipy-notebook: This image is around 2-3 GB and includes Python, Jupyter Notebook, and libraries like NumPy, Pandas, Matplotlib, and more, making it a comprehensive option for data science tasks.\nr-base: This image provides a base for R environments, useful for data analysis and statistical computing.-\nrocker/rstudio: This image includes RStudio and a base R environment, making it perfect for those working in R for statistical computing and data analysis.\n\n\nOnce you have chosen your base image, use a Dockerfile to modify its components, specifying commands for software installation and configuration. The Dockerfile acts as a recipe, providing a list of steps to build the image.\n\n\nDockerfile\n\n# deploy docker container\nFROM &lt;node|debian|python|jupyter-base&gt;\n\n# Info and rights of the app\nLABEL software=\"App_name - sandbox\" \\\n    maintainer=\"&lt;author.address@sund.ku.dk&gt;\"  \\\n    version=\"YYYY.MM.DD\" \n\n# root: needed to install and modify the container \nUSER 0 \n\n# run bash commands (eg. installing packages or softwares)\nRUN mkdir -p\n\n# install packages & dependencies \nRUN apt update \\\n    && apt install -y jupyter-notebook python3-matplotlib python3-pandas python3-numpy  \\\n    && rm -rf /var/lib/apt/lists/* # cleanup tmp files created by apt\n    && rm -fr node_modules # remove directory which contains Node.js packages & dependencies\n\n# set working directory (this directory should act as the main application directory)\nWORKDIR /app\n\n# copy files to-from directories\nCOPY /from/path/... /to/path/...\n\n# set environment variables for Conda\nENV\n\n# switch to user 1000 instead of the default root user \nUSER 11042\n\nIn this example, the RUN command updates the package list and installs Jupyter along with all necessary dependencies. It’s common practice to remove unnecessary dependencies afterward, as shown in the example above.\n\n\n\n\n\n\nLabel\n\n\n\nUse key-value pair syntax to add the following labels to the container:\n\nsoftware = name of the app\nauthor = maintainer or author of the app\nversion = version of the app\nlicense = app licence, e.g., “MIT”\ndescription = very short description of the app\n\n\n\nAfter preparing your Dockerfile, use the docker build command to create a new image based on those instructions. Docker’s isolation ensures that any changes made within a container are lost when you exit it. Nevertheless, you can use docker commit to save these changes by creating a new image from the updated container.This new image will retain your modifications and can be used to launch new container.\ndocker build -t &lt;account/app:version&gt; &lt;directory_docker&gt;\n\n\n\n\n\n\nExample of a tag name\n\n\n\nFor the docker build -t flag, the format of the tag is used to specify both the repository and the version of the Docker image. It consists of three different elements (e.g.: albarema/sandbox_app:v1.0):\n\nRepository name where the image will be stored, account in Docker registry (e.g. albarema)\nName of the image (e.g. sandbox_app)\nVersion label for the image (e.g. v1.0, test, etc.)\n\nThe repository name can be deferred until the application is ready for publication on Docker Hub. You can also modify the tag at a later time.\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\n\nCreate a Dockerfile in a project-specific dir (e.g.: sandbox-debian-jupyter). We will add a command to clean up the package after after installation used to reduce the image size.\n\nFROM debian:stable \n\nLABEL maintainer=\"Name Surname &lt;abd123@ku.dk&gt;\"\n\n# Update package list and install necessary packages\nRUN apt update \\\n    && apt install -y jupyter-notebook \\\n                      python3-matplotlib \\\n                      python3-pandas \\\n                      python3-numpy \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/* # cleanup tmp files created by apt\n\n# You may consider adding a working directory\n# WORKDIR /notebooks\n\n# and a command to start Jupyter Notebook\n# CMD [\"jupyter-notebook\", \"--ip=0.0.0.0\", \"--allow-root\"]\n\nBuild the Docker image using, for example, docker build -t sandbox-debian-jupyter:1.0 sandbox-debian-jupyter\n\n\n\n\n\n\n\nTesting the custom image\nLet’s verify if the custom image functions as expected, running the following command:\n\n\nTerminal\n\ndocker run --rm -p 8888:8888 --volume=$(pwd):/root sandbox-debian-jupyter:1.0 jupyter-notebook\n\nJupyter typically refuses to run as root or accept network connections by default. To address this, you need to either add --ip=0.0.0.0 --allow-root when starting Jupyter to the command above or uncomment the last line in the Dockerfile above (CMD [\"jupyter-notebook\", \"--ip=0.0.0.0\", \"--allow-root\"]). Alternatively, you can run the container with the flag --user=$(id -u):$(id -g)to ensure that files created in the container have matching user and group ownership with those on the host machine, preventing permission issues. However, this restricts the container from performing root-level operations. For broader usability and security, it is advisable to create a non-root user (e.g. jovyan) within the Docker image by adding user setup commands to the Dockerfile (see below). This approach makes the image more user-friendly and avoids file ownership conflicts.\n\n\nDockerfile2\n\n\n##-------- ADD CONTENT FROM Dockerfile HERE --------\n\n# Creating a group & user\nRUN addgroup --gid 1000 user && \\\n    adduser --uid 1000 --gid 1000 --gecos \"\" --disabled-password jovyan\n# Setting active user \nUSER jovyan\n# setting working directory \nWORKDIR /home/jovyan\n\n# OPT: command to open jupyter automatically\nCMD [\"jupyter-notebook\", \"--ip=0.0.0.0\"]\n\n\n\n\n\n\n\nTip\n\n\n\n\nUse --rm flag to remove automatically the container once it stops running to avoid clustering your system with stopped containers.\nUse --volume to mount data into the container (/root), for example, your working directory\nUse --file flag to test to dockerfile versions (default:“PATH/Dockerfile”)\n\ndocker build -t sandbox-debian-jupyter:2.0 sandbox-debian-jupyter -f sandbox-debian-jupyter/Dockerfile2\n\n\nNow that we have fixed that problem, we will test A. using a port to launch a Jupyter Notebook (or Rstudio server) and B. starting a bash shell interactively.\n# Assuming the Dockerfile includes CMD [\"jupyter-notebook\", \"--ip=0.0.0.0\"]\n\n# Option A. Start jupyter-notebook or on the server \ndocker run --rm -p 8888:8888 --volume=$(pwd):/home/jovyan sandbox-debian-jupyter:2.0 \n\n# Option B. Start an interactive shells instead \ndocker run -it --rm --volume=$(pwd):/home/jovyan sandbox-debian-jupyter:2.0 /bin/bash\n\n\n\n\n\n\nWhich port to use?\n\n\n\nThe -p option in Docker allows services (e.g.: Jupyter Notebooks) running inside the container to be accessible from outside (through localhost:1234 on your local machine).\n\n-p host_port:container_port # port mappings between the host machine and the container\n-p 8787:8787 # connect port 8787 on the host machine to port 8787 inside the container. This setup allows you to access RStudio, which is running inside the container, by navigating to http://localhost:8787 on your host machine\n-p 8888:8888 # similarly, this setup enables you to access JupyterLab, which is running inside the container, by going to http://localhost:8888 on your host machine.\n\n\n\n\n\nCreate a new app from scratch\nWhen working with containers, you usually need to create a Dockerfile to define your image and compose.yaml file that defines how to run it. As an alternative to starting with a base image and modifying it, you can use the following command:\ndocker init\nThis utility will walk you through creating the following files with sensible defaults for your project:\n- .dockerignore\n- Dockerfile\n- compose.yaml\n- README.Docker.made\n\n\nPublish your Docker image on Docker Hub\nPublishing your Docker image on Docker Hub is straightforward. You just need a Docker Hub account—preferably linked to your GitHub account if you have one. For detailed instructions, refer to the documentation. The process involves only a few commands.\n# login\ndocker login # &lt;username&gt; &lt;pw&gt;\n# optional change tag\n# docker tag &lt;old&gt; &lt;new&gt;\n# push image \ndocker push sandbox-debian-jupyter:1.0\nIn Docker, the file system of an image is built using several layers, or overlays. Each layer represents a set of changes or additions made to the image. When you update software or packages within a Docker container, a new layer is created with only the new or changed content, rather than modifying the existing layers.\nYou are now ready to share the link to your Docker image with your colleagues, ensuring that everyone uses the exact same environment.",
    "crumbs": [
      "HPC Pipes",
      "Computational environments",
      "Docker"
    ]
  },
  {
    "objectID": "develop/e2-docker.html#sources",
    "href": "develop/e2-docker.html#sources",
    "title": "Docker",
    "section": "Sources",
    "text": "Sources\n\nDocker Hub\nContainers and HPC\n\nScientific articles:\n\nAlser, Mohammed, et al. “Packaging and containerization of computational methods.” Nature Protocols (2024): 1-11.",
    "crumbs": [
      "HPC Pipes",
      "Computational environments",
      "Docker"
    ]
  },
  {
    "objectID": "develop/exercise_launch.html",
    "href": "develop/exercise_launch.html",
    "title": "Knowledge Checks",
    "section": "",
    "text": "Put your learning to the test with what you’ve covered so far.",
    "crumbs": [
      "HPC Launch",
      "Quizzes",
      "Knowledge Checks"
    ]
  },
  {
    "objectID": "develop/exercise_launch.html#general-hpc-launch",
    "href": "develop/exercise_launch.html#general-hpc-launch",
    "title": "Knowledge Checks",
    "section": "General HPC launch",
    "text": "General HPC launch\nWhich of the following operations I should do from the front-end (login) nodes:\n1. Unzip a large file unzip myfile.zip to decompress the file? TRUEFALSE\n2. Small folders and files managment? TRUEFALSE\n3. Heavy data transfers? TRUEFALSE\n4. Run computations? TRUEFALSE\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nAvoid running anything from the login node as this will slow down all other users. Create instead an interactive session to do this!\n\n\n\n\n\nAre the following statements true or false?\n5. Is it a good idea to keep data in the scratch folder until the project is finished? TRUEFALSE\n6. I must backup all generated files, inlcuding intermediate files, to make sure the analysis are reproducible. TRUEFALSE\n7. I should not fill up my home folder with data.  TRUEFALSE\n8. Virtual environments keeps project-specific software and their dependencies separted - without interferring with each other.  TRUEFALSE\n9. I must always run the analysis/pipeline on a small subset of the data to estimate CPU/RAM resources. TRUEFALSE",
    "crumbs": [
      "HPC Launch",
      "Quizzes",
      "Knowledge Checks"
    ]
  },
  {
    "objectID": "develop/hpc-programming.html",
    "href": "develop/hpc-programming.html",
    "title": "HPC coding",
    "section": "",
    "text": "This section outlines useful best practices to consider when coding and writing new software on an HPC.",
    "crumbs": [
      "HPC Launch",
      "HPC systems",
      "HPC coding"
    ]
  },
  {
    "objectID": "develop/hpc-programming.html#coding-practices",
    "href": "develop/hpc-programming.html#coding-practices",
    "title": "HPC coding",
    "section": "Coding practices",
    "text": "Coding practices\n\nCode coverage, testing, and continuous integration\nTesting is a critical aspect of coding that should be performed regularly throughout a project. Here are some primary types of tests to consider:\n\nRegression Test: Given a specific input, the code is tested to reproduce the expected output.\nUnit Test: Tests the smallest units of the software (e.g., individual functions) to identify bugs, particularly under extreme input and output conditions.\nContinuous Integration: A suite of tests runs automatically every time the software code is updated, helping to catch bugs before anyone uses the code.\n\nAdditional aspects to test include the performance and scalability of the code, usability, and response to all intended types of input data.\nWhile unit and regression tests are valuable, they may become unfeasible as the codebase grows in size and complexity. Therefore, it’s advisable to use continuous integration and implement simple yet representative tests that cover the entire codebase, enabling the early detection of bugs before end-users encounter them. Code coverage tools are available for various programming languages and can also be used for testing code deployed on GitHub version control.\n\n\n\nLink\nDescription\n\n\n\n\npyTest\nPackage to test python code\n\n\nCmake\nTool to test both C, C++ and Fortran code\n\n\nTravis CI\nTool for continuous integration in most of the used programming languages. Works on Git version control.\n\n\ncovr\nPackage to test coverage reports for R\n\n\n\n\n\nCode styling\nAn essential aspect of code is its readability by others. To achieve this, a clean and consistent coding style should be employed throughout the project. Some languages have established preferred coding styles, which can often be enforced in certain IDEs (e.g. Visual Studio Code). While you can adopt your own coding style, it should prioritize readability and be consistently applied across the entire project. Here are some general code styling tools:\n\n\n\nTool & Link\nDescription\n\n\n\n\nstyleguide\nGoogle guide for coding styles of the major programming languages\n\n\nawesome guidelines\nA guide to coding styles covering also documentations, tools and development environments\n\n\n\nClick on the callout below if you want to learn about language-specific tools for code formatting.\n\n\n\n\n\n\nFormatting tools\n\n\n\n\n\n\n\n\nLanguage\nFormatted tools\n\n\n\n\nPython\nBlack, yapf, read intro Pythonic rules\n\n\nR\nformatR, read post R style\n\n\nSnakemake\nSnakefmt\n\n\nBash/Shell\nShellIndent\n\n\nC/C++\nGNUIndent, GreatCode\n\n\nPerl\nPerlTidy\n\n\nJavascript\nbeautifier\n\n\nMATLAB/Octove\nMISS_HIT\n\n\nJava\nGoogle Java format, JIndent\n\n\nCSS\nCSSTidy\n\n\nHTML\nTidy\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nQuick Tip: If you use VS Code as your main text editor, you can enable automatic code formatting in your browser. Go to your preferences page in JSON mode and add:\n\n\njson\n\n\"editor.formatOnSave\": true\n\n\n\n\n\nPackaging a coding project\nWhen developing software that includes multiple newly implemented functions, organizing these functions into a package can be beneficial for reuse and easy sharing. This approach is particularly straightforward for coding projects in both Python and R, allowing developers to streamline their workflow and enhance collaboration.\n\n\n\nLink\nDescription\n\n\n\n\npyPA\npython packaging user guide\n\n\nR package development\nDevelop an R package using Rstudio\n\n\n\n\n\nCode documentation\nWhen developing software, it’s essential to create documentation that clearly explains the usage of each code element. For software packages, there are tools available that can automatically generate documentation by utilizing function declarations and any accompanying text included as strings within the code.\n\n\n\nTool & Link\nDescription\n\n\n\n\nMkDocs\nA generator for static webpages, with design and themes targeted to documentation pages, but also other type of websites. This website is itself made with MkDocs.\n\n\nPython - mkdocstrings\nPython handler to automatically generate documentation with MkDocs\n\n\npdoc3\nA package who creates automatically the documentation for your coding projects. It is semi automatic (infers your dependencies, classes, … but adds a description based on your docstrings)\n\n\npdoc3 101\nHow to run pdoc to create an html documentation\n\n\nR-Roxygen2\nA package to generate R documentation - it can be used also with Rcpp\n\n\nSphinx\nAnother tool to write documentation - it produces also printable outputs. Sphinx was first created to write the python language documentation. Even though it is a tool especially thought for python code, it can be used to generate static webpages for other projects.",
    "crumbs": [
      "HPC Launch",
      "HPC systems",
      "HPC coding"
    ]
  },
  {
    "objectID": "develop/hpc-programming.html#parallel-programming",
    "href": "develop/hpc-programming.html#parallel-programming",
    "title": "HPC coding",
    "section": "Parallel programming",
    "text": "Parallel programming\nAn HPC system can be used for both sequential and parallel programming. Sequential programming involves writing computer programs that execute one instruction at a time, following a logical sequence of steps. In contrast, parallel programming allows multiple instructions to run simultaneously.\nWhile there is typically only one approach to writing sequential code, there are various methods for creating parallelized code. ## Parallel Programming\n\nOpenMP (Multithreading)\nOne popular approach to parallel programming is through OpenMP, where developers write sequential code and identify specific sections that can be parallelized into threads using a fork-join mechanism. Each thread operates independently and has its own allocated memory, as illustrated in the figure below from ADMIN magazine.\n\n\n\nOpenMP Diagram\n\n\nIf the execution times of threads vary, some may have to wait for others to complete when they need to be joined (e.g. to collect data), leading to inefficient use of execution time. It is the programmer’s responsibility to balance the distribution of threads to optimize performance.\nModern CPUs inherently support OpenMP, particularly multicore CPUs, which can run threads independently. OpenMP is available as an extension for programming languages such as C and Fortran, and is commonly used to parallelize for loops that create performance bottlenecks in software execution.\n\n\n\nLink\nDescription\n\n\n\n\nVideo Course\nA video course hosted by ARCHER UK, linked to the first lesson with access to all subsequent lessons.\n\n\nOpenMP Starter Guide\nA beginner’s guide to OpenMP.\n\n\nWikitolearn OpenMP Course\nAn OpenMP course available on Wikitolearn.\n\n\nMIT OpenMP Course\nA comprehensive course from MIT that also covers MPI usage.\n\n\n\n\n\nMPI (Message Passing Interface)\nMPI facilitates the distribution of data among different processes that cannot otherwise access it. This is illustrated in the image below from LLNL.\n\n\n\nMPI Diagram\n\n\nAlthough MPI is often considered difficult to learn, this reputation stems from the explicit programming required for message passing.\n\n\n\nLink\nDescription\n\n\n\n\nVideo Course\nA video course hosted by ARCHER UK, linked to the first lesson with access to all subsequent lessons.\n\n\nMPI Starter Guide\nA beginner’s guide to MPI.\n\n\nPRACE Course\nA PRACE course on the MOCC platform, FutureLearn.\n\n\n\n\n\nGPU Programming\nGPUs (Graphics Processing Unit) serve as computing accelerators, significantly enhancing the performance of heavy linear algebra applications, such as deep learning. A GPU typically comprises numerous specialized processing units, enabling extreme parallelization of computer code, as shown in the figure below from astrocomputing.\n\n\n\nGPU Computing\n\n\nAMD and Nvidia are the two primary GPU manufacturers, with Nvidia maintaining a dominant position in the market for many years. Danish HPCs Type 1 and 2 feature various Nvidia graphic card models, while Type 5 (LUMI) includes the latest AMD Instinct cards. The distinction between AMD and Nvidia primarily lies in their programming dialects, necessitating specific coding for multithreading tailored to each GPU brand.\n\nNvidia CUDA\nCUDA is a dialect of C++ that also offers various libraries for popular languages and frameworks (e.g., Python, PyTorch, MATLAB, etc.).\n\n\n\nLink\nDescription\n\n\n\n\nNvidia Developer Training\nTraining resources for CUDA programming provided by Nvidia.\n\n\nCUDA Book Archive\nAn archive of books focused on CUDA programming.\n\n\nAdvanced CUDA Books\nA collection of advanced books for CUDA programming.\n\n\npyCUDA\nResources for coding in CUDA using Python.\n\n\n\n\n\nAMD HIP\nHIP is a newly introduced dialect for AMD GPUs that can be compiled for both AMD and Nvidia hardware. The advantage of HIP is that it allows CUDA code to be converted to HIP code with minimal adjustments by the programmer.\nThe LUMI HPC consortium has organized courses focused on HIP programming and CUDA-to-HIP conversion. Check their page for upcoming courses.\n\n\n\nLink\nDescription\n\n\n\n\nVideo Introduction 1\nAn introductory video on HIP programming.\n\n\nVideo Introduction 2\nA second introductory video on HIP programming.\n\n\nAMD Programming Guide\nThe official programming guide for HIP from AMD.",
    "crumbs": [
      "HPC Launch",
      "HPC systems",
      "HPC coding"
    ]
  },
  {
    "objectID": "develop/scripts/01.iris.html",
    "href": "develop/scripts/01.iris.html",
    "title": "Notebook Iris Dataset",
    "section": "",
    "text": "Before we dive in, here’s a quick summary: the dataset contains 150 samples of iris flowers, each characterized by four features: Sepal Length, Sepal Width, Petal Length, and Petal Width, all measured in centimeters. These samples are grouped into three species: Setosa, Versicolor, and Virginica. If you’re not familiar with the dataset, you can learn more about it here."
  },
  {
    "objectID": "develop/scripts/01.iris.html#loading-the-dataset",
    "href": "develop/scripts/01.iris.html#loading-the-dataset",
    "title": "Notebook Iris Dataset",
    "section": "1 Loading the dataset",
    "text": "1 Loading the dataset\nLet’s start by importing the iris dataset and manipulating the dataframe so that the column names match the feature names.\n\nfrom sklearn.datasets import load_iris\nimport matplotlib.pyplot as plt\nimport pandas as pd \n\n# Load toy dataset\niris = load_iris() \n # Create dataframe using feature names\ndf = pd.DataFrame(iris.data, columns=iris.feature_names)"
  },
  {
    "objectID": "develop/scripts/01.iris.html#exploring-the-dataset",
    "href": "develop/scripts/01.iris.html#exploring-the-dataset",
    "title": "Notebook Iris Dataset",
    "section": "2 Exploring the dataset",
    "text": "2 Exploring the dataset\nLet´s start by exploring the species by plotting the sepal length vs. width in a scatter plot\n\nimport matplotlib.pyplot as plt\n\n_, ax = plt.subplots()\nscatter = ax.scatter(iris.data[:, 0], iris.data[:, 1], c=iris.target)\nax.set(xlabel=iris.feature_names[0], ylabel=iris.feature_names[1])\n_ = ax.legend(\n    scatter.legend_elements()[0], iris.target_names, loc=\"lower right\", title=\"Classes\"\n)\n\n\n\n\n\n\n\n\nYou can already see a pattern regarding the Setosa type, which is easily identifiable based on its short and wide sepal. Only considering these 2 dimensions, sepal width and length, there’s still overlap between the Versicolor and Virginica types."
  },
  {
    "objectID": "develop/scripts/01.iris.html#transforming-the-dataset",
    "href": "develop/scripts/01.iris.html#transforming-the-dataset",
    "title": "Notebook Iris Dataset",
    "section": "3 Transforming the dataset",
    "text": "3 Transforming the dataset\nWe will now perform feature engineering and create a new feature called petal area (petal length * petal width), and will do the same for the sepal.\n\ndf['sepal_area'] = df['sepal length (cm)'] * df['sepal width (cm)']\ndf['petal_area'] = df['petal length (cm)'] * df['petal width (cm)']\n\nFinally, let’s new information by binning the sepal length into 3 categories (short, medium and long)\n\ndf['sepal_length_bin'] = pd.cut(df['sepal length (cm)'], bins=3, labels=[\"short\", \"medium\", \"long\"])\ndf\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\nsepal_area\npetal_area\nsepal_length_bin\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n17.85\n0.28\nshort\n\n\n1\n4.9\n3.0\n1.4\n0.2\n14.70\n0.28\nshort\n\n\n2\n4.7\n3.2\n1.3\n0.2\n15.04\n0.26\nshort\n\n\n3\n4.6\n3.1\n1.5\n0.2\n14.26\n0.30\nshort\n\n\n4\n5.0\n3.6\n1.4\n0.2\n18.00\n0.28\nshort\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\n20.10\n11.96\nmedium\n\n\n146\n6.3\n2.5\n5.0\n1.9\n15.75\n9.50\nmedium\n\n\n147\n6.5\n3.0\n5.2\n2.0\n19.50\n10.40\nmedium\n\n\n148\n6.2\n3.4\n5.4\n2.3\n21.08\n12.42\nmedium\n\n\n149\n5.9\n3.0\n5.1\n1.8\n17.70\n9.18\nmedium\n\n\n\n\n150 rows × 7 columns"
  },
  {
    "objectID": "develop/scripts/01.iris.html#computing-summary-statistics",
    "href": "develop/scripts/01.iris.html#computing-summary-statistics",
    "title": "Notebook Iris Dataset",
    "section": "4 Computing summary statistics",
    "text": "4 Computing summary statistics\nNow, we can extract summary statistics of the species “setosa” and compare it to another species\n\n# Map targets to species names and add them to a new column \ndf['species'] = iris.target_names[iris.target] \n# Display first few rows\ndf.head() \n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\nsepal_area\npetal_area\nsepal_length_bin\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n17.85\n0.28\nshort\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\n14.70\n0.28\nshort\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\n15.04\n0.26\nshort\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\n14.26\n0.30\nshort\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\n18.00\n0.28\nshort\nsetosa\n\n\n\n\n\n\n\n\n# Select setosa \ndf_setosa = df[df['species'] == \"setosa\"]\nsummary_stats = df_setosa.describe() \n\n# Display summary statistics\nprint(summary_stats)\n\n       sepal length (cm)  sepal width (cm)  petal length (cm)  \\\ncount           50.00000         50.000000          50.000000   \nmean             5.00600          3.428000           1.462000   \nstd              0.35249          0.379064           0.173664   \nmin              4.30000          2.300000           1.000000   \n25%              4.80000          3.200000           1.400000   \n50%              5.00000          3.400000           1.500000   \n75%              5.20000          3.675000           1.575000   \nmax              5.80000          4.400000           1.900000   \n\n       petal width (cm)  sepal_area  petal_area  \ncount         50.000000   50.000000   50.000000  \nmean           0.246000   17.257800    0.365600  \nstd            0.105386    2.933775    0.181155  \nmin            0.100000   10.350000    0.110000  \n25%            0.200000   15.040000    0.280000  \n50%            0.200000   17.170000    0.300000  \n75%            0.300000   19.155000    0.420000  \nmax            0.600000   25.080000    0.960000"
  },
  {
    "objectID": "develop/hpc-jobs.html",
    "href": "develop/hpc-jobs.html",
    "title": "HPC jobs",
    "section": "",
    "text": "This section outlines useful best practices to consider when coding and running applications and pipelines on an HPC.",
    "crumbs": [
      "HPC Launch",
      "HPC systems",
      "HPC jobs"
    ]
  },
  {
    "objectID": "develop/hpc-jobs.html#job-scheduler",
    "href": "develop/hpc-jobs.html#job-scheduler",
    "title": "HPC jobs",
    "section": "Job scheduler",
    "text": "Job scheduler\nTo execute jobs in most HPC environments, you must first submit them to a queueing system. This system ensures that compute resources are allocated fairly and efficiently among users while scheduling the execution of jobs. Some of the most common queueing systems operate using SLURM (utilizing s-type commands), TORQUE (utilizing q-type commands), and Moab (utilizing m-type commands).\n\n\n\nProgram\nlink\n\n\n\n\nTORQUE qsub documentation\nLink\n\n\nMoab msub documentation\nLink\n\n\nSLURM sbatch/Srun documentation\nsbatch and srun\n\n\n\nWe will focus on SLURM (Simple Linux Utility for Resource Management), a widely used open-source job scheduler designed to manage and allocate resources in high-performance computing (HPC) clusters. It efficiently schedules and runs batch jobs, handles job queues and optimizes resource utilization across multiple users and tasks.\n\nSubmitting Jobs using ‘sbatch’\nIt is ideal for running programs non-interactively, typically for tasks that require more time than a brief interactive session. A batch script includes:\n\nThe requested resources\nA sequence of commands to execute\n\n\n\n\n\n\n\nNote\n\n\n\nUseful SLURM commands:\n# Submit the job\nsbatch \n\n# This will show you the status of your job, including its current state, the amount of time it has been running, and the amount of resources it is currently using.\nsqueue -j job_id\n\n# Cancel a job\nscancel job_id1, job_id2\nHere is example of a bash script to submit to the queueing system:\n\n\nmybash.sh\n\n#!/bin/bash\n#SBATCH --job-name=myjob\n#SBATCH --output=myjob.out\n#SBATCH --error=myjob.err\n#SBATCH -D /home/USERNAME/  # working directory\n#SBATCH --cpus-per-task 1        # number of CPUs. Commonly default: 1\n#SBATCH --time 00:10:00 # time for the job HH:MM:SS.\n#SBATCH --mem=1G    # RAM memory\n##SBATCH --array=1-10%4 # this directive submits a job array of 10 tasks, but only 4 of them can run concurrently. Alternative: #SBATCH --ntasks=1\n\n# Activate environment\neval \"$(conda shell.bash hook)\"\nconda activate bam_tools\n\n# My commands: software, pipeline, etc.\nsnakemake -j1\n\nexit 0\n\nThe first line of the script (#!/bin/bash) tells the system that this is a bash script. The remaining lines starting with #SBATCH are directives for ‘sbatch’ that specify various options for the job.\nFinally, we can submit your job to the queueing system:\n{.bash filename=\"\"Terminal} sbatch mybash.sh\nSubmitted batch job 39712574\nCheck details about a specific job (e.g. 39712574) using the following command:\njobinfo 39712574\nYou can also include email notifications in your bash script by adding the following options:\n#SBATCH --mail-type=begin        # send email when job begins\n#SBATCH --mail-type=end          # send email when job ends\n#SBATCH --mail-type=fail         # send email if job fails\n#SBATCH --mail-user=your mail address\n\n\nTo monitor the job’s output in real-time, refresh the last few lines of its log file using:\nwatch tail align.sh-39712574.out\nTo view the entire log file (not in real-time), you can check it anytime with:\nless -S align.sh-39712574.out\nReviewing the log files is helpful for debugging, especially when a command encounters an error and causes the job to terminate prematurely.\nIn the figure below, you can see how the priority assigned to a SLURM job decreases as the requested time increases, while keeping memory and CPU resources constant. Higher values indicate lower priority.\n\n\n\nSlurm priority, figure adapted from Simakov et al. (2018)\n\n\n\n\n\nDescription\nLinks\n\n\n\n\nSlurm official guide\nQuick start\n\n\nSlurm cheat sheet\nCheat Sheet\n\n\nSlurm Universities usage examples\nGenomeDK and Princeton guides\n\n\nGwf, a simple python tool to create interdependent job submissions\nGwf, developed at the University of Aarhus, makes it easy to create Slurm jobs and organize them as a pipeline with dependencies, using the python language (you need python 3.5+). You get to simply create the shell scripts and the dependencies, without the complicating syntax of Slurm. The page contains also a useful guide.\n\n\n\n\n\nJob parallelization\nJob parallelization is crucial for achieving high performance and running jobs effectively on an HPC. Here are two key scenarios where it is particularly important:\n\nIndependent computational tasks: When tasks are independent of each other, job parallelization can enhance efficiency by allowing them to run concurrently.\nMulti-threaded tools: Some tools are specifically designed to perform parallel computations through multi-threading, enabling them to utilize multiple CPU cores for increased performance.\n\n\n\n\n\n\n\nJob parallelization using slurm\n\n\n\nJobs arrays -a",
    "crumbs": [
      "HPC Launch",
      "HPC systems",
      "HPC jobs"
    ]
  },
  {
    "objectID": "develop/hpc-jobs.html#interactive-jobs",
    "href": "develop/hpc-jobs.html#interactive-jobs",
    "title": "HPC jobs",
    "section": "Interactive jobs",
    "text": "Interactive jobs\nUse this option when you need to execute tasks that don’t require automation or repetitive scheduling. You can work interactively, monitor progress, and adjust commands as needed in real-time. Examples:\n\nSplitting a BAM file by chromosome for further analysis.\nRunning exploratory or simple statistical analysis in Python or R.\nCompressing or decompressing multiple files in parallel required as input for a pipeline.\n\nKeep in mind, that once you exit the session, any processes running within it will stop automatically. This makes it ideal for tasks that require manual intervention or those that don’t need continuous execution after logging out.\nHow do interactive jobs work?\nThe queuing system schedules your job based on the resources you request (such as CPU, memory, and time requirements) and the current workload of the nodes. Once a node is assigned, the requested resources will be available to you, and the node’s name will appear in your terminal prompt.\nSlurm provides the ‘srun -pty bash’ command to submit interactive jobs on a compute node. Specify the resource requirements by including one or more of the following options:\n\n--cpus-per-task: Number of CPUs per task\n--mem: Memory per node (e.g., --mem=4G)\n--time: Time limit for the job (e.g., --time=2:00:00 for 2 hours)\n\n\n\nTerminal\n\n# Slurm example\nsrun --mem=&lt;GB_of_RAM&gt;g -c &lt;n_cores&gt; --time=&lt;days-h:min:sec&gt; --pty /bin/bash\n\nFor example, to request 2 CPUs, 4 GB of memory, and 1 hour wall time, use the following command:\n\n\nslurm example\n\nsrun --cpus-per-task=2 --mem=4G --time=1:00:00 --pty bash",
    "crumbs": [
      "HPC Launch",
      "HPC systems",
      "HPC jobs"
    ]
  },
  {
    "objectID": "develop/hpc-jobs.html#efficient-resource-usage",
    "href": "develop/hpc-jobs.html#efficient-resource-usage",
    "title": "HPC jobs",
    "section": "Efficient resource usage",
    "text": "Efficient resource usage\n\nManaging a large number of short jobs\nEach time a job is submitted to the job manager (e.g., SLURM) in a computing cluster, there’s a time overhead required for resource allocation, output preparation, and queue organization. To minimize this overhead, it’s often more efficient to group tasks into longer jobs when possible.\nHowever, it’s important to strike a balance. If jobs are too long and encounter an issue, significant time and resources can be lost. This risk can be mitigated by tracking outputs at each stage, ensuring you only rerun the necessary portions of the job. For example, by checking whether a specific output already exists, you can prevent redundant computations and reduce wasted effort.\nA particularly powerful feature in queue systems like SLURM is batch arrays. These allow you to automate running large numbers of similar jobs. A batch array consists of multiple jobs with identical code and parameters but with different input files. Each job in the array is assigned a unique index, passed as an argument to the job script. This greatly simplifies managing and executing large-scale tasks.\n\n\n\nDescription\nLinks\n\n\n\n\nSLURM tutorial on job arrays\nJob Array\n\n\nSLURM cheat sheet\nCheat Sheet\n\n\nSLURM guide\nQuick start\n\n\n\nWorkflow managers can also assist in automating and tracking jobs, ensuring that resources are efficiently allocated while reducing overhead and preventing errors in complex workflows.\n\n\nManaging large STOUT outputs\nMinimize the amount of information printed to the standard output (STDOUT) to avoid overwhelming the terminal screen. Excessive outputs can become problematic, especially when numerous parallel jobs are running, potentially cluttering the home directory and leading to errors or data loss. Instead, consider directing outputs to software-specific data formats (like .RData files for R) or, at the very least, to plain text files. This approach helps maintain a clean workspace and reduces the risk of encountering issues related to excessive STDOUT content.",
    "crumbs": [
      "HPC Launch",
      "HPC systems",
      "HPC jobs"
    ]
  },
  {
    "objectID": "develop/hpc-jobs.html#queueing-system-best-practices",
    "href": "develop/hpc-jobs.html#queueing-system-best-practices",
    "title": "HPC jobs",
    "section": "Queueing system best practices",
    "text": "Queueing system best practices\n\nAvoid running jobs or scripts on the login nodes.\nSubmit batch jobs using the sbatch command and ensure that your submission scripts primarily consist of queueing system parameters and job executions.\nIntroduce a delay between job submissions when submitting multiple jobs to prevent overwhelming the system.\nUtilize job arrays for submitting multiple identical jobs efficiently.\nUse interactive sessions for testing and interactive jobs.\nIncorporate software modules in your pipelines for improved environment control.\nEstimate resource requirements before submitting jobs, including CPU, memory, and time, to optimize resource usage. Always test your code with a small, representative sample (toy example) to ensure the pipeline functions correctly before running larger jobs.",
    "crumbs": [
      "HPC Launch",
      "HPC systems",
      "HPC jobs"
    ]
  }
]
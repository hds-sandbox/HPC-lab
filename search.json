[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "HPC best practices",
    "section": "",
    "text": "High Performance Computing (HPC) plays a crucial role for researchers by offering the computational speed and power needed to manage large and complex data sets, perform simulations, and address intricate problems that would be impractical or too time-consuming with standard computing methods.\nKnowing which HPC resources are accessible and how to use them efficiently is essential for researchers. Making the most of these resources can significantly expedite research and drive innovation. By becoming proficient with the available tools and technologies, researchers can address complex challenges, analyze extensive data sets, and execute advanced simulations with increased speed and accuracy. This module provides essential knowledge on HPC resources and best practices for their utilization.\nThis module offers content for three distinct courses:\n\nHPC Launch: Foundations on HPC and essential knowledge on national HPC resources\nHPC Pipes: Best practices for using workflow management systems and computational environments with HPC\nHPC ML (Machine Learning): Insights into applying HPC for machine learning tasks, including model training, data analysis, and optimization techniques.\n\nBy the end of all the modules, you will gain practical skills in promoting reproducibility through comprehensive training in HPC resource management, workflow pipelines, and computing environments.\n\n\n\n\n\n\nGeneral Course Goals\n\n\n\nBy the end of this workshop, you should be able to apply the following concepts in the context of Next Generation Sequencing data:\n\nUnderstand the Importance of Research Data Management (RDM)\nMake your data analysis and workflows reproducible and FAIR\nMake FAIR environment using conda or Docker\n\n\n\n\n\n\n\n\n\nHPC best practices\n\n\n\nWe offer in-person workshops, keep an eye on the upcoming events on the Sandbox website."
  },
  {
    "objectID": "index.html#hpc-pipes",
    "href": "index.html#hpc-pipes",
    "title": "HPC best practices",
    "section": "HPC pipes",
    "text": "HPC pipes\nThe course “HPC pipes” is designed to provide participants with foundational knowledge and practical skills in writing reproducible pipelines. As part of effective data management, it is crucial that researchers create reproducible analyses that enable others to validate and build upon their work. We will explore essential elements of reproducibility and efficiency in computational research, highlighting techniques and tools for creating robust and transparent coding and workflows. By prioritizing reproducibility and replicability, researchers can enhance the credibility and impact of their findings while fostering collaboration and knowledge dissemination within the scientific community. This approach guarantees efficient research management. Explore our content on practical RDM for more details.\n\n\n\n\n\n\nCourse Requirements\n\n\n\nIt is important to be familiar with unix and python. Follow the tutorials in the links below if you need a refresher.\n\nCommand Line experience (Software Carprentry Shell)\nProgramming experience (Python)\n\n\n\n\n\n\n\n\n\nModule Goals\n\n\n\n\nUnderstand the rol of scientific pipelines\nRun existing pipelines\nImplement and modify pipelines\nSpecify software and computational resource needs\nCustomise your pipeline to accept user-defined configurations (params)\nCreate reproducible analyses that can be adapted to new data with little effort\nIntegrate workflows with software environments\n\n\n\n\nAcknowledgements\nOur exercises are developed using the R packaged developed by Barr and DeBruine (2023)."
  },
  {
    "objectID": "develop/jobs.html",
    "href": "develop/jobs.html",
    "title": "HPC jobs",
    "section": "",
    "text": "Note\n\n\n\nThere are many job scheduler programs available but a very common one is SLURM. Useful commands:\n# Submit the job\nsbatch \n# Check the job is in the queue\nsqueue \nToy example of a bash script to submit\n#!/bin/bash\n#SBATCH -D /home/USERNAME/  # working directory\n#SBATCH -c 1        # number of CPUs. Default: 1\n#SBATCH -t 00:10:00 # time for the job HH:MM:SS.\n#SBATCH --mem=1G    # RAM memory\n\n# my commands: software, pipeline, etc.\nsnakemake -j1\n\n\n\n\n\nJob parallelization is crucial for achieving high performance and running jobs effectively on an HPC. Here are two key scenarios where it is particularly important:\n\nIndependent computational tasks: When tasks are independent of each other, job parallelization can enhance efficiency by allowing them to run concurrently.\nMulti-threaded tools: Some tools are specifically designed to perform parallel computations through multi-threading, enabling them to utilize multiple CPU cores for increased performance.\n\n\n\n\n\n\n\nJob parallelisation using slurm\n\n\n\nJobs arrays -a",
    "crumbs": [
      "HPC launch",
      "HPC systems",
      "HPC jobs"
    ]
  },
  {
    "objectID": "develop/jobs.html#best-practices-for-running-a-job-on-a-cluster",
    "href": "develop/jobs.html#best-practices-for-running-a-job-on-a-cluster",
    "title": "HPC jobs",
    "section": "",
    "text": "Note\n\n\n\nThere are many job scheduler programs available but a very common one is SLURM. Useful commands:\n# Submit the job\nsbatch \n# Check the job is in the queue\nsqueue \nToy example of a bash script to submit\n#!/bin/bash\n#SBATCH -D /home/USERNAME/  # working directory\n#SBATCH -c 1        # number of CPUs. Default: 1\n#SBATCH -t 00:10:00 # time for the job HH:MM:SS.\n#SBATCH --mem=1G    # RAM memory\n\n# my commands: software, pipeline, etc.\nsnakemake -j1\n\n\n\n\n\nJob parallelization is crucial for achieving high performance and running jobs effectively on an HPC. Here are two key scenarios where it is particularly important:\n\nIndependent computational tasks: When tasks are independent of each other, job parallelization can enhance efficiency by allowing them to run concurrently.\nMulti-threaded tools: Some tools are specifically designed to perform parallel computations through multi-threading, enabling them to utilize multiple CPU cores for increased performance.\n\n\n\n\n\n\n\nJob parallelisation using slurm\n\n\n\nJobs arrays -a",
    "crumbs": [
      "HPC launch",
      "HPC systems",
      "HPC jobs"
    ]
  },
  {
    "objectID": "develop/jobs.html#sources",
    "href": "develop/jobs.html#sources",
    "title": "HPC jobs",
    "section": "Sources",
    "text": "Sources\nUseful links\n\nAcknowledgements",
    "crumbs": [
      "HPC launch",
      "HPC systems",
      "HPC jobs"
    ]
  },
  {
    "objectID": "develop/exercise_launch.html",
    "href": "develop/exercise_launch.html",
    "title": "Exercises",
    "section": "",
    "text": "Put your learning to the test with what you’ve covered so far.",
    "crumbs": [
      "HPC launch",
      "Exercises"
    ]
  },
  {
    "objectID": "develop/exercise_launch.html#general-hpc-launch",
    "href": "develop/exercise_launch.html#general-hpc-launch",
    "title": "Exercises",
    "section": "General HPC launch",
    "text": "General HPC launch\n1. Should I run the commmand unzip myfile.zip to decompress the file from the login node? TRUEFALSE\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nAvoid running anything from the login node. Create an interactive session to do this!\n\n\n\n\n\n2. Is it a good idea to keep data in the scratch folder until the project is finished? TRUEFALSE",
    "crumbs": [
      "HPC launch",
      "Exercises"
    ]
  },
  {
    "objectID": "develop/nextflow.html",
    "href": "develop/nextflow.html",
    "title": "Nextflow",
    "section": "",
    "text": "Nextflow is a workfow management system that offers scalable and portable NGS data analysis pipelines, facilitating data processing across diverse computing environments. It streamlines and automates various data analysis steps, enabling parallel processing and seamless integration with existing tools. ç",
    "crumbs": [
      "HPC pipes",
      "Pipeline languages",
      "Nextflow"
    ]
  },
  {
    "objectID": "develop/nextflow.html#nf-core",
    "href": "develop/nextflow.html#nf-core",
    "title": "Nextflow",
    "section": "nf-core",
    "text": "nf-core\nTemplates",
    "crumbs": [
      "HPC pipes",
      "Pipeline languages",
      "Nextflow"
    ]
  },
  {
    "objectID": "develop/nextflow.html#sources",
    "href": "develop/nextflow.html#sources",
    "title": "Nextflow",
    "section": "Sources",
    "text": "Sources\n\nfaircookbook worflows",
    "crumbs": [
      "HPC pipes",
      "Pipeline languages",
      "Nextflow"
    ]
  },
  {
    "objectID": "develop/fair_envs.html",
    "href": "develop/fair_envs.html",
    "title": "FAIR environments",
    "section": "",
    "text": "Computational environments vary significantly from one system to another, encompassing differences in the operating system, installed software, and software package versions. If a research project is moved to a different computer or platform, the analysis might not run or produce consistent results, specially if the software relies on specific configurations or dependencies. Dependencies are libraries or tools that software relies on and can change over time or be poorly documented, leading to hidden variations between setups. Simply knowing a software version might often be insufficient for guaranteeing consistent results across different environments.\nFor research to be reproducible, the original computational environment must be recorded so others can replicate it accurately. This involves making your code easy to install and run by others, document the setup process thoroughly, and carefully manage and share your software environment. There are several methods to achieve this:\nWhile package managers are very easy to use and share across different systems, and are lightweight and efficient, offering fast start-up times, containers provide a full environment isolation (including the operating system) which ensures consistent behavior across different systems. Check the table below for a full overview.\nRecording and sharing the computational environment is essential for ensuring reproducibility and transparency. Below, we will explore two tools that can help with this: mamba, a package manager, and Docker, a container system. We will explain the differences between them and provide guidance on choosing the right tool for your specific scenario.",
    "crumbs": [
      "HPC pipes",
      "Computational environments",
      "FAIR environments"
    ]
  },
  {
    "objectID": "develop/fair_envs.html#package-managers",
    "href": "develop/fair_envs.html#package-managers",
    "title": "FAIR environments",
    "section": "Package managers",
    "text": "Package managers\nMamba is a reimplementation of the Conda package manager in C++. While our focus will be on Mamba, it’s important to note that it maintains compatibility with Conda by using the same command-line parser, package installation and uninstallation code, and transaction verification routines.\nMamba uses software installation specifications that are maintained by extensive communities of developers, organized into channels, which serve as software repositories. For example, the “bioconda” channel specializes in bioinformatics tools, while “conda-forge” covers a broad range of data science packages.\n\n\n\n\n\n\nMamba vs. conda\n\n\n\nAs previously mentioned, mamba is a newer and faster implementation. The two commands can be used interchangeable (for most tasks). If you use Conda, you should still complete the exercises, as you’ll gain experience with both tools. For more information on their ecosystem and advantages here.\n\n\nMamba allows you to create different software environments, where multiple package version can co-exit on your system.\n\n\n\n\n\n\nBuild your mamba environment\n\n\n\n\n\n\n\nFollow mamba instructions to install it. Let’s also include bioconda and conda-forge channels which will come very handy.\nconda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge\nNow you are set to create your first environment. Follow these steps:\n\nCreate a new environment named myenv\nInstall the following packages in myenv: bowtie2, numpy=1.26.4, matplotlib=3.8.3\nCheck the environments available\nLoad/activate the environment\nCheck which python executable is being used and that bowtie2 is installed.\nDeactivate the environment\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nHere are some of the commands you need for the exercise.\nmamba create -n &lt;ENV-NAME&gt;\nmamba install --channel &lt;CHANNEL-NAME&gt; --name &lt;ENV-NAME&gt;\nmamba env list\n# mamba init \nmamba activate &lt;ENV-NAME&gt;\nmamba deactivate \n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThe syntax to create a new environment is: mamba create --name myenv\nExample “bowtie2”: Go to anaconda.org and search for “bowtie2” to confirm it is available through Mamba and which software channel it is provided from. You will find that it is available via the “bioconda” channel: https://anaconda.org/bioconda/bowtie2. The syntax to install packages is: mamba install --channel &lt;CHANNEL-NAME&gt; --name &lt;ENV-NAME&gt; &lt;SOFTWARE-NAME&gt;\n\nmamba install --name myenv --channel bioconda bowtie2=2.5.3 \"matplotlib=3.8.3\" \"numpy=1.26.4\"\nDo the same with the others. 3. To see al environments available mamba env list. There will be a “*” showi8ng the one is activated. 4. Load the environment mamba activate myenv. 5. which python -&gt; should print the one in the environment that is active (path similar to /home/mambaforge/envs/myenv/bin/python). bowtie2 --help 6. Conda deactivate\n\n\n\n\n\n\n\n\nIf you have different environments set up for various projects, you can switch between them or run commands directly within a specific environment using:\nmamba run -n &lt;ENV-NAME&gt; python myscript.py\n\n\n\n\n\n\nLoading mamba environments in shell scripts\n\n\n\nIf you need to activate an environment in a shell script that will be submitted to SLURM, you must first source Mamba’s configuration file. For instance, to load the myenv environment we created, the script would include the following code:\n# Always add these two commands to your scripts\neval \"$(conda shell.bash hook)\"\nsource $CONDA_PREFIX/etc/profile.d/mamba.sh\n\n# then you can activate the environment\nmamba activate myenv\nWhen jobs are submitted to SLURM, they run in a non-interactive shell where Mamba isn’t automatically set up. By running the source command, you ensure that Mamba’s activate function is available. It’s important to remember that even if the environment is loaded on the login node, the scripts will execute on a different machine (one of the compute nodes). Therefore, always include the command to load the Mamba environment in your SLURM submission scripts.",
    "crumbs": [
      "HPC pipes",
      "Computational environments",
      "FAIR environments"
    ]
  },
  {
    "objectID": "develop/fair_envs.html#containers",
    "href": "develop/fair_envs.html#containers",
    "title": "FAIR environments",
    "section": "Containers",
    "text": "Containers\nEssentially, a container is a self-contained, lightweight package that includes everything needed to run a specific application—such as the operating system, libraries, and the application code itself. Containers operate independently of the host system, which allows them to run the same software across various environments without any conflicts or interference. This isolation ensures that researchers can consistently execute their code on different systems and platforms, without worrying about dependency issues or conflicts with other software on the host machine.\n\n\n\n\n\n\nDocker vs. Singularity\n\n\n\nThe most significant difference is at the permission level required to run them. Docker containers operate as root by default, giving them full access to the host system. While this can be useful in certain situations, it also poses security risks, especially in multi-user environments. In contrast, Singularity containers run as non-root users by default, enhancing security and preventing unauthorized access to the host system.\n\nDocker is ideal for building and distributing software across different operating systems\nSingularity is designed for HPC environments and offers high performance without needing root access\n\n\n\nIn the following sections, we’ll cover how to retrieve environment information, utilize containers, and automate environment setup to improve reproducibility.\n\nSingularity on a remote server\nWhile you can build your own Singularity images, many popular software packages already have pre-built images available from public repositories. The two repositories you’ll most likely use or hear about are:\n\ndepot.galaxyproject.org\nContainer Library (Sylabs)\nDocker Hub\n\n\n\n\n\n\n\nInstallation\n\n\n\n\nSingularity installation guides\n\n# You will only need to vagrant init once \nexport VM=sylabs/singularity-3.0-ubuntu-bionic64 && \\\n    vagrant init $VM && \\\n    vagrant up && \\\n    vagrant ssh\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nWe recommend using the pre-installed version provided by your system administrators if you’re working on a shared system. If you’re working on your own computer, you can install the necessary software using Mamba.\nThey might host different versions of the same software, so it’s worth checking both to find the version you need.\nTo download a software container from public repositories, use the singularity pull command.\nTo execute a command within the software container, use the singularity run command.\nGood practice: create a directory to save all singularity images together. .sif is the standard extension for the images.\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nDownload a singularity image from one of the two repositories listed above (choose a software like bcftools, bedtools, bowtie2, seqkit…) and run the --help command. This command displays the help documentation of the program, verifying that our image is functioning correctly and includes the intended software.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n# create a directory for our singularity images\nmkdir images\n\n# download the image\nsingularity pull images/bowtie2-2.5.4.sif https://depot.galaxyproject.org/singularity/bowtie2%3A2.5.4--he20e202_2\n\n# run the image: singularity run &lt;PATH-TO-IMAGE&gt; &lt;YOUR COMMANDS&gt;\nsingularity run images/bowtie2-2.5.4.sif bowtie2 --help",
    "crumbs": [
      "HPC pipes",
      "Computational environments",
      "FAIR environments"
    ]
  },
  {
    "objectID": "develop/fair_envs.html#sources",
    "href": "develop/fair_envs.html#sources",
    "title": "FAIR environments",
    "section": "Sources",
    "text": "Sources\n\nAnaconda for searching Mamba/conda packages\nBioconda for installing software package related to biomedical research\nConda cheat sheet\nfaircookbook worflows\nDocker\nDocker get-started\nThe turing way - reproducible research\n\nFind pre-built singularity images:\n\ndepot.galaxyproject.org\nSylabs\n\nOther training resources: - The turing way - reproducible research - HPC intro by Cambridge - Highly recommend Reproducible Research II: Practices and Tools for Managing Computations and Data by members of France Université Numérique.",
    "crumbs": [
      "HPC pipes",
      "Computational environments",
      "FAIR environments"
    ]
  },
  {
    "objectID": "develop/cheat_sheet.html",
    "href": "develop/cheat_sheet.html",
    "title": "Cheat sheet",
    "section": "",
    "text": "Collection of useful commands for package and environment management.",
    "crumbs": [
      "HPC pipes",
      "Computational environments",
      "Cheat sheet"
    ]
  },
  {
    "objectID": "develop/cheat_sheet.html#environment-descriptors",
    "href": "develop/cheat_sheet.html#environment-descriptors",
    "title": "Cheat sheet",
    "section": "Environment descriptors",
    "text": "Environment descriptors\n\nGit: git log -1 and git status -u. In python, use the following command for a specific module version: .version.git_revision) or .__git_version__.\nR: sessionInfo() or devtools::session_info(): In R, these functions provide detailed information about the current session and list all dependencies.\nPython: the best way seems to import sys, and then, __version__, __file__ which will display package versions and their location without having to list all packages using pip. It’s essential to load the package first and then use the following code to print all its dependencies:",
    "crumbs": [
      "HPC pipes",
      "Computational environments",
      "Cheat sheet"
    ]
  },
  {
    "objectID": "develop/cheat_sheet.html#shell",
    "href": "develop/cheat_sheet.html#shell",
    "title": "Cheat sheet",
    "section": "SHELL",
    "text": "SHELL\n# finds the location of a command by searching through the PATH environment variable\nwhich &lt;program&gt;\n\n# lists all occurrences of a command found in the PATH\nwhich -a &lt;program&gt; \n\n# shows the shared libraries required by a specific program\nldd &lt;program&gt; \n\n# provides detailed system information, including machine name and kernel version\nuname -a \n\n# displays operating system identification data (such as Debian, Ubuntu, etc.)\ncat /etc/os-release \n\n# provides operating system identification data and is recommended if available. This command might not be installed by default but is part of the lsb-release package on Debian-based systems\nlsb_release -a \n\n# Environmental variables for locations \n$HOME # home directory\n$PYTHONPATH # empty by default\n$PYTHONHOME # python libraries\n$RHOME # R libraries\n$LD_LIBRARY_PATH # dynamic loader for shared libraries when running a program\nUnderstanding PYTHONPATH\n\nPYTHONPATH: you can set the $PYTHONPATH environment variable to include additional directories where Python will look for modules and packages. This allows you to extend the search path beyond the default locations and load packages that has been installed in a different directory. It is highly discouraged to mix different versions of libraries and interpreters! as some libraries are complex packages with dependencies.\n\nexport PYTHONPATH=/path/to/packages/\n# unset the variable \nunset PYTHONPATH",
    "crumbs": [
      "HPC pipes",
      "Computational environments",
      "Cheat sheet"
    ]
  },
  {
    "objectID": "develop/cheat_sheet.html#pip",
    "href": "develop/cheat_sheet.html#pip",
    "title": "Cheat sheet",
    "section": "PIP",
    "text": "PIP\npip3 show &lt;module&gt;\npip3 install &lt;module&gt;      # install the latest version (and dependencies)\npip3 uninstall &lt;module&gt;    # remove\npip3 freeze                # output installed packages in requirements.txt format (similar to pip3 list) which can conveniently be used with: pip3 install -r requirements.txt\nWe highly recommend to avoid using pip and start using Python virtualenv management tools, pipenv.\n\nAdvantages: it will generates and checks file hashes for locked dependencies when installing from Pipfile.lock and it creates a virtualenv in a standard customizable location.",
    "crumbs": [
      "HPC pipes",
      "Computational environments",
      "Cheat sheet"
    ]
  },
  {
    "objectID": "develop/cheat_sheet.html#python",
    "href": "develop/cheat_sheet.html#python",
    "title": "Cheat sheet",
    "section": "PYTHON",
    "text": "PYTHON\n# version info on installed packages\n\ndef print_imported_modules():\n  import sys\n  for name, val in sorted(sys.modules.items()):\n      if(hasattr(val, '__version__')): \n          print(val.__name__, val.__version__)\n      else:\n          print(val.__name__, \"(unknown version)\")\n\nprint(\"==== Package list after loading pandas ====\");\nimport &lt;module&gt;\nprint_imported_modules()",
    "crumbs": [
      "HPC pipes",
      "Computational environments",
      "Cheat sheet"
    ]
  },
  {
    "objectID": "develop/cheat_sheet.html#r",
    "href": "develop/cheat_sheet.html#r",
    "title": "Cheat sheet",
    "section": "R",
    "text": "R\ninstall.packages()     # install a given package\nlibrary()              # loads a given package\nremove.packages()      # install a given package\nWe highly recommend using renv.",
    "crumbs": [
      "HPC pipes",
      "Computational environments",
      "Cheat sheet"
    ]
  },
  {
    "objectID": "develop/cheat_sheet.html#conda",
    "href": "develop/cheat_sheet.html#conda",
    "title": "Cheat sheet",
    "section": "CONDA",
    "text": "CONDA\nconda -V\nconda list \nconda env list",
    "crumbs": [
      "HPC pipes",
      "Computational environments",
      "Cheat sheet"
    ]
  },
  {
    "objectID": "develop/cheat_sheet.html#dpkg",
    "href": "develop/cheat_sheet.html#dpkg",
    "title": "Cheat sheet",
    "section": "DPKG",
    "text": "DPKG\nLow-level tool for package management on Debian-based systems\ndpkg -S package_name # Seach   \ndpkg -I package.deb  # --info\ndpkg -L package_name # --list files installed by a package\ndpkg -i package.deb  # --install   [requires sudo]\ndpkg -r   # --remove:  Remove debian_package        [requires sudo]\ndpkg --get-selections    # List all the packages known by dpkg and whether they are installed or not\ndpkg --set-selections    # Set which package should be installed     [requires sudo]\ndpkg-query -W -f='${Package} == ${Version}\\n' # package version",
    "crumbs": [
      "HPC pipes",
      "Computational environments",
      "Cheat sheet"
    ]
  },
  {
    "objectID": "develop/cheat_sheet.html#apt",
    "href": "develop/cheat_sheet.html#apt",
    "title": "Cheat sheet",
    "section": "APT",
    "text": "APT\nHigh-level tool for package management on Debian-based systems. It also handles package dependencies and repositories.\napt\napt update                      # Update the package list                  [sudo]\napt search &lt;package_name&gt;\napt show &lt;package_name&gt;\napt install &lt;package_name&gt;      # Install a debian package                 [sudo]\napt upgrade                     # Upgrade all your packages                [sudo]\napt clean                       # Delete all the .deb you've downloaded (save some space)\napt remove --purge &lt;package_name&gt; # Remove a debian package                  [sudo]\napt-cache search keyword # Search for packages containing a keyword.",
    "crumbs": [
      "HPC pipes",
      "Computational environments",
      "Cheat sheet"
    ]
  },
  {
    "objectID": "develop/cheat_sheet.html#docker-commandline",
    "href": "develop/cheat_sheet.html#docker-commandline",
    "title": "Cheat sheet",
    "section": "Docker",
    "text": "Docker\nUseful commands to build and deploy a Docker image.\ndocker pull         # download image from a registry e.g. Docker Hub \ndocker images       # list all Docker images on your local machine\ndocker run -it &lt;image_name&gt;  # creates and starts a new container from the specified Docker image, -it flag means interactive virtual machine which is very useful during the development-phase for testing the container \ndocker build -t &lt;my-app&gt;  # build a Docker image form a Dockerfile and a context \ndocker tag &lt;my-app&gt; &lt;myrepo/my-app:v1.0&gt; # creates an new alias for an existing Docker image. Useful for versioning\ndocker push         # upload image from local machine to Docker registry \ndocker login        # logs you into a Docker register (after pull and push), username and pw needed\nOther common commands use in Dockerfiles to clean up the image and reduce its size:\n\n\nDockerfile\n\nRUN apt-get -y autoclean && \\\n    apt-get -y autoremove && \\\n    rm -rf /var/lib/apt/lists/* && \\\n    rm -rf /var/cache/apt/archives/*deb",
    "crumbs": [
      "HPC pipes",
      "Computational environments",
      "Cheat sheet"
    ]
  },
  {
    "objectID": "develop/ssh_keys.html",
    "href": "develop/ssh_keys.html",
    "title": "HPC Lab",
    "section": "",
    "text": "Using SSH keys for authentication is recommended for accessing servers and version control platforms such as GitLab, GitHub, or BitBucket. SSH keys provide a more convenient and secure method than passwords, allowing you to avoid typing a password every time you log in.\nThis tutorial guides you through configuring SSH keys, using GitHub as an example.\n\n\nAn SSH (Secure Shell) is a network protocol that allows you to execute commands on a remote server securely. SSH keys are cryptographic keys used for authenticating a user or device within the SSH protocol. They come in pairs: the “private” key and the “public” key.\n\nPublic Key: This key can be freely shared with anyone and is added to the ~/.ssh/authorized_keys file on the remote server you wish to access.\nPrivate Key: This key should remain on your local machine and be carefully protested, similar to a password. It should never be shared.\n\nThe holder of the private key can authenticate with any remote system that has the corresponding public key. For example, you can use a single public/private key pair to authenticate with the cluster at your research center and your GitHub account.\n\n\n\n\n\n\nHow SSH keys work\n\n\n\nThink of the public key as a complex number x, and the private key as its inverse 1/x. Encrypting a message m with your public key is like multiplying your message by x, making mx completely unreadable. Using the private key, you can decrypt m by multiplying mx by 1/x. This process is secure because you cannot derive 1/x from x. Therefore, you can safely distribute the public key to the machines you want to access while keeping the private key on your machine.\n\n\n\n\n\n\n\nOn your (Mac or Linux) workstation, create the public/private key pair. There are several types of keys and by default you will get an RSA pair. When prompted for a passphrase, just hit enter so that you won’t need a password when logging in.\nWhen prompted, press Enter to store the keys in .ssh/id_rsa (we recommend not to modify this). Press Enter twice again to skip setting a passphrase, which otherwise would require you to enter it each time you use git push or git pull. This process generates two files: the public key (~/.ssh/id_rsa.pub) and the private key (~/.ssh/id_rsa).\nssh-keygen\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/Users/abc123/.ssh/id_rsa): \nCreated directory '/Users/abc123/.ssh'.\nEnter passphrase (empty for no passphrase): \nEnter same passphrase again: \nYour identification has been saved in /Users/abc123/.ssh/id_rsa\nYour public key has been saved in /Users/abc123/.ssh/id_rsa.pub\nThe key fingerprint is:\nSHA256:w8JIQXafdq6scMiNZefNd8lVz9UmIzm0eiO+pvGOM6A abc123@c9b1d769db6e\nThe key's randomart image is:\n+---[RSA 3072]----+\n|   .+ .     .    |\n|   . o . . . o  .|\n|    .   + . = o =|\n|   . o o o . o =+|\n|    . = S + o   +|\n|   . *.= * o o o |\n|    =.o.= + . +  |\n|    Eo .o+.o .   |\n|      . o*+      |\n+----[SHA256]-----+\ncat `~/.ssh/id_rsa.pub\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCs3nqhRr7FyB8UKxqQ5mA3Gk60oty5lGdzcpVxjL08QmjLz15X6Odo2EotmJQuzCyYbzPw4Tq404N3NOmVqYywrn03i3kmCnZd5MYq8yi72Zsk9qFfgEt7pxpf5WJ+AWSDXwUkUQK2FFby2RWbi4gwTmRHfkB999j8F3F/d5EpB0llQfxmZWIIrX9ampgxfI8+yTIyeG8gpx5028h1oR5qukEFJk6nAoj5on+/vBqHdc0AVPd6jNoNCDZ6ur4xwUPam8iQ6A4NfbnCVUPa95Wqt+QWEF6Mn4UB1WIvly+blDzg5rVXvd9B685bZjQWfUsxSiRPkhtV6X2yImklt+KjV0ufB/Yl1O+x6S8r2+I41WSxwJNr5h5L588l3GH4ehT79uCtrxiu4zas+4s9OW02Ox3auCfLi1/jp/CJ5IsuLwM/jPBXrgzw+HREKM2gtm9d12gU2b9o9bmASZVhEKqeTb0aRPjcTYYdnjOWUl+pqkdVPyB7mJs8NAQemN/shvM= abc123@c9b1d769db6e\nThe public key can be safely displayed cat and shared with any achine where it will function as a personal lock that can be only opened with your private key.\n\n\n\n\n\n\nTip\n\n\n\nTip 1: If you provided a passphrase when generating your keys, you would need to enter this passphrase each time you use your private key. The passphrase adds an extra layer of security in case your private key is compromised. For simplicity, we not always recommend using a passphrase unless you are working in environments where people not part of the project might have access.\nTip 2: DSA keys should be avoided. An RSA key with at least 2048 bits should provide sufficient security. Finally, ecdsa and ed25519 keys are also fine (don’t worry about the number of bits for these keys).\n# Recommended\nssh-keygen -t rsa -b 4096\n# Alternative\nssh-keygen -t ed25519\n\n\nUsing an existing public/private key pair\nIf you already have an SSH key pair, you can use it without generating a new one. To check the details of your existing key, use the ssh-keygen command to display the key’s fingerprint.\nssh-keygen -l -f ~/.ssh/id_rsa\n2048 SHA256:7l0HauYJVRaQhuzmti8XEZImnRbzipu3NKGnE6tDFRk grg@t430s (RSA)\nAlternatively, check the content of the ssh directory: - Mac/Unix: ls -la ~/.ssh/ - Windows: ls ~/.ssh/\n\n\n\nFollow the steps below if you are using Mac or Linux.\n\n\n\n\n\n\nWarning\n\n\n\n\nWindows users: follow these instructions instead, the process is very similar.\nIf you are getting errors when running the commands below, please read carefully these guidelines as GitHUb provides instructions on what to do with common issues users run into for this specific step 2.\nGeneral GitHUbtroubleshooting\n\n\n\n# ONLY if you need to start the ssh agent. First, verify it is not already running: \n# pgrep -u $USER ssh-agent\n# If that command returns a process ID, your SSH agent is running! If not, run the command below: \n# eval \"$(ssh-agent -s)\"\n\n# Manually modify the `~/.ssh/config` file to automatically load keys into ssh-agent, like in the example below. For GitHub, the User is **git**, do not write your own. \n\nHost github.com\n  AddKeysToAgent yes\n  IdentityFile ~/.ssh/id_rsa\n\n# Note: if the file does not `touch ~/.ssh/config`to create the file. \n\n# Add the SSH private to the ssh-agent\nssh-add ~/.ssh/id_rsa\n\n# Optional, verify the keys are loaded\nssh-add -l\n\n\n\n\n\n\nOther tips\n\n\n\n\n~/.ssh/config: add several hosts (servers, GitLab, GitHUb, etc.).\n\nEdit the SSH config file Create and use a ~/.ssh/config file on your workstation. E.g., using a password-less ssh key for authentication and the following ~/.ssh/config, user abc123 can login with the ssh xyz command.\nHost xyz\n    Hostname ssh-myserver.dk\n    User abc123\n    ServerAliveInterval 60\n    ForwardX11 yes\n    ForwardX11Trusted yes\n\nUsing paraphrases: store paraphrases in your keychain if you are using one. Please, follow the instructions here.\n\n\n\nStep 3: Transferring the key\nA. Transfer the key to GitHub/GitLab\nCopy your SSH public key ~/.ssh/id_rsa.pub with your favorite editor, go to your GitHub profile (top left side) &gt; Settings &gt; Access &gt; SSH and GPG keys &gt; New/Add SSH key. You just need to fill up the title (e.g.: Personal laptop) and paste the copied key (e.g. ssh-rsa or ecdsa-xxx).\n# Mac/Linux: copy key \ncat ~/.ssh/id_rsa.pub | pbcopy\n# Windows\ncat ~/.ssh/id_rsa.pub | clip\n\n\n\n\n\n\nGitHub Guidelines Links\n\n\n\n\n\n\nGeneral\nLast step, key transfer\n\n\n\n\nB: Transfer the key to a remote server\nTransfer the public part of your key pair to the remote server. This can be done manually, but the easiest way is using ssh-copy-id:\nssh-copy-id -i ~/.ssh/id_rsa.pub &lt;userid&gt;@ssh-myserver.dk\nThis will prompt for a password to authenticate with the remote server in order to copy the file. Once successful though, future logins will use the key pair for authentication, and will not prompt for a password (assuming you login from your workstation, which has the private part of your key pair).\n\n\n\n\n\n\nNote\n\n\n\nssh-copy-id will blindly append the given key file to the remote ~/.ssh/authorized_keys file. If you made a mistake or copied the same key multiple times, you may want to edit this file! Each line in the file corresponds to one key.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nYou are set to start using your SSH key. If you are using UCloud, follow the next section; otherwise, you are done with this tutorial.\n\n\n\n\n\n\nSimilarly to what is explained above, copy your key (manually or using e.g. pbcopy ~/.ssh/id_rsa.pub) and go to Resources (navigation panel on the left) &gt; SSH keys.\n\nPaste the public key here and give a title to your key as in the image below:\n\nWhen you have added the public part of the SSH key pair to your UCloud account, you can access jobs from your local computer if SSH access is possible for the given app and enabled upon job submission. Open Visual Studio Code to test this yourself! You will need to click on “Enable SSH Server” when submitting the job as follows:\n\n\n\nSelect “Enable SSH server”\n\n\nWhen a job with SSH access enabled starts, the command for accessing the job from your local computer via SSH is displayed from the job’s progress view.\n\n\n\nCopy ssh command\n\n\nNow, open a terminal and run the command:\n# the 4 digits will change everytime you start a new job\nssh ucloud@ssh.cloud.sdu.dk -p 2465\nIf it is the first time you do this, this message will prompt, write yes\nThe authenticity of host '[ssh.cloud.sdu.dk]:2465 ([130.225.164.106]:2465)' can't be established.\nED25519 key fingerprint is SHA256:0Q5WMne+hzOwj5bEfssH/gQrxFDz2fvclCGsQbfLLb8.\nThis key is not known by any other names.\nAre you sure you want to continue connecting (yes/no/[fingerprint])?\nYour terminal will look like the image below which means you can starting interacting with the job from the console on your computer. \nThe connection can be closed using the command:\nexit",
    "crumbs": [
      "HPC launch",
      "Data management",
      "SSH keys"
    ]
  },
  {
    "objectID": "develop/ssh_keys.html#ssh-keys",
    "href": "develop/ssh_keys.html#ssh-keys",
    "title": "HPC Lab",
    "section": "",
    "text": "Using SSH keys for authentication is recommended for accessing servers and version control platforms such as GitLab, GitHub, or BitBucket. SSH keys provide a more convenient and secure method than passwords, allowing you to avoid typing a password every time you log in.\nThis tutorial guides you through configuring SSH keys, using GitHub as an example.\n\n\nAn SSH (Secure Shell) is a network protocol that allows you to execute commands on a remote server securely. SSH keys are cryptographic keys used for authenticating a user or device within the SSH protocol. They come in pairs: the “private” key and the “public” key.\n\nPublic Key: This key can be freely shared with anyone and is added to the ~/.ssh/authorized_keys file on the remote server you wish to access.\nPrivate Key: This key should remain on your local machine and be carefully protested, similar to a password. It should never be shared.\n\nThe holder of the private key can authenticate with any remote system that has the corresponding public key. For example, you can use a single public/private key pair to authenticate with the cluster at your research center and your GitHub account.\n\n\n\n\n\n\nHow SSH keys work\n\n\n\nThink of the public key as a complex number x, and the private key as its inverse 1/x. Encrypting a message m with your public key is like multiplying your message by x, making mx completely unreadable. Using the private key, you can decrypt m by multiplying mx by 1/x. This process is secure because you cannot derive 1/x from x. Therefore, you can safely distribute the public key to the machines you want to access while keeping the private key on your machine.\n\n\n\n\n\n\n\nOn your (Mac or Linux) workstation, create the public/private key pair. There are several types of keys and by default you will get an RSA pair. When prompted for a passphrase, just hit enter so that you won’t need a password when logging in.\nWhen prompted, press Enter to store the keys in .ssh/id_rsa (we recommend not to modify this). Press Enter twice again to skip setting a passphrase, which otherwise would require you to enter it each time you use git push or git pull. This process generates two files: the public key (~/.ssh/id_rsa.pub) and the private key (~/.ssh/id_rsa).\nssh-keygen\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/Users/abc123/.ssh/id_rsa): \nCreated directory '/Users/abc123/.ssh'.\nEnter passphrase (empty for no passphrase): \nEnter same passphrase again: \nYour identification has been saved in /Users/abc123/.ssh/id_rsa\nYour public key has been saved in /Users/abc123/.ssh/id_rsa.pub\nThe key fingerprint is:\nSHA256:w8JIQXafdq6scMiNZefNd8lVz9UmIzm0eiO+pvGOM6A abc123@c9b1d769db6e\nThe key's randomart image is:\n+---[RSA 3072]----+\n|   .+ .     .    |\n|   . o . . . o  .|\n|    .   + . = o =|\n|   . o o o . o =+|\n|    . = S + o   +|\n|   . *.= * o o o |\n|    =.o.= + . +  |\n|    Eo .o+.o .   |\n|      . o*+      |\n+----[SHA256]-----+\ncat `~/.ssh/id_rsa.pub\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCs3nqhRr7FyB8UKxqQ5mA3Gk60oty5lGdzcpVxjL08QmjLz15X6Odo2EotmJQuzCyYbzPw4Tq404N3NOmVqYywrn03i3kmCnZd5MYq8yi72Zsk9qFfgEt7pxpf5WJ+AWSDXwUkUQK2FFby2RWbi4gwTmRHfkB999j8F3F/d5EpB0llQfxmZWIIrX9ampgxfI8+yTIyeG8gpx5028h1oR5qukEFJk6nAoj5on+/vBqHdc0AVPd6jNoNCDZ6ur4xwUPam8iQ6A4NfbnCVUPa95Wqt+QWEF6Mn4UB1WIvly+blDzg5rVXvd9B685bZjQWfUsxSiRPkhtV6X2yImklt+KjV0ufB/Yl1O+x6S8r2+I41WSxwJNr5h5L588l3GH4ehT79uCtrxiu4zas+4s9OW02Ox3auCfLi1/jp/CJ5IsuLwM/jPBXrgzw+HREKM2gtm9d12gU2b9o9bmASZVhEKqeTb0aRPjcTYYdnjOWUl+pqkdVPyB7mJs8NAQemN/shvM= abc123@c9b1d769db6e\nThe public key can be safely displayed cat and shared with any achine where it will function as a personal lock that can be only opened with your private key.\n\n\n\n\n\n\nTip\n\n\n\nTip 1: If you provided a passphrase when generating your keys, you would need to enter this passphrase each time you use your private key. The passphrase adds an extra layer of security in case your private key is compromised. For simplicity, we not always recommend using a passphrase unless you are working in environments where people not part of the project might have access.\nTip 2: DSA keys should be avoided. An RSA key with at least 2048 bits should provide sufficient security. Finally, ecdsa and ed25519 keys are also fine (don’t worry about the number of bits for these keys).\n# Recommended\nssh-keygen -t rsa -b 4096\n# Alternative\nssh-keygen -t ed25519\n\n\nUsing an existing public/private key pair\nIf you already have an SSH key pair, you can use it without generating a new one. To check the details of your existing key, use the ssh-keygen command to display the key’s fingerprint.\nssh-keygen -l -f ~/.ssh/id_rsa\n2048 SHA256:7l0HauYJVRaQhuzmti8XEZImnRbzipu3NKGnE6tDFRk grg@t430s (RSA)\nAlternatively, check the content of the ssh directory: - Mac/Unix: ls -la ~/.ssh/ - Windows: ls ~/.ssh/\n\n\n\nFollow the steps below if you are using Mac or Linux.\n\n\n\n\n\n\nWarning\n\n\n\n\nWindows users: follow these instructions instead, the process is very similar.\nIf you are getting errors when running the commands below, please read carefully these guidelines as GitHUb provides instructions on what to do with common issues users run into for this specific step 2.\nGeneral GitHUbtroubleshooting\n\n\n\n# ONLY if you need to start the ssh agent. First, verify it is not already running: \n# pgrep -u $USER ssh-agent\n# If that command returns a process ID, your SSH agent is running! If not, run the command below: \n# eval \"$(ssh-agent -s)\"\n\n# Manually modify the `~/.ssh/config` file to automatically load keys into ssh-agent, like in the example below. For GitHub, the User is **git**, do not write your own. \n\nHost github.com\n  AddKeysToAgent yes\n  IdentityFile ~/.ssh/id_rsa\n\n# Note: if the file does not `touch ~/.ssh/config`to create the file. \n\n# Add the SSH private to the ssh-agent\nssh-add ~/.ssh/id_rsa\n\n# Optional, verify the keys are loaded\nssh-add -l\n\n\n\n\n\n\nOther tips\n\n\n\n\n~/.ssh/config: add several hosts (servers, GitLab, GitHUb, etc.).\n\nEdit the SSH config file Create and use a ~/.ssh/config file on your workstation. E.g., using a password-less ssh key for authentication and the following ~/.ssh/config, user abc123 can login with the ssh xyz command.\nHost xyz\n    Hostname ssh-myserver.dk\n    User abc123\n    ServerAliveInterval 60\n    ForwardX11 yes\n    ForwardX11Trusted yes\n\nUsing paraphrases: store paraphrases in your keychain if you are using one. Please, follow the instructions here.\n\n\n\nStep 3: Transferring the key\nA. Transfer the key to GitHub/GitLab\nCopy your SSH public key ~/.ssh/id_rsa.pub with your favorite editor, go to your GitHub profile (top left side) &gt; Settings &gt; Access &gt; SSH and GPG keys &gt; New/Add SSH key. You just need to fill up the title (e.g.: Personal laptop) and paste the copied key (e.g. ssh-rsa or ecdsa-xxx).\n# Mac/Linux: copy key \ncat ~/.ssh/id_rsa.pub | pbcopy\n# Windows\ncat ~/.ssh/id_rsa.pub | clip\n\n\n\n\n\n\nGitHub Guidelines Links\n\n\n\n\n\n\nGeneral\nLast step, key transfer\n\n\n\n\nB: Transfer the key to a remote server\nTransfer the public part of your key pair to the remote server. This can be done manually, but the easiest way is using ssh-copy-id:\nssh-copy-id -i ~/.ssh/id_rsa.pub &lt;userid&gt;@ssh-myserver.dk\nThis will prompt for a password to authenticate with the remote server in order to copy the file. Once successful though, future logins will use the key pair for authentication, and will not prompt for a password (assuming you login from your workstation, which has the private part of your key pair).\n\n\n\n\n\n\nNote\n\n\n\nssh-copy-id will blindly append the given key file to the remote ~/.ssh/authorized_keys file. If you made a mistake or copied the same key multiple times, you may want to edit this file! Each line in the file corresponds to one key.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nYou are set to start using your SSH key. If you are using UCloud, follow the next section; otherwise, you are done with this tutorial.\n\n\n\n\n\n\nSimilarly to what is explained above, copy your key (manually or using e.g. pbcopy ~/.ssh/id_rsa.pub) and go to Resources (navigation panel on the left) &gt; SSH keys.\n\nPaste the public key here and give a title to your key as in the image below:\n\nWhen you have added the public part of the SSH key pair to your UCloud account, you can access jobs from your local computer if SSH access is possible for the given app and enabled upon job submission. Open Visual Studio Code to test this yourself! You will need to click on “Enable SSH Server” when submitting the job as follows:\n\n\n\nSelect “Enable SSH server”\n\n\nWhen a job with SSH access enabled starts, the command for accessing the job from your local computer via SSH is displayed from the job’s progress view.\n\n\n\nCopy ssh command\n\n\nNow, open a terminal and run the command:\n# the 4 digits will change everytime you start a new job\nssh ucloud@ssh.cloud.sdu.dk -p 2465\nIf it is the first time you do this, this message will prompt, write yes\nThe authenticity of host '[ssh.cloud.sdu.dk]:2465 ([130.225.164.106]:2465)' can't be established.\nED25519 key fingerprint is SHA256:0Q5WMne+hzOwj5bEfssH/gQrxFDz2fvclCGsQbfLLb8.\nThis key is not known by any other names.\nAre you sure you want to continue connecting (yes/no/[fingerprint])?\nYour terminal will look like the image below which means you can starting interacting with the job from the console on your computer. \nThe connection can be closed using the command:\nexit",
    "crumbs": [
      "HPC launch",
      "Data management",
      "SSH keys"
    ]
  },
  {
    "objectID": "develop/data_compression.html#data-organization",
    "href": "develop/data_compression.html#data-organization",
    "title": "HPC Lab",
    "section": "Data organization",
    "text": "Data organization\n\nregular expression\ncookiecutter"
  },
  {
    "objectID": "develop/smk.html",
    "href": "develop/smk.html",
    "title": "Snakemake",
    "section": "",
    "text": "In this section, we will guide you through transitioning from bash scripts or notebooks to workflows. This approach is particularly useful as computations become more complex.",
    "crumbs": [
      "HPC pipes",
      "Pipeline languages",
      "Snakemake"
    ]
  },
  {
    "objectID": "develop/smk.html#sources",
    "href": "develop/smk.html#sources",
    "title": "Snakemake",
    "section": "Sources",
    "text": "Sources\n\nSnakemake tutorial\nSnakemake turorial slides by Johannes Koster\nKöster, Johannes and Rahmann, Sven. “Snakemake - A scalable bioinformatics workflow engine”. Bioinformatics 2012.\nKöster, Johannes. “Parallelization, Scalability, and Reproducibility in Next-Generation Sequencing Analysis”, PhD thesis, TU Dortmund 2014.\nfaircookbook worflows",
    "crumbs": [
      "HPC pipes",
      "Pipeline languages",
      "Snakemake"
    ]
  },
  {
    "objectID": "develop/hpc_intro.html",
    "href": "develop/hpc_intro.html",
    "title": "HPC launch",
    "section": "",
    "text": "Course Overview\n\n\n\n\n⏰ Total Time Estimation: X hours\n\n📁 Supporting Materials:\n\n👨‍💻 Target Audience: Ph.D., MSc, anyone interested in HPC systems.\n👩‍🎓 Level: Advanced.\n🔒 License: Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license.\n\n💰 Funding: This project was funded by the Novo Nordisk Fonden (NNF20OC0063268).",
    "crumbs": [
      "HPC launch",
      "HPC systems",
      "HPC launch"
    ]
  },
  {
    "objectID": "develop/hpc_intro.html#title",
    "href": "develop/hpc_intro.html#title",
    "title": "HPC launch",
    "section": "Title",
    "text": "Title\nIntroduction High-Performance Computing (HPCs) and HPC cluster.\nHPC main resources:\n\nCPU\nRAM\nGPU\n\nSchematic of components of an HPC\n\nNodes\nThere are two typoes of nodes on a cluster: - login nodes (also known as head or submit nodes). - compute nodes (also known as worker nodes).\n\n\n\n\n\n\nWhat can I run from a login node\n\n\n\nA straightforward rule: do not run anything on the login node to prevent potential problems. If the login node crashes, the entire system may need to be rebooted, affecting everyone. Remember, you’re not the only one using the HPC—so be considerate of others. For easy, quick tasks, request an interactive access to one of the compute nodes.\n\n\n\n\nJob scheduler\n\n\n\n\n\n\nNote\n\n\n\nSeveral job scheduler programs are available, and SLURM is among the most widely used. In the next section, we’ll explore SLURM in greater detail, along with general best practices for running jobs.\n\n\n\n\nFilesystem\nThe filesystem is the content all the directories and files available to a given process.\n\nScratch\nUsers working space\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nI have an omics pipeline that produces a large number of files, resulting in a couple of terabytes of data after processing and analysis. The project will continue for a few more years, and I’ve decided to store the data in the scratch folder. Do you agree with this decision, and why? What factors should be considered when deciding which data to retain and where to store it?\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nTypically, scratch storage is not backed up, so it’s not advisable to rely on it for important data. At a minimum, ensure you back up the raw data and the scripts used for processing. This way, if some processed files are lost, you can replicate the analyses.\nWhen deciding which data to keep on the HPC, back up, or delete, consider the following:\n\nProcessing Time: Evaluate how long each step of the analysis takes to run. There may be significant computational costs associated with re-running heavy data processing steps.\nStorage Management: Use tools like Snakemake to manage intermediate files. You can configure Snakemake to automatically delete intermediate files once the final results are produced, helping you manage storage more efficiently.\n\n\n\n\n\n\n\n\n\n\n\n\n\nKernel\nThe kernel is essential for managing multiple programs on your machine, each of which runs as a process. Even if you write code assuming full control over the CPU and memory, the kernel ensures that multiple processes can run simultaneously without interfering with each other. It does this by scheduling time for each process and translating virtual memory addresses into physical ones, ensuring security and preventing conflicts.\nThe kernel also ensures that processes can’t access each other’s memory or directly modify the hard drive, maintaining system security and stability. For example, when a process needs to write to a file, it asks the kernel to do so through a system call, rather than writing directly.\nIn conlcusion, it plays a crucial role in managing the CPU, memory, disk, and software environment. By mediating access to these resources, it maintains process isolation, security, and the smooth operation of your system.\n\n\n\n\n\n\nKernel primary roles:\n\n\n\n\nInterfaces with hardware to facilitate program operations\nManages and schedules the execution of processes\nRegulates and allocates system resources among processes\n\n\n\n\n\nBefore start using an HPC\nHigh-Performance Computing (HPC) systems might be organized differently, but there is typically an HPC administration team you can contact to understand how your specific HPC is structured. Key information you should seek from them includes:\n\nThe types of compute nodes available.\nThe storage options you can access and the amount allocated per user.\nWhether a job scheduler software is in use, and if so, which one. You can also request a sample submission script to help you get started.\nThe policy on who bears the cost of using the HPC resources.\nWhether you can install your own software and create custom environments.\n\n\n\n\n\n\n\nBe nice\n\n\n\nIf your HPC system doesn’t have a job scheduler in place, we recommend using the nice command. This command allows you to adjust and manage the scheduling priority of your processes, giving you the ability to run tasks with lower priority when sharing resources with others. By using nice, you can ensure that your processes do not dominate the CPU, allowing other users’ tasks to run smoothly. This is particularly useful in environments where multiple users are working on the same system without a job scheduler to automatically manage resource allocation.\n\n\n\n\n\n\n\n\nHPC\n\n\n\n\n\n\n\n\nDescribe how a typical HPC is organised: nodes, job scheduler and filesystem.\nWhat are the roles of a login node and a compute node? how do they differ?\nDescribe the role of a job scheduler\nWhat are the differences between scratch and home storage and when each should be used?\nWhat is a kernel?",
    "crumbs": [
      "HPC launch",
      "HPC systems",
      "HPC launch"
    ]
  },
  {
    "objectID": "develop/hpc_intro.html#sources",
    "href": "develop/hpc_intro.html#sources",
    "title": "HPC launch",
    "section": "Sources",
    "text": "Sources\nUseful links\n\nAcknowledgements",
    "crumbs": [
      "HPC launch",
      "HPC systems",
      "HPC launch"
    ]
  },
  {
    "objectID": "develop/fair_workflow.html",
    "href": "develop/fair_workflow.html",
    "title": "FAIR computational pipelines",
    "section": "",
    "text": "Course Overview\n\n\n\n\n👨‍💻 Target Audience: PAnyone interested in workflow management systems and software enviroments.\n👩‍🎓 Level: Advanced.\n🔒 License: Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license.\n\n💰 Funding: This project was funded by the Novo Nordisk Fonden (NNF20OC0063268).\nData analysis typically involves the use of different tools, algorithms, and scripts. It often requires multiple steps to transform, filter, aggregate, and visualize data. The process can be time-consuming because each tool may demand specific inputs and parameter settings. As analyses become more complex, the importance of reproducible and scalable automated workflow management increases since the key for reproducibility is automation.",
    "crumbs": [
      "HPC pipes",
      "Pipeline languages",
      "FAIR computational pipelines"
    ]
  },
  {
    "objectID": "develop/fair_workflow.html#good-practices",
    "href": "develop/fair_workflow.html#good-practices",
    "title": "FAIR computational pipelines",
    "section": "Good practices",
    "text": "Good practices\nIf you develop your own software make sure you follow FAIR principles. We highly endorse following these FAIR recommendations.\n\nRemember to create portable code and use relative paths to ensure transferability between users.\nUse git repositories to save your projects and pipelines.\nRegister and publish your scientific computational workflow on WorkflowHub.",
    "crumbs": [
      "HPC pipes",
      "Pipeline languages",
      "FAIR computational pipelines"
    ]
  },
  {
    "objectID": "develop/fair_workflow.html#sources",
    "href": "develop/fair_workflow.html#sources",
    "title": "FAIR computational pipelines",
    "section": "Sources",
    "text": "Sources\n\nSnakemake tutorial\nSnakemake tutorial slides by Johannes Koster\nhttps://bioconda.github.io\nKöster, Johannes and Rahmann, Sven. “Snakemake - A scalable bioinformatics workflow engine”. Bioinformatics 2012.\nKöster, Johannes. “Parallelization, Scalability, and Reproducibility in Next-Generation Sequencing Analysis”, PhD thesis, TU Dortmund 2014.\nfaircookbook worflows\nThe turing way - reproducible research\nParts of the content are inspired by Reproducible Research II: Practices and Tools for Managing Computations and Data by members of France Université Numérique. Enroll here.",
    "crumbs": [
      "HPC pipes",
      "Pipeline languages",
      "FAIR computational pipelines"
    ]
  },
  {
    "objectID": "develop/scripts/01.iris.html",
    "href": "develop/scripts/01.iris.html",
    "title": "Notebook Iris Dataset",
    "section": "",
    "text": "Before we dive in, here’s a quick summary: the dataset contains 150 samples of iris flowers, each characterized by four features: Sepal Length, Sepal Width, Petal Length, and Petal Width, all measured in centimeters. These samples are grouped into three species: Setosa, Versicolor, and Virginica. If you’re not familiar with the dataset, you can learn more about it here."
  },
  {
    "objectID": "develop/scripts/01.iris.html#loading-the-dataset",
    "href": "develop/scripts/01.iris.html#loading-the-dataset",
    "title": "Notebook Iris Dataset",
    "section": "1 Loading the dataset",
    "text": "1 Loading the dataset\nLet’s start by importing the iris dataset and manipulating the dataframe so that the column names match the feature names.\n\nfrom sklearn.datasets import load_iris\nimport matplotlib.pyplot as plt\nimport pandas as pd \n\n# Load toy dataset\niris = load_iris() \n # Create dataframe using feature names\ndf = pd.DataFrame(iris.data, columns=iris.feature_names)"
  },
  {
    "objectID": "develop/scripts/01.iris.html#exploring-the-dataset",
    "href": "develop/scripts/01.iris.html#exploring-the-dataset",
    "title": "Notebook Iris Dataset",
    "section": "2 Exploring the dataset",
    "text": "2 Exploring the dataset\nLet´s start by exploring the species by plotting the sepal length vs. width in a scatter plot\n\nimport matplotlib.pyplot as plt\n\n_, ax = plt.subplots()\nscatter = ax.scatter(iris.data[:, 0], iris.data[:, 1], c=iris.target)\nax.set(xlabel=iris.feature_names[0], ylabel=iris.feature_names[1])\n_ = ax.legend(\n    scatter.legend_elements()[0], iris.target_names, loc=\"lower right\", title=\"Classes\"\n)\n\n\n\n\n\n\n\n\nYou can already see a pattern regarding the Setosa type, which is easily identifiable based on its short and wide sepal. Only considering these 2 dimensions, sepal width and length, there’s still overlap between the Versicolor and Virginica types."
  },
  {
    "objectID": "develop/scripts/01.iris.html#transforming-the-dataset",
    "href": "develop/scripts/01.iris.html#transforming-the-dataset",
    "title": "Notebook Iris Dataset",
    "section": "3 Transforming the dataset",
    "text": "3 Transforming the dataset\nWe will now perform feature engineering and create a new feature called petal area (petal length * petal width), and will do the same for the sepal.\n\ndf['sepal_area'] = df['sepal length (cm)'] * df['sepal width (cm)']\ndf['petal_area'] = df['petal length (cm)'] * df['petal width (cm)']\n\nFinally, let’s new information by binning the sepal length into 3 categories (short, medium and long)\n\ndf['sepal_length_bin'] = pd.cut(df['sepal length (cm)'], bins=3, labels=[\"short\", \"medium\", \"long\"])\ndf\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\nsepal_area\npetal_area\nsepal_length_bin\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n17.85\n0.28\nshort\n\n\n1\n4.9\n3.0\n1.4\n0.2\n14.70\n0.28\nshort\n\n\n2\n4.7\n3.2\n1.3\n0.2\n15.04\n0.26\nshort\n\n\n3\n4.6\n3.1\n1.5\n0.2\n14.26\n0.30\nshort\n\n\n4\n5.0\n3.6\n1.4\n0.2\n18.00\n0.28\nshort\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\n20.10\n11.96\nmedium\n\n\n146\n6.3\n2.5\n5.0\n1.9\n15.75\n9.50\nmedium\n\n\n147\n6.5\n3.0\n5.2\n2.0\n19.50\n10.40\nmedium\n\n\n148\n6.2\n3.4\n5.4\n2.3\n21.08\n12.42\nmedium\n\n\n149\n5.9\n3.0\n5.1\n1.8\n17.70\n9.18\nmedium\n\n\n\n\n150 rows × 7 columns"
  },
  {
    "objectID": "develop/scripts/01.iris.html#computing-summary-statistics",
    "href": "develop/scripts/01.iris.html#computing-summary-statistics",
    "title": "Notebook Iris Dataset",
    "section": "4 Computing summary statistics",
    "text": "4 Computing summary statistics\nNow, we can extract summary statistics of the species “setosa” and compare it to another species\n\n# Map targets to species names and add them to a new column \ndf['species'] = iris.target_names[iris.target] \n# Display first few rows\ndf.head() \n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\nsepal_area\npetal_area\nsepal_length_bin\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n17.85\n0.28\nshort\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\n14.70\n0.28\nshort\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\n15.04\n0.26\nshort\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\n14.26\n0.30\nshort\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\n18.00\n0.28\nshort\nsetosa\n\n\n\n\n\n\n\n\n# Select setosa \ndf_setosa = df[df['species'] == \"setosa\"]\nsummary_stats = df_setosa.describe() \n\n# Display summary statistics\nprint(summary_stats)\n\n       sepal length (cm)  sepal width (cm)  petal length (cm)  \\\ncount           50.00000         50.000000          50.000000   \nmean             5.00600          3.428000           1.462000   \nstd              0.35249          0.379064           0.173664   \nmin              4.30000          2.300000           1.000000   \n25%              4.80000          3.200000           1.400000   \n50%              5.00000          3.400000           1.500000   \n75%              5.20000          3.675000           1.575000   \nmax              5.80000          4.400000           1.900000   \n\n       petal width (cm)  sepal_area  petal_area  \ncount         50.000000   50.000000   50.000000  \nmean           0.246000   17.257800    0.365600  \nstd            0.105386    2.933775    0.181155  \nmin            0.100000   10.350000    0.110000  \n25%            0.200000   15.040000    0.280000  \n50%            0.200000   17.170000    0.300000  \n75%            0.300000   19.155000    0.420000  \nmax            0.600000   25.080000    0.960000"
  },
  {
    "objectID": "develop/RDM_containers.html",
    "href": "develop/RDM_containers.html",
    "title": "RDM for containers",
    "section": "",
    "text": "Now that you’re familiar with containers, it’s time to focus on making them reproducible and ensuring good Research Data Management (RDM) practices.\nThe current approach that we introduce on the Docker lesson has a significant drawback: it doesn’t ensure a reproducible environment because it depends on external servers and services that frequently update. If you lose your Docker image, you might not be able to rebuild it or even know precisely what was in it. You could save the output of the commands below alongside your Dockerfile. This information will be crucial if you need to rebuild the image.\nHow do we improve reproducibility? - Specify the versioned base image: use a specific version of your base image in the Dockerfile (e.g., FROM debian:stable-20240812). Be aware that even with a versioned tag, maintainers or Docker Hub admins could introduce silent changes and update the image. - Use a SHA256 Digest: for the highest level of reproducibility, specify the base image using its SHA256 digest, which is a unique identifier. This ensures that you’re using the exact same image every time, with no risk of unexpected changes.",
    "crumbs": [
      "HPC pipes",
      "Computational environments",
      "RDM for containers"
    ]
  },
  {
    "objectID": "develop/RDM_containers.html#sources",
    "href": "develop/RDM_containers.html#sources",
    "title": "RDM for containers",
    "section": "Sources",
    "text": "Sources\n\nContent adapted from Reproducible Research II: Practices and tools for managing computations and data by members of France Universite Numerique.",
    "crumbs": [
      "HPC pipes",
      "Computational environments",
      "RDM for containers"
    ]
  },
  {
    "objectID": "develop/singularity.html",
    "href": "develop/singularity.html",
    "title": "Singularity",
    "section": "",
    "text": "Requirements\n\n\n\n\nInstall singularity\nRead the documentation",
    "crumbs": [
      "HPC pipes",
      "Computational environments",
      "Singularity"
    ]
  },
  {
    "objectID": "develop/singularity.html#building-sif-images",
    "href": "develop/singularity.html#building-sif-images",
    "title": "Singularity",
    "section": "Building SIF images",
    "text": "Building SIF images\nThe equivalent to a Dockerfile for singularity is Singularity definition file where the instructions for the image are specified.\nBootstrap: docker\nFrom: debian:stable\nStage: build\n\n%post\n    apt-get update && apt-get install -y jupyter-notebook python3-matplotlib python3-pandas python3-numpy\n%labels\n    Author Name Surname &lt;abc123@ku.dk&gt;\n    Version v1.0\n# Build the image\nsingularity build --fakeroot  &lt;my-app&gt;.sif &lt;my-app&gt;.def\n# Run the container \nsingularity run &lt;my-app&gt;.sif jupyter-notebook \n\n\n\n\n\n\nTips\n\n\n\nLimitations\n\nKeep in mind that Singularity always operates using your user ID, meaning you cannot switch to the root user inside a Singularity container. This will cause troubles when installing package managers like apt.\nSIF images are not writable by default.\n\nSolution\n\n--fakeroot option is used to create a container image with root-like permissions without requiring actual root access on the host system. This is particularly useful for users who need to build or modify containers (installing software) in environments where they do not have superuser privileges.",
    "crumbs": [
      "HPC pipes",
      "Computational environments",
      "Singularity"
    ]
  },
  {
    "objectID": "develop/singularity.html#using-docker-images",
    "href": "develop/singularity.html#using-docker-images",
    "title": "Singularity",
    "section": "Using Docker images",
    "text": "Using Docker images\nLet’s use the same docker image as in the Docker section.\nsingularity pull docker://debian@sha256:2171c87301e5ea528cee5c2308a4589f8f2ac819d4d434b679b7cb1dc8de6912\n\n# creates a sif \nsingularity shell debian@sha256_2171c87301e5ea528cee5c2308a4589f8f2ac819d4d434b679b7cb1dc8de6912.sif\n\n# Let's check a couple commands on the host machine  (as we did in the previous module)\n\ngzip --version # same as docker\nhostname # vagrant (macß) different\nwhoami # vagrant (docker is root)\n\n# deploy the container  \nsingularity shell debian@sha256_2171c87301e5ea528cee5c2308a4589f8f2ac819d4d434b679b7cb1dc8de6912.sif\n\n# Install filter in the container \napt-get install filter\n\n# Alternative command to install filter\nsingularity exec docker://debian@sha256:2171c87301e5ea528cee5c2308a4589f8f2ac819d4d434b679b7cb1dc8de6912 bash -c \"apt install filter 2&gt;&1 || true \"\n\n\n\n\n\n\nTip\n\n\n\nThe docker tag (docker://) step is required as singularity has sometimes trouble handling the sha256, image description.\nOther important information to consider:\n\nSingularity Hub is no longer maintained. Alternatives: git-annex for hosting images.\nSIF images are much smaller than Docker images.",
    "crumbs": [
      "HPC pipes",
      "Computational environments",
      "Singularity"
    ]
  },
  {
    "objectID": "develop/exercise_pipes.html",
    "href": "develop/exercise_pipes.html",
    "title": "Exercises",
    "section": "",
    "text": "Put your learning to the test with what you’ve covered so far.",
    "crumbs": [
      "HPC pipes",
      "Pipeline languages",
      "Exercises"
    ]
  },
  {
    "objectID": "develop/exercise_pipes.html#general-hpc-pipes",
    "href": "develop/exercise_pipes.html#general-hpc-pipes",
    "title": "Exercises",
    "section": "General HPC pipes",
    "text": "General HPC pipes\n1. What role does a workflow manager play in computational research??\n\n Automating the execution of complex computational processes To manually execute each step of a computation Offering real-time and intuitive data analysis To minimize the reliance on specific software environments\n\n2.What is the primary drawback of using shell scripts for automating computations?\n\n Limited error handling and debugging capabilities Difficulty in integrating with notebooks Not compatible with all operating systems They re-run all the steps every time Insufficient support for parallel processing Complex and extensive coding for simple tasks\n\n3. What are the key features of workflow manager in computational research? (Several possible solutions)\n\n Executing tasks only when required Managing task dependencies Overseeing storage and resource allocation Providing intuitive graphical interfaces\n\n4. Workflow managers can run tasks (different) concurrently if there are no dependencies (True or False) TRUEFALSE\n5. A workflow manager can execute a single parallelized task on multiple nodes in a computing cluster (True or False) TRUEFALSE",
    "crumbs": [
      "HPC pipes",
      "Pipeline languages",
      "Exercises"
    ]
  },
  {
    "objectID": "develop/exercise_pipes.html#snakemake",
    "href": "develop/exercise_pipes.html#snakemake",
    "title": "Exercises",
    "section": "Snakemake",
    "text": "Snakemake\n\n\n\n\n\n\nExercise 1S: Exploring Rule Invocation in Snakemake\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this exercise, we will explore how rules are invoked in a Snakemake workflow. Download the Snakefile and data required for this exercise using the links below.\n Download data input   Download Snakefile \nNow follow these steps and answer the questions:\n\nOpen the snakefile, named process_1kgp.smk and try to understand every single line. If you request Snakemake to generate the file results/all_female.txt, what commands will be executed and in what sequence?\nDry run the workflow: Check the number of jobs that will be executed.\n6. How many jobs will Snakemake run? \nRun the workflow: Use the name flag --snakefile | -s follow by the name of the file.\nVerify output: Ensure that the output files are in your working directory.\nClean Up: remove all files starting with EUR in your results folder.\nRerun the workflow: Execute the Snakefile again.\n7. How many jobs did Snakemake run in this last execution? \nRemove lines 4-6 in the process_1kgp.smk. How else can you run the workflow but to generate instead all_male.txt using only the command-line?\nrule all:\n   input:\n      expand(\"results/all_{gender}.txt\", gender=[\"female\"])\n8. Tip: what is missing at the end of the command ( e.g. what should be added to ensure all_male.txt is generated)? snakemake -s process_1kgp.smk -c1 \n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\n# dry run \nsnakemake -s process_1kgp.smk -n \n# run the workflow \nsnakemake -s process_1kgp.smk-c1 &lt;name_rule|name_output&gt;\n# verify output \nls &lt;name_output&gt;\n# remove file belonging to european individuals \nrm results/EUR.tsv results/all_female.txt\n# rerun again \nsnakemake -s process_1kgp.smk -c1 &lt;name_rule|name_output&gt;",
    "crumbs": [
      "HPC pipes",
      "Pipeline languages",
      "Exercises"
    ]
  },
  {
    "objectID": "develop/data_transfer.html",
    "href": "develop/data_transfer.html",
    "title": "HPC Lab",
    "section": "",
    "text": "To ensure efficient file transfers on HPC systems, we will go through:\n\nOverview rsync and scp\nBasic commands\n\nWhen transferring files between servers, it’s important to ensure that the files are consistent. Differences in file size can occur due to variations in filesystems, but large differences might indicate an issue. Manually checking file sizes in a terminal (ls -lh or du -sh) to determine if a transfer was successful might not be ideal, as it doesn’t guarantee file integrity. Using checksums provides a more reliable verification method.\nYou can use md5sum to verify that the file contents are identical on both servers. Run the following command on each server:\nmd5sum /path/to/file\nUsing rsync for Efficient File Transfers rsync is a powerful alternative to scp for transferring files. It only sends data if the file has changed, making it more efficient.\n# Transferring files between local machine-server\nrsync -avz local/path/to/file user@server:/remote/path/to/file\n# Transferring files between servers\nrsync -azv server1:/path/to/my_folder server2:/path/to/destination_folder\n\n\n\n\n\n\nNote\n\n\n\nTo transfer files directly between two servers from your local workstation, ensure your SSH setup (configuration, keys, etc.) allows access to both servers. Check this section if you need help setting up your keys (generating, configuring and managing).\n\n\n\n\nrsync is an efficient protocol to compare and copy files between directories or server. It can resume interrupted transfers and compress files on the fly.\n\nChecksum Verification: rsync checks the hashsums of files and only transfers data if the hashes differ. This ensures that only the changed parts of the files are sent (so you can rsync a whole folder, and only the changes files will be send).\nTimestamp Preservation: Using the -a flag with rsync preserves the modified timestamps of files, which is particularly useful for tools like Snakemake.\n\n\n\n\n\n-a: Archive mode, preserves file attributes like timestamps and permissions (important if you are using snakemake).\n-v: Verbose mode, provides detailed information during transfer.\n-z: Compresses data during transfer, reducing the amount of data sent over the network.\n-c: Enables checksum checking, ensuring that files are identical by comparing their contents rather than just their size and modification time.\n\n\n\n\nFor users who prefer a graphical interface, tools like Cyberduck and FileZilla can also be used for transferring files between servers.",
    "crumbs": [
      "HPC launch",
      "Data management",
      "Data transfer"
    ]
  },
  {
    "objectID": "develop/data_transfer.html#data-transfer",
    "href": "develop/data_transfer.html#data-transfer",
    "title": "HPC Lab",
    "section": "",
    "text": "To ensure efficient file transfers on HPC systems, we will go through:\n\nOverview rsync and scp\nBasic commands\n\nWhen transferring files between servers, it’s important to ensure that the files are consistent. Differences in file size can occur due to variations in filesystems, but large differences might indicate an issue. Manually checking file sizes in a terminal (ls -lh or du -sh) to determine if a transfer was successful might not be ideal, as it doesn’t guarantee file integrity. Using checksums provides a more reliable verification method.\nYou can use md5sum to verify that the file contents are identical on both servers. Run the following command on each server:\nmd5sum /path/to/file\nUsing rsync for Efficient File Transfers rsync is a powerful alternative to scp for transferring files. It only sends data if the file has changed, making it more efficient.\n# Transferring files between local machine-server\nrsync -avz local/path/to/file user@server:/remote/path/to/file\n# Transferring files between servers\nrsync -azv server1:/path/to/my_folder server2:/path/to/destination_folder\n\n\n\n\n\n\nNote\n\n\n\nTo transfer files directly between two servers from your local workstation, ensure your SSH setup (configuration, keys, etc.) allows access to both servers. Check this section if you need help setting up your keys (generating, configuring and managing).\n\n\n\n\nrsync is an efficient protocol to compare and copy files between directories or server. It can resume interrupted transfers and compress files on the fly.\n\nChecksum Verification: rsync checks the hashsums of files and only transfers data if the hashes differ. This ensures that only the changed parts of the files are sent (so you can rsync a whole folder, and only the changes files will be send).\nTimestamp Preservation: Using the -a flag with rsync preserves the modified timestamps of files, which is particularly useful for tools like Snakemake.\n\n\n\n\n\n-a: Archive mode, preserves file attributes like timestamps and permissions (important if you are using snakemake).\n-v: Verbose mode, provides detailed information during transfer.\n-z: Compresses data during transfer, reducing the amount of data sent over the network.\n-c: Enables checksum checking, ensuring that files are identical by comparing their contents rather than just their size and modification time.\n\n\n\n\nFor users who prefer a graphical interface, tools like Cyberduck and FileZilla can also be used for transferring files between servers.",
    "crumbs": [
      "HPC launch",
      "Data management",
      "Data transfer"
    ]
  },
  {
    "objectID": "develop/docker.html",
    "href": "develop/docker.html",
    "title": "Docker",
    "section": "",
    "text": "Content\n\n\n\nThis section is divided into two main parts:\n\nUsing Docker images\nBuilding custom Docker images\n\nRefer to the Docker commands overview for a handy checklist or quick reference.\nDocker enables developers build, share, run, and verify applicationsseamlessly across different environments, eliminating the need for complex environment configuration and management. Before diving into hands-on activities, ensure you understand these three key concepts:",
    "crumbs": [
      "HPC pipes",
      "Computational environments",
      "Docker"
    ]
  },
  {
    "objectID": "develop/docker.html#using-docker-images",
    "href": "develop/docker.html#using-docker-images",
    "title": "Docker",
    "section": "Using Docker images",
    "text": "Using Docker images\nFirst, we’ll start by using basic commands with existing Docker images from Docker Hub. This will help you become familiar with Docker’s functionality before we move on to creating our own custom images. While people commonly use shorthand Docker commands like docker pull and docker run, the full versions (e.g., docker image pull, docker container run) provide clearer insights into their underlying concepts and specific functions.\nDocker Hub\n\nSearch for images docker search &lt;name&gt; (e.g. docker search debian)\nDownload an image docker pull &lt;image-name&gt;\n\nLocal Docker Daemon\n\nDisplay all docker images currently stored on your local Docker daemon docker images (alias for docker image ls)\nInspect docker image docker inspect &lt;image_name&gt; (alias for docker image inspect)\nRun a command (cmd) in a container docker run &lt;image_name&gt; cmd (alias for docker container run  &lt;image_name&gt; cmd)\nStart an interactive bash shell docker run -it &lt;image_name&gt; bash. Add other flags like:\n\n-p 8888:8888 to access your interactive shell through ‘localhost:8888’ on your host.\n-rm to automatically delete the container once it stops, keeping your system clean (including its filesystem changes, logs and metadata). If you don’t run this flag, a container will automatically be created and information about tje processes will be kept. Check all containers in your Docker daemon docker container ls -a\n--user=$(id -u):$(id -g) useful if you are using sharing volumes and need appropriate permissions on the host to manipulate files.\n\nShare the current directory with the container docker run -it --volume=$(pwd):/directory/in/container image_name bash, the output of pwd will be mounted to the /directory/in/container (e.g. data, shared, etc.)\nManage your containers using pause or stop\n\nTag images with a new name docker image tag image_name:tag new_name:tag\ndocker logs &lt;container_id&gt;\nRemove images and clean up your hard drive docker rmi &lt;image_name&gt;\nRemove containers docker container rm &lt;container_name&gt;. Alternatively, remove all dead containers: docker container prune\n\nAll Docker containers have a digest which is thesha256 hash of the image. It allows to uniquely identify a docker image and it is great for reproducibility.\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nIn this exercise, we will use a Debian stable image (sha256-540ebf19fb0bbc243e1314edac26b9fe7445e9c203357f27968711a45ea9f1d4) as an example for pulling and inspecting Docker images. This image offers a reliable, minimal base with essential tools, including fundamental utilities like bash for shell scripting and apt for package management. It’s an excellent starting point for developing and testing pipelines or configuring server environments, providing a stable foundation that can be customized with additional packages as needed.\n1. Get a container image\n\nPull docker image (using tag stable otherwise, latest will be pulled by default)\ndocker pull debian:stable@sha256:2171c87301e5ea528cee5c2308a4589f8f2ac819d4d434b679b7cb1dc8de6912\n2. Run commands in a container\nList the content of the container\ndocker run -it -rm debian@sha256:2171c87301e5ea528cee5c2308a4589f8f2ac819d4d434b679b7cb1dc8de6912 ls\nCheck is there is python or perl in this container:\ndocker run -rm debian@sha256:2171c87301e5ea528cee5c2308a4589f8f2ac819d4d434b679b7cb1dc8de6912 which -a python perl\n\n3. Use docker interactively\n\nEnter the container interactively with a Bash shell\ndocker run -it -rm debian@sha256:2171c87301e5ea528cee5c2308a4589f8f2ac819d4d434b679b7cb1dc8de6912 /bin/bash\nNow, collect info about the packages installed in the environment.\n\n\nInteractive Docker container\n\nhostname\nwhoami\nls -la ~/\npython  \necho \"Hello world!\" &gt; ~/myfile.txt\nls -la ~/\n\n4. Exit and check the container\nExit the container\nexit\n\nNow, rerun commands from step 3 under “Interactive Docker container”. Does the output look any different?\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nYou will notice that you are the root but the name of the machine has now changed and the file that you had created has disappeared.\n\n\n\n\n\n5. Inspect the docker image\ndocker image inspect debian:stable@sha256:2171c87301e5ea528cee5c2308a4589f8f2ac819d4d434b679b7cb1dc8de6912\n6. Inspect Docker Image Details Identify the date of creation, the name of the field with the digest of the image and command run by default when entering this container.\n7. Remove container\ndocker image docker rmi debian:stable@sha256:2171c87301e5ea528cee5c2308a4589f8f2ac819d4d434b679b7cb1dc8de6912\n\n\n\n\n\n\nWhen you exit a container, any changes made are lost because each new container starts in a clean state. Containers are isolated environments that don’t retain memory of past interactions, allowing you to experiment without affecting your system. This immutability ensures that your program will run under the same conditions in the future, even months later. So, how can you retrieve the results of computations performed inside a container?\nTo preserve your work, it is crucial to use Docker’s volume mounting feature. When running a Docker container with the -v (volume) option, it’s best to execute the command from the temporary directory (e.g. /tmp/ ) or a project-specific directory rather than your home directory. This practice helps maintain a clean and isolated environment within the container, while ensuring your results are saved in a designated location outside the container’s ephemeral filesystem.\n\nNamed volumes\nNow, we will be using named volumes or shared directories. When you make changes in these directories within the container, the updates are immediately reflected in the corresponding directory on the host machine. This setup enables seamless synchronization between the container and the host file system.\n# -v: mounting only your project-specific dir \ndocker run -it --volumes &lt;project_dir&gt;\n# Named volumes: are managed by Docker and are isolated from your host file system\ndocker volume create &lt;my_project_volume&gt;\ndocker run -it --volumes &lt;my_project_volume&gt;\n\n\n\n\n\n\nWhy not mounting your home directory\n\n\n\nMounting your current working directory (${PWD}) into the container at e.g. /home/rstudiocan compromise the benefits of container isolation. If you run the command from your home directory, any local packages or configurations (such as R or Python packages installed with install.packages or pip) would be accessible inside the container.This inadvertently includes your personal setup and packages, potentially undermining the container’s intended clean and isolated environment. To maintain effective isolation, it’s better to use a temporary or project-specific directory for mounting.\n\n\nAlternatively, you can add a volume to a project, by modifying the compose.yaml (also named docker-compose.yml) file. There are two types of volumes:\n\nService-level name: specify how volumes are mounted inside the container. In this case, dataset volume (defined at the top-level) will be mounted to the /path/in/container/ (e.g. data, results or logs directory) inside the myApp container.\nTop-level volume: volumes shared across multiple services in the compose.yaml file. The volume dataset can be referenced by any service and will be created if it doesn’t exit. If the host path is not specified, Docker will automatically create and manage the volume in a default location on the host machine. However, if you need the volume to be located in a specific directory, you can specify the host path directly (option 2).\n\n\n\ncompose.yaml\n\n# Service-level name\nmyApp:\n    # ...\n    volumes:\n      - dataset:/path/in/container/\n    # Option 2 \n    # - /my/local/path:/path/in/container/\n                      \n# Top-level volume\nvolumes:\n  dataset:\n\nIn this case, a volume named mydata will be mounted to the /data/ directory inside the container running the todo-databse service.\nLet’s not forget to track changes to container images for reproducibility by using version control. Store your images in a shared storage area, such as with Git or Git Annex, to manage versions and facilitate collaboration.\n\n\nTransfer and backup Docker images\nSaving a Docker image as a tar file is useful for transferring the image to another system or operating system without requiring access to the original Docker registry. The tar file contains several key components:\n\nmetadata: JSON files essential to reconstruct the image\n\nlayer information with each layer associated with metadata that includes which commands are used to create the later.\n\n\nDockerfile\n\nFROM ubuntu:20.04 # Layer 1 - base image \nRUN apt-get update && apt-get install -y python3 # Layer 2 - the result of running the command \nCOPY . /app # Layer 3 - add application files to the images\n\ntags pointers to specific image digests or versions.\nhistory of the image and instructions from the DOckerfile that were used to build the image.\nmanifest which ties together the layers and the overall image structure.\n\nFilesystem: the actual content, files and directories that make up the image.\n\n# save to tar file \ndocker image save --output=image.tar image_name \n# load tar file\ndocker image load --input=image.tar  \nIn some situations, you may need to access the filesystem content of a Docker image for purposes such as debugging, backup, or repurposing. To do this, you should create a container from the image and then export the container’s root filesystem to a tar file. Similarly, you can create a new Docker image from this tar file if needed.\ndocker container create --name=temp_container image_name\ndocker container export --output=image.tar temp_container\ndocker container rm temp_container\n# new image importing tar file \ndocker image import --input image.tar image_name",
    "crumbs": [
      "HPC pipes",
      "Computational environments",
      "Docker"
    ]
  },
  {
    "objectID": "develop/docker.html#building-docker-images",
    "href": "develop/docker.html#building-docker-images",
    "title": "Docker",
    "section": "Building Docker images",
    "text": "Building Docker images\nWe’re now ready to build custom Docker images. We’ll start with a minimal pre-built image, add the necessary software, and then upload the final image to Docker Hub.\nA common practice is to start with a minimal image from Docker Hub. There are many base images you can use to start building your customized Docker image, and the key difference between them lies in what they already include. For example:\n\nDebian: This is a very minimal base image, including only the most essential utilities and libraries required to run Debian, such as the core Linux utilities and the package manager (apt). It offers a high degree of customization, giving you full control over what you install. However, you’ll need to manually add all the necessary software, making the setup process longer. This image is relatively small in size, making it a good starting point if you want to build your environment from scratch.\nData Science-Specific Images: These images come pre-configured with a wide range of tools pre-installed, reducing the amount of customization needed. This allows you to get started much faster, though it also means less flexibility. These images tend to be larger in size due to the pre-installed software. For example:\n\ntensorflow/tensorflow: This image includes TensorFlow and often other machine learning libraries, making it ideal for deep learning projects.\njupyter/scipy-notebook: This image is around 2-3 GB and includes Python, Jupyter Notebook, and libraries like NumPy, Pandas, Matplotlib, and more, making it a comprehensive option for data science tasks.\nr-base: This image provides a base for R environments, useful for data analysis and statistical computing.-\nrocker/rstudio: This image includes RStudio and a base R environment, making it perfect for those working in R for statistical computing and data analysis.\n\n\nOnce you have chosen your base image, use a Dockerfile to modify its components, specifying commands for software installation and configuration. The Dockerfile acts as a recipe, providing a list of steps to build the image.\n\n\nDockerfile\n\n# deploy docker container\nFROM &lt;node|debian|python|jupyter-base&gt;\n\n# Info and rights of the app\nLABEL software=\"App_name - sandbox\" \\\n    maintainer=\"&lt;author.address@sund.ku.dk&gt;\"  \\\n    version=\"YYYY.MM.DD\" \n\n# root: needed to install and modify the container \nUSER 0 \n\n# run bash commands (eg. installing packages or softwares)\nRUN mkdir -p\n\n# install packages & dependencies \nRUN apt update \\\n    && apt install -y jupyter-notebook python3-matplotlib python3-pandas python3-numpy  \\\n    && rm -rf /var/lib/apt/lists/* # cleanup tmp files created by apt\n    && rm -fr node_modules # remove directory which contains Node.js packages & dependencies\n\n# set working directory (this directory should act as the main application directory)\nWORKDIR /app\n\n# copy files to-from directories\nCOPY /from/path/... /to/path/...\n\n# set environment variables for Conda\nENV\n\n# switch to user 1000 instead of the default root user \nUSER 11042\n\nIn this example, the RUN command updates the package list and installs Jupyter along with all necessary dependencies. It’s common practice to remove unnecessary dependencies afterward, as shown in the example above.\n\n\n\n\n\n\nLabel\n\n\n\nUse key-value pair syntax to add the following labels to the container:\n\nsoftware = name of the app\nauthor = maintainer or author of the app\nversion = version of the app\nlicense = app licence, e.g., “MIT”\ndescription = very short description of the app\n\n\n\nAfter preparing your Dockerfile, use the docker build command to create a new image based on those instructions. Docker’s isolation ensures that any changes made within a container are lost when you exit it. Nevertheless, you can use docker commit to save these changes by creating a new image from the updated container.This new image will retain your modifications and can be used to launch new container.\ndocker build -t &lt;account/app:version&gt; &lt;directory_docker&gt;\n\n\n\n\n\n\nExample of a tag name\n\n\n\nFor the docker build -t flag, the format of the tag is used to specify both the repository and the version of the Docker image. It consists of three different elements (e.g.: albarema/sandbox_app:v1.0):\n\nRepository name where the image will be stored, account in Docker registry (e.g. albarema)\nName of the image (e.g. sandbox_app)\nVersion label for the image (e.g. v1.0, test, etc.)\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\n\nCreate a Dockerfile in a project-specific dir (e.g.: sandbox-debian-jupyter). We will add a command to clean up the package after after installation used to reduce the image size.\n\nFROM debian:stable \n\nLABEL maintainer=\"Name Surname &lt;abd123@ku.dk&gt;\"\n\n# Update package list and install necessary packages\nRUN apt update \\\n    && apt install -y jupyter-notebook \\\n                      python3-matplotlib \\\n                      python3-pandas \\\n                      python3-numpy \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/* # cleanup tmp files created by apt\n\n# You may consider adding a working directory\nWORKDIR /notebooks\n\n# and a command to start Jupyter Notebook\n# CMD [\"jupyter-notebook\", \"--ip=0.0.0.0\", \"--allow-root\"]\n\nBuild the Docker image using, for example, docker build -t albarema/sandbox-debian-jupyter:1.0 sandbox-debian-jupyter\n\n\n\n\n\n\n\nTesting the custom image\nLet’s verify if the custom image functions as expected, running the following command:\n\n\nTerminal\n\ndocker run --rm -p 8888:8888 --volume=$(pwd):/root albarema/sandbox-debian-jupyter:1.0 jupyter-notebook\n\nJupyter typically refuses to run as root or accept network connections by default. To address this, you need to either add --ip=0.0.0.0 --allow-root when starting Jupyter to the command above or uncomment the last line in the Dockerfile above (CMD [\"jupyter-notebook\", \"--ip=0.0.0.0\", \"--allow-root\"]). Alternatively, you can run the container with the flag --user=$(id -u):$(id -g)to ensure that files created in the container have matching user and group ownership with those on the host machine, preventing permission issues. However, this restricts the container from performing root-level operations. For broader usability and security, it is advisable to create a non-root user (e.g. jovyan) within the Docker image by adding user setup commands to the Dockerfile (see below). This approach makes the image more user-friendly and avoids file ownership conflicts.\n\n\nDockerfile\n\n# Creating a group & user\nRUN addgroup --gid 1000 user && \\\n    adduser --uid 1000 --gid 1000 --gecos \"\" --disabled-password jovyan\n# Setting active user \nUSER jovyan\n# setting working directory \nWORKDIR /home/jovyan\n\n\n\n\n\n\n\nTip\n\n\n\n\nUse --rm flag to remove automatically the container once it stops running to avoid clustering your system with stopped containers.\nUse --volume to mount data into the container (/root), for example, your working directory\nUse --file flag to test to dockerfile versions (default:“PATH/Dockerfile”)\n\ndocker build -t albarema/sandbox-debian-jupyter:2.0 sandbox-debian-jupyter -f sandbox-debian-jupyter/Dockerfile2\n\n\nNow that we have fixed that problem, we will test A. using a port to launch a Jupyter Notebook (or Rstudio server) and B. starting a bash shell interactively.\n# Option A. Start jupyter-notebook or on the server \ndocker run --rm -p 8888:8888 --volume=$(pwd):/home/jovyan albarema/sandbox-debian-jupyter:1.0 jupyter-notebook --ip=0.0.0.0 --allow-root\n\n# Option B. Start an interactive shells instead \ndocker run -it --rm sandbox-debian-jupyter:1.0 /bin/bash\n\n\n\n\n\n\nWhich port to use?\n\n\n\nThe -p option in Docker allows services (e.g.: Jupyter Notebooks) running inside the container to be accessible from outside (through localhost:1234 on your local machine).\n\n-p host_port:container_port # port mappings between the host machine and the container\n-p 8787:8787 # connect port 8787 on the host machine to port 8787 inside the container. This setup allows you to access RStudio, which is running inside the container, by navigating to http://localhost:8787 on your host machine\n-p 8888:8888 # similarly, this setup enables you to access JupyterLab, which is running inside the container, by going to http://localhost:8888 on your host machine.\n\n\n\n\n\nCreate a new app from scratch\nWhen working with containers, you usually need to create a Dockerfile to define your image and compose.yaml file that defines how to run it. As an alternative to starting with a base image and modifying it, you can use the following command:\ndocker init\nThis utility will walk you through creating the following files with sensible defaults for your project:\n- .dockerignore\n- Dockerfile\n- compose.yaml\n- README.Docker.made\n\n\nPublish your Docker image on Docker Hub\nPublishing your Docker image on Docker Hub is straightforward. You just need a Docker Hub account—preferably linked to your GitHub account if you have one. For detailed instructions, refer to the documentation. The process involves only a few commands.\n# login\ndocker login # &lt;username&gt; &lt;pw&gt;\n# optional change tag\n# docker tag &lt;old&gt; &lt;new&gt;\n# push image \ndocker push albarema/sandbox-debian-jupyter:1.0\nIn Docker, the file system of an image is built using several layers, or overlays. Each layer represents a set of changes or additions made to the image. When you update software or packages within a Docker container, a new layer is created with only the new or changed content, rather than modifying the existing layers.\nYou are now ready to share the link to your Docker image with your colleagues, ensuring that everyone uses the exact same environment.",
    "crumbs": [
      "HPC pipes",
      "Computational environments",
      "Docker"
    ]
  },
  {
    "objectID": "develop/docker.html#sources",
    "href": "develop/docker.html#sources",
    "title": "Docker",
    "section": "Sources",
    "text": "Sources\n\nDocker Hub",
    "crumbs": [
      "HPC pipes",
      "Computational environments",
      "Docker"
    ]
  }
]
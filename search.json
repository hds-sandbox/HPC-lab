[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "HPC best practices",
    "section": "",
    "text": "High Performance Computing (HPC) plays a crucial role for researchers by offering the computational speed and power needed to manage large and complex data sets, perform simulations, and address intricate problems that would be impractical or too time-consuming with standard computing methods.\nKnowing which HPC resources are accessible and how to use them efficiently is essential for researchers. Making the most of these resources can significantly expedite research and drive innovation. By becoming proficient with the available tools and technologies, researchers can address complex challenges, analyze extensive data sets, and execute advanced simulations with increased speed and accuracy. This module provides essential knowledge on HPC resources and best practices for their utilization.\nThis module offers content for three distinct courses:\n\nHPC Launch: Foundations on HPC and essential knowledge on national HPC resources\nHPC Pipes: Best practices for using workflow management systems and computational environments with HPC\nHPC ML (Machine Learning): Insights into applying HPC for machine learning tasks, including model training, data analysis, and optimization techniques.\n\nBy the end of all the modules, you will gain practical skills in promoting reproducibility through comprehensive training in HPC resource management, workflow pipelines, and computing environments.\n\n\n\n\n\n\nGeneral Course Goals\n\n\n\nBy the end of this workshop, you should be able to apply the following concepts in the context of Next Generation Sequencing data:\n\nUnderstand the Importance of Research Data Management (RDM)\nMake your data analysis and workflows reproducible and FAIR\nMake FAIR environment using conda or Docker\n\n\n\n\n\n\n\n\n\nHPC best practices\n\n\n\nWe offer in-person workshops, keep an eye on the upcoming events on the Sandbox website."
  },
  {
    "objectID": "index.html#hpc-pipes",
    "href": "index.html#hpc-pipes",
    "title": "HPC best practices",
    "section": "HPC pipes",
    "text": "HPC pipes\nThe course “HPC pipes” is designed to provide participants with foundational knowledge and practical skills in writing reproducible pipelines. As part of effective data management, it is crucial that researchers create reproducible analyses that enable others to validate and build upon their work. We will explore essential elements of reproducibility and efficiency in computational research, highlighting techniques and tools for creating robust and transparent coding and workflows. By prioritizing reproducibility and replicability, researchers can enhance the credibility and impact of their findings while fostering collaboration and knowledge dissemination within the scientific community. This approach guarantees efficient research management. Explore our content on practical RDM for more details.\n\n\n\n\n\n\nCourse Requirements\n\n\n\nIt is important to be familiar with unix and python. Follow the tutorials in the links below if you need a refresher.\n\nCommand Line experience (Software Carprentry Shell)\nProgramming experience (Python)\n\n\n\n\n\n\n\n\n\nModule Goals\n\n\n\n\nUnderstand the rol of scientific pipelines\nRun existing pipelines\nImplement and modify pipelines\nSpecify software and computational resource needs\nCustomise your pipeline to accept user-defined configurations (params)\nCreate reproducible analyses that can be adapted to new data with little effort\nIntegrate workflows with software environments\n\n\n\n\nAcknowledgements\nOur exercises are developed using the R packaged developed by Barr and DeBruine (2023)."
  },
  {
    "objectID": "develop/jobs.html",
    "href": "develop/jobs.html",
    "title": "HPC jobs",
    "section": "",
    "text": "Note\n\n\n\nThere are many job scheduler programs available but a very common one is SLURM. Useful commands:\n# Submit the job\nsbatch \n# Check the job is in the queue\nsqueue \nToy example of a bash script to submit\n#!/bin/bash\n#SBATCH -D /home/USERNAME/  # working directory\n#SBATCH -c 1        # number of CPUs. Default: 1\n#SBATCH -t 00:10:00 # time for the job HH:MM:SS.\n#SBATCH --mem=1G    # RAM memory\n\n# my commands: software, pipeline, etc.\nsnakemake -j1\n\n\n\n\n\nJob parallelization is crucial for achieving high performance and running jobs effectively on an HPC. Here are two key scenarios where it is particularly important:\n\nIndependent computational tasks: When tasks are independent of each other, job parallelization can enhance efficiency by allowing them to run concurrently.\nMulti-threaded tools: Some tools are specifically designed to perform parallel computations through multi-threading, enabling them to utilize multiple CPU cores for increased performance.\n\n\n\n\n\n\n\nJob parallelisation using slurm\n\n\n\nJobs arrays -a",
    "crumbs": [
      "HPC launch",
      "HPC systems",
      "HPC jobs"
    ]
  },
  {
    "objectID": "develop/jobs.html#best-practices-for-running-a-job-on-a-cluster",
    "href": "develop/jobs.html#best-practices-for-running-a-job-on-a-cluster",
    "title": "HPC jobs",
    "section": "",
    "text": "Note\n\n\n\nThere are many job scheduler programs available but a very common one is SLURM. Useful commands:\n# Submit the job\nsbatch \n# Check the job is in the queue\nsqueue \nToy example of a bash script to submit\n#!/bin/bash\n#SBATCH -D /home/USERNAME/  # working directory\n#SBATCH -c 1        # number of CPUs. Default: 1\n#SBATCH -t 00:10:00 # time for the job HH:MM:SS.\n#SBATCH --mem=1G    # RAM memory\n\n# my commands: software, pipeline, etc.\nsnakemake -j1\n\n\n\n\n\nJob parallelization is crucial for achieving high performance and running jobs effectively on an HPC. Here are two key scenarios where it is particularly important:\n\nIndependent computational tasks: When tasks are independent of each other, job parallelization can enhance efficiency by allowing them to run concurrently.\nMulti-threaded tools: Some tools are specifically designed to perform parallel computations through multi-threading, enabling them to utilize multiple CPU cores for increased performance.\n\n\n\n\n\n\n\nJob parallelisation using slurm\n\n\n\nJobs arrays -a",
    "crumbs": [
      "HPC launch",
      "HPC systems",
      "HPC jobs"
    ]
  },
  {
    "objectID": "develop/jobs.html#sources",
    "href": "develop/jobs.html#sources",
    "title": "HPC jobs",
    "section": "Sources",
    "text": "Sources\nUseful links\n\nAcknowledgements",
    "crumbs": [
      "HPC launch",
      "HPC systems",
      "HPC jobs"
    ]
  },
  {
    "objectID": "develop/exercise_launch.html",
    "href": "develop/exercise_launch.html",
    "title": "Exercises",
    "section": "",
    "text": "Put your learning to the test with what you’ve covered so far.",
    "crumbs": [
      "HPC launch",
      "Exercises"
    ]
  },
  {
    "objectID": "develop/exercise_launch.html#general-hpc-launch",
    "href": "develop/exercise_launch.html#general-hpc-launch",
    "title": "Exercises",
    "section": "General HPC launch",
    "text": "General HPC launch\n1. Should I run the commmand unzip myfile.zip to decompress the file from the login node? TRUEFALSE\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nAvoid running anything from the login node. Create an interactive session to do this!\n\n\n\n\n\n2. Is it a good idea to keep data in the scratch folder until the project is finished? TRUEFALSE",
    "crumbs": [
      "HPC launch",
      "Exercises"
    ]
  },
  {
    "objectID": "develop/nextflow.html",
    "href": "develop/nextflow.html",
    "title": "Nextflow",
    "section": "",
    "text": "Nextflow is a workfow management system that offers scalable and portable NGS data analysis pipelines, facilitating data processing across diverse computing environments. It streamlines and automates various data analysis steps, enabling parallel processing and seamless integration with existing tools. ç",
    "crumbs": [
      "HPC pipes",
      "Pipeline languages",
      "Nextflow"
    ]
  },
  {
    "objectID": "develop/nextflow.html#nf-core",
    "href": "develop/nextflow.html#nf-core",
    "title": "Nextflow",
    "section": "nf-core",
    "text": "nf-core\nTemplates",
    "crumbs": [
      "HPC pipes",
      "Pipeline languages",
      "Nextflow"
    ]
  },
  {
    "objectID": "develop/nextflow.html#sources",
    "href": "develop/nextflow.html#sources",
    "title": "Nextflow",
    "section": "Sources",
    "text": "Sources\n\nfaircookbook worflows",
    "crumbs": [
      "HPC pipes",
      "Pipeline languages",
      "Nextflow"
    ]
  },
  {
    "objectID": "develop/fair_envs.html",
    "href": "develop/fair_envs.html",
    "title": "FAIR environments",
    "section": "",
    "text": "Computational environments vary significantly from one system to another, encompassing differences in the operating system, installed software, and software package versions. If a research project is moved to a different computer or platform, the analysis might not run or produce consistent results, specially if the software relies on specific configurations or dependencies. Dependencies are libraries or tools that software relies on and can change over time or be poorly documented, leading to hidden variations between setups. Simply knowing a software version might often be insufficient for guaranteeing consistent results across different environments.\nFor research to be reproducible, the original computational environment must be recorded so others can replicate it accurately. This involves making your code easy to install and run by others, document the setup process thoroughly, and carefully manage and share your software environment. There are several methods to achieve this:\nWhile package managers are very easy to use and share across different systems, and are lightweight and efficient, offering fast start-up times, containers provide a full environment isolation (including the operating system) which ensures consistent behavior across different systems. Check the table below for a full overview.\nRecording and sharing the computational environment is essential for ensuring reproducibility and transparency. Below, we will explore two tools that can help with this: mamba, a package manager, and Docker, a container system. We will explain the differences between them and provide guidance on choosing the right tool for your specific scenario.",
    "crumbs": [
      "HPC pipes",
      "Computational environments",
      "FAIR environments"
    ]
  },
  {
    "objectID": "develop/fair_envs.html#package-managers",
    "href": "develop/fair_envs.html#package-managers",
    "title": "FAIR environments",
    "section": "Package managers",
    "text": "Package managers\nMamba is a reimplementation of the Conda package manager in C++. While our focus will be on Mamba, it’s important to note that it maintains compatibility with Conda by using the same command-line parser, package installation and uninstallation code, and transaction verification routines.\nMamba uses software installation specifications that are maintained by extensive communities of developers, organized into channels, which serve as software repositories. For example, the “bioconda” channel specializes in bioinformatics tools, while “conda-forge” covers a broad range of data science packages.\n\n\n\n\n\n\nMamba vs. conda\n\n\n\nAs previously mentioned, mamba is a newer and faster implementation. The two commands can be used interchangeable (for most tasks). If you use Conda, you should still complete the exercises, as you’ll gain experience with both tools. For more information on their ecosystem and advantages here.\n\n\nMamba allows you to create different software environments, where multiple package version can co-exit on your system.\n\n\n\n\n\n\nBuild your mamba environment\n\n\n\n\n\n\n\nFollow mamba instructions to install it. Let’s also include bioconda and conda-forge channels which will come very handy.\nconda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge\nNow you are set to create your first environment. Follow these steps:\n\nCreate a new environment named myenv\nInstall the following packages in myenv: bowtie2, numpy=1.26.4, matplotlib=3.8.3\nCheck the environments available\nLoad/activate the environment\nCheck which python executable is being used and that bowtie2 is installed.\nDeactivate the environment\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nHere are some of the commands you need for the exercise.\nmamba create -n &lt;ENV-NAME&gt;\nmamba install --channel &lt;CHANNEL-NAME&gt; --name &lt;ENV-NAME&gt;\nmamba env list\n# mamba init \nmamba activate &lt;ENV-NAME&gt;\nmamba deactivate \n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThe syntax to create a new environment is: mamba create --name myenv\nExample “bowtie2”: Go to anaconda.org and search for “bowtie2” to confirm it is available through Mamba and which software channel it is provided from. You will find that it is available via the “bioconda” channel: https://anaconda.org/bioconda/bowtie2. The syntax to install packages is: mamba install --channel &lt;CHANNEL-NAME&gt; --name &lt;ENV-NAME&gt; &lt;SOFTWARE-NAME&gt;\n\nmamba install --name myenv --channel bioconda bowtie2=2.5.3 \"matplotlib=3.8.3\" \"numpy=1.26.4\"\nDo the same with the others. 3. To see al environments available mamba env list. There will be a “*” showi8ng the one is activated. 4. Load the environment mamba activate myenv. 5. which python -&gt; should print the one in the environment that is active (path similar to /home/mambaforge/envs/myenv/bin/python). bowtie2 --help 6. Conda deactivate\n\n\n\n\n\n\n\n\nIf you have different environments set up for various projects, you can switch between them or run commands directly within a specific environment using:\nmamba run -n &lt;ENV-NAME&gt; python myscript.py\n\n\n\n\n\n\nLoading mamba environments in shell scripts\n\n\n\nIf you need to activate an environment in a shell script that will be submitted to SLURM, you must first source Mamba’s configuration file. For instance, to load the myenv environment we created, the script would include the following code:\n# Always add these two commands to your scripts\neval \"$(conda shell.bash hook)\"\nsource $CONDA_PREFIX/etc/profile.d/mamba.sh\n\n# then you can activate the environment\nmamba activate myenv\nWhen jobs are submitted to SLURM, they run in a non-interactive shell where Mamba isn’t automatically set up. By running the source command, you ensure that Mamba’s activate function is available. It’s important to remember that even if the environment is loaded on the login node, the scripts will execute on a different machine (one of the compute nodes). Therefore, always include the command to load the Mamba environment in your SLURM submission scripts.",
    "crumbs": [
      "HPC pipes",
      "Computational environments",
      "FAIR environments"
    ]
  },
  {
    "objectID": "develop/fair_envs.html#containers",
    "href": "develop/fair_envs.html#containers",
    "title": "FAIR environments",
    "section": "Containers",
    "text": "Containers\nEssentially, a container is a self-contained, lightweight package that includes everything needed to run a specific application—such as the operating system, libraries, and the application code itself. Containers operate independently of the host system, which allows them to run the same software across various environments without any conflicts or interference. This isolation ensures that researchers can consistently execute their code on different systems and platforms, without worrying about dependency issues or conflicts with other software on the host machine.\n\n\n\n\n\n\nDocker vs. Singularity\n\n\n\nThe most significant difference is at the permission level required to run them. Docker containers operate as root by default, giving them full access to the host system. While this can be useful in certain situations, it also poses security risks, especially in multi-user environments. In contrast, Singularity containers run as non-root users by default, enhancing security and preventing unauthorized access to the host system.\n\nDocker is ideal for building and distributing software across different operating systems\nSingularity is designed for HPC environments and offers high performance without needing root access\n\n\n\nIn the following sections, we’ll cover how to retrieve environment information, utilize containers, and automate environment setup to improve reproducibility.\n\nSingularity on a remote server\nWhile you can build your own Singularity images, many popular software packages already have pre-built images available from public repositories. The two repositories you’ll most likely use or hear about are:\n\ndepot.galaxyproject.org\nContainer Library (Sylabs)\nDocker Hub\n\n\n\n\n\n\n\nInstallation\n\n\n\n\nSingularity installation guides\n\n# You will only need to vagrant init once \nexport VM=sylabs/singularity-3.0-ubuntu-bionic64 && \\\n    vagrant init $VM && \\\n    vagrant up && \\\n    vagrant ssh\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nWe recommend using the pre-installed version provided by your system administrators if you’re working on a shared system. If you’re working on your own computer, you can install the necessary software using Mamba.\nThey might host different versions of the same software, so it’s worth checking both to find the version you need.\nTo download a software container from public repositories, use the singularity pull command.\nTo execute a command within the software container, use the singularity run command.\nGood practice: create a directory to save all singularity images together. .sif is the standard extension for the images.\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nDownload a singularity image from one of the two repositories listed above (choose a software like bcftools, bedtools, bowtie2, seqkit…) and run the --help command. This command displays the help documentation of the program, verifying that our image is functioning correctly and includes the intended software.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n# create a directory for our singularity images\nmkdir images\n\n# download the image\nsingularity pull images/bowtie2-2.5.4.sif https://depot.galaxyproject.org/singularity/bowtie2%3A2.5.4--he20e202_2\n\n# run the image: singularity run &lt;PATH-TO-IMAGE&gt; &lt;YOUR COMMANDS&gt;\nsingularity run images/bowtie2-2.5.4.sif bowtie2 --help",
    "crumbs": [
      "HPC pipes",
      "Computational environments",
      "FAIR environments"
    ]
  },
  {
    "objectID": "develop/fair_envs.html#sources",
    "href": "develop/fair_envs.html#sources",
    "title": "FAIR environments",
    "section": "Sources",
    "text": "Sources\n\nAnaconda for searching Mamba/conda packages\nBioconda for installing software package related to biomedical research\nConda cheat sheet\nfaircookbook worflows\nDocker\nDocker get-started\nThe turing way - reproducible research\n\nFind pre-built singularity images:\n\ndepot.galaxyproject.org\nSylabs\n\nOther training resources: The turing way - reproducible research and HPC intro by Cambridge",
    "crumbs": [
      "HPC pipes",
      "Computational environments",
      "FAIR environments"
    ]
  },
  {
    "objectID": "develop/scripts/01.iris.html",
    "href": "develop/scripts/01.iris.html",
    "title": "Notebook Iris Dataset",
    "section": "",
    "text": "Before we dive in, here’s a quick summary: the dataset contains 150 samples of iris flowers, each characterized by four features: Sepal Length, Sepal Width, Petal Length, and Petal Width, all measured in centimeters. These samples are grouped into three species: Setosa, Versicolor, and Virginica. If you’re not familiar with the dataset, you can learn more about it here."
  },
  {
    "objectID": "develop/scripts/01.iris.html#loading-the-dataset",
    "href": "develop/scripts/01.iris.html#loading-the-dataset",
    "title": "Notebook Iris Dataset",
    "section": "1 Loading the dataset",
    "text": "1 Loading the dataset\nLet’s start by importing the iris dataset and manipulating the dataframe so that the column names match the feature names.\n\nfrom sklearn.datasets import load_iris\nimport matplotlib.pyplot as plt\nimport pandas as pd \n\n# Load toy dataset\niris = load_iris() \n # Create dataframe using feature names\ndf = pd.DataFrame(iris.data, columns=iris.feature_names)"
  },
  {
    "objectID": "develop/scripts/01.iris.html#exploring-the-dataset",
    "href": "develop/scripts/01.iris.html#exploring-the-dataset",
    "title": "Notebook Iris Dataset",
    "section": "2 Exploring the dataset",
    "text": "2 Exploring the dataset\nLet´s start by exploring the species by plotting the sepal length vs. width in a scatter plot\n\nimport matplotlib.pyplot as plt\n\n_, ax = plt.subplots()\nscatter = ax.scatter(iris.data[:, 0], iris.data[:, 1], c=iris.target)\nax.set(xlabel=iris.feature_names[0], ylabel=iris.feature_names[1])\n_ = ax.legend(\n    scatter.legend_elements()[0], iris.target_names, loc=\"lower right\", title=\"Classes\"\n)\n\n\n\n\n\n\n\n\nYou can already see a pattern regarding the Setosa type, which is easily identifiable based on its short and wide sepal. Only considering these 2 dimensions, sepal width and length, there’s still overlap between the Versicolor and Virginica types."
  },
  {
    "objectID": "develop/scripts/01.iris.html#transforming-the-dataset",
    "href": "develop/scripts/01.iris.html#transforming-the-dataset",
    "title": "Notebook Iris Dataset",
    "section": "3 Transforming the dataset",
    "text": "3 Transforming the dataset\nWe will now perform feature engineering and create a new feature called petal area (petal length * petal width), and will do the same for the sepal.\n\ndf['sepal_area'] = df['sepal length (cm)'] * df['sepal width (cm)']\ndf['petal_area'] = df['petal length (cm)'] * df['petal width (cm)']\n\nFinally, let’s new information by binning the sepal length into 3 categories (short, medium and long)\n\ndf['sepal_length_bin'] = pd.cut(df['sepal length (cm)'], bins=3, labels=[\"short\", \"medium\", \"long\"])\ndf\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\nsepal_area\npetal_area\nsepal_length_bin\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n17.85\n0.28\nshort\n\n\n1\n4.9\n3.0\n1.4\n0.2\n14.70\n0.28\nshort\n\n\n2\n4.7\n3.2\n1.3\n0.2\n15.04\n0.26\nshort\n\n\n3\n4.6\n3.1\n1.5\n0.2\n14.26\n0.30\nshort\n\n\n4\n5.0\n3.6\n1.4\n0.2\n18.00\n0.28\nshort\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\n20.10\n11.96\nmedium\n\n\n146\n6.3\n2.5\n5.0\n1.9\n15.75\n9.50\nmedium\n\n\n147\n6.5\n3.0\n5.2\n2.0\n19.50\n10.40\nmedium\n\n\n148\n6.2\n3.4\n5.4\n2.3\n21.08\n12.42\nmedium\n\n\n149\n5.9\n3.0\n5.1\n1.8\n17.70\n9.18\nmedium\n\n\n\n\n150 rows × 7 columns"
  },
  {
    "objectID": "develop/scripts/01.iris.html#computing-summary-statistics",
    "href": "develop/scripts/01.iris.html#computing-summary-statistics",
    "title": "Notebook Iris Dataset",
    "section": "4 Computing summary statistics",
    "text": "4 Computing summary statistics\nNow, we can extract summary statistics of the species “setosa” and compare it to another species\n\n# Map targets to species names and add them to a new column \ndf['species'] = iris.target_names[iris.target] \n# Display first few rows\ndf.head() \n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\nsepal_area\npetal_area\nsepal_length_bin\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n17.85\n0.28\nshort\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\n14.70\n0.28\nshort\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\n15.04\n0.26\nshort\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\n14.26\n0.30\nshort\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\n18.00\n0.28\nshort\nsetosa\n\n\n\n\n\n\n\n\n# Select setosa \ndf_setosa = df[df['species'] == \"setosa\"]\nsummary_stats = df_setosa.describe() \n\n# Display summary statistics\nprint(summary_stats)\n\n       sepal length (cm)  sepal width (cm)  petal length (cm)  \\\ncount           50.00000         50.000000          50.000000   \nmean             5.00600          3.428000           1.462000   \nstd              0.35249          0.379064           0.173664   \nmin              4.30000          2.300000           1.000000   \n25%              4.80000          3.200000           1.400000   \n50%              5.00000          3.400000           1.500000   \n75%              5.20000          3.675000           1.575000   \nmax              5.80000          4.400000           1.900000   \n\n       petal width (cm)  sepal_area  petal_area  \ncount         50.000000   50.000000   50.000000  \nmean           0.246000   17.257800    0.365600  \nstd            0.105386    2.933775    0.181155  \nmin            0.100000   10.350000    0.110000  \n25%            0.200000   15.040000    0.280000  \n50%            0.200000   17.170000    0.300000  \n75%            0.300000   19.155000    0.420000  \nmax            0.600000   25.080000    0.960000"
  },
  {
    "objectID": "develop/fair_workflow.html",
    "href": "develop/fair_workflow.html",
    "title": "FAIR computational pipelines",
    "section": "",
    "text": "Course Overview\n\n\n\n\n👨‍💻 Target Audience: PAnyone interested in workflow management systems and software enviroments.\n👩‍🎓 Level: Advanced.\n🔒 License: Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license.\n\n💰 Funding: This project was funded by the Novo Nordisk Fonden (NNF20OC0063268).\nData analysis typically involves the use of different tools, algorithms, and scripts. It often requires multiple steps to transform, filter, aggregate, and visualize data. The process can be time-consuming because each tool may demand specific inputs and parameter settings. As analyses become more complex, the importance of reproducible and scalable automated workflow management increases since the key for reproducibility is automation.",
    "crumbs": [
      "HPC pipes",
      "Pipeline languages",
      "FAIR computational pipelines"
    ]
  },
  {
    "objectID": "develop/fair_workflow.html#good-practices",
    "href": "develop/fair_workflow.html#good-practices",
    "title": "FAIR computational pipelines",
    "section": "Good practices",
    "text": "Good practices\nIf you develop your own software make sure you follow FAIR principles. We highly endorse following these FAIR recommendations.\n\nRemember to create portable code and use relative paths to ensure transferability between users.\nUse git repositories to save your projects and pipelines.\nRegister and publish your scientific computational workflow on WorkflowHub.",
    "crumbs": [
      "HPC pipes",
      "Pipeline languages",
      "FAIR computational pipelines"
    ]
  },
  {
    "objectID": "develop/fair_workflow.html#sources",
    "href": "develop/fair_workflow.html#sources",
    "title": "FAIR computational pipelines",
    "section": "Sources",
    "text": "Sources\n\nSnakemake tutorial\nSnakemake tutorial slides by Johannes Koster\nhttps://bioconda.github.io\nKöster, Johannes and Rahmann, Sven. “Snakemake - A scalable bioinformatics workflow engine”. Bioinformatics 2012.\nKöster, Johannes. “Parallelization, Scalability, and Reproducibility in Next-Generation Sequencing Analysis”, PhD thesis, TU Dortmund 2014.\nfaircookbook worflows\nThe turing way - reproducible research",
    "crumbs": [
      "HPC pipes",
      "Pipeline languages",
      "FAIR computational pipelines"
    ]
  },
  {
    "objectID": "develop/hpc_intro.html",
    "href": "develop/hpc_intro.html",
    "title": "HPC launch",
    "section": "",
    "text": "Course Overview\n\n\n\n\n⏰ Total Time Estimation: X hours\n\n📁 Supporting Materials:\n\n👨‍💻 Target Audience: Ph.D., MSc, anyone interested in HPC systems.\n👩‍🎓 Level: Advanced.\n🔒 License: Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license.\n\n💰 Funding: This project was funded by the Novo Nordisk Fonden (NNF20OC0063268).",
    "crumbs": [
      "HPC launch",
      "HPC systems",
      "HPC launch"
    ]
  },
  {
    "objectID": "develop/hpc_intro.html#title",
    "href": "develop/hpc_intro.html#title",
    "title": "HPC launch",
    "section": "Title",
    "text": "Title\nIntroduction High-Performance Computing (HPCs) and HPC cluster.\nHPC main resources:\n\nCPU\nRAM\nGPU\n\nSchematic of components of an HPC\n\nNodes\nThere are two typoes of nodes on a cluster: - login nodes (also known as head or submit nodes). - compute nodes (also known as worker nodes).\n\n\n\n\n\n\nWhat can I run from a login node\n\n\n\nA straightforward rule: do not run anything on the login node to prevent potential problems. If the login node crashes, the entire system may need to be rebooted, affecting everyone. Remember, you’re not the only one using the HPC—so be considerate of others. For easy, quick tasks, request an interactive access to one of the compute nodes.\n\n\n\n\nJob scheduler\n\n\n\n\n\n\nNote\n\n\n\nSeveral job scheduler programs are available, and SLURM is among the most widely used. In the next section, we’ll explore SLURM in greater detail, along with general best practices for running jobs.\n\n\n\n\nFilesystem\nThe filesystem is the content all the directories and files available to a given process.\n\nScratch\nUsers working space\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nI have an omics pipeline that produces a large number of files, resulting in a couple of terabytes of data after processing and analysis. The project will continue for a few more years, and I’ve decided to store the data in the scratch folder. Do you agree with this decision, and why? What factors should be considered when deciding which data to retain and where to store it?\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nTypically, scratch storage is not backed up, so it’s not advisable to rely on it for important data. At a minimum, ensure you back up the raw data and the scripts used for processing. This way, if some processed files are lost, you can replicate the analyses.\nWhen deciding which data to keep on the HPC, back up, or delete, consider the following:\n\nProcessing Time: Evaluate how long each step of the analysis takes to run. There may be significant computational costs associated with re-running heavy data processing steps.\nStorage Management: Use tools like Snakemake to manage intermediate files. You can configure Snakemake to automatically delete intermediate files once the final results are produced, helping you manage storage more efficiently.\n\n\n\n\n\n\n\n\n\n\n\n\n\nKernel\nThe kernel is essential for managing multiple programs on your machine, each of which runs as a process. Even if you write code assuming full control over the CPU and memory, the kernel ensures that multiple processes can run simultaneously without interfering with each other. It does this by scheduling time for each process and translating virtual memory addresses into physical ones, ensuring security and preventing conflicts.\nThe kernel also ensures that processes can’t access each other’s memory or directly modify the hard drive, maintaining system security and stability. For example, when a process needs to write to a file, it asks the kernel to do so through a system call, rather than writing directly.\nIn conlcusion, it plays a crucial role in managing the CPU, memory, disk, and software environment. By mediating access to these resources, it maintains process isolation, security, and the smooth operation of your system.\n\n\n\n\n\n\nKernel primary roles:\n\n\n\n\nInterfaces with hardware to facilitate program operations\nManages and schedules the execution of processes\nRegulates and allocates system resources among processes\n\n\n\n\n\nBefore start using an HPC\nHigh-Performance Computing (HPC) systems might be organized differently, but there is typically an HPC administration team you can contact to understand how your specific HPC is structured. Key information you should seek from them includes:\n\nThe types of compute nodes available.\nThe storage options you can access and the amount allocated per user.\nWhether a job scheduler software is in use, and if so, which one. You can also request a sample submission script to help you get started.\nThe policy on who bears the cost of using the HPC resources.\nWhether you can install your own software and create custom environments.\n\n\n\n\n\n\n\nBe nice\n\n\n\nIf your HPC system doesn’t have a job scheduler in place, we recommend using the nice command. This command allows you to adjust and manage the scheduling priority of your processes, giving you the ability to run tasks with lower priority when sharing resources with others. By using nice, you can ensure that your processes do not dominate the CPU, allowing other users’ tasks to run smoothly. This is particularly useful in environments where multiple users are working on the same system without a job scheduler to automatically manage resource allocation.\n\n\n\n\n\n\n\n\nHPC\n\n\n\n\n\n\n\n\nDescribe how a typical HPC is organised: nodes, job scheduler and filesystem.\nWhat are the roles of a login node and a compute node? how do they differ?\nDescribe the role of a job scheduler\nWhat are the differences between scratch and home storage and when each should be used?\nWhat is a kernel?",
    "crumbs": [
      "HPC launch",
      "HPC systems",
      "HPC launch"
    ]
  },
  {
    "objectID": "develop/hpc_intro.html#sources",
    "href": "develop/hpc_intro.html#sources",
    "title": "HPC launch",
    "section": "Sources",
    "text": "Sources\nUseful links\n\nAcknowledgements",
    "crumbs": [
      "HPC launch",
      "HPC systems",
      "HPC launch"
    ]
  },
  {
    "objectID": "develop/smk.html",
    "href": "develop/smk.html",
    "title": "Snakemake",
    "section": "",
    "text": "In this section, we will guide you through transitioning from bash scripts or notebooks to workflows. This approach is particularly useful as computations become more complex.",
    "crumbs": [
      "HPC pipes",
      "Pipeline languages",
      "Snakemake"
    ]
  },
  {
    "objectID": "develop/smk.html#sources",
    "href": "develop/smk.html#sources",
    "title": "Snakemake",
    "section": "Sources",
    "text": "Sources\n\nSnakemake tutorial\nSnakemake turorial slides by Johannes Koster\nKöster, Johannes and Rahmann, Sven. “Snakemake - A scalable bioinformatics workflow engine”. Bioinformatics 2012.\nKöster, Johannes. “Parallelization, Scalability, and Reproducibility in Next-Generation Sequencing Analysis”, PhD thesis, TU Dortmund 2014.\nfaircookbook worflows",
    "crumbs": [
      "HPC pipes",
      "Pipeline languages",
      "Snakemake"
    ]
  },
  {
    "objectID": "develop/data_compression.html#data-organization",
    "href": "develop/data_compression.html#data-organization",
    "title": "HPC Lab",
    "section": "Data organization",
    "text": "Data organization\n\nregular expression\ncookiecutter"
  },
  {
    "objectID": "develop/ssh_keys.html",
    "href": "develop/ssh_keys.html",
    "title": "HPC Lab",
    "section": "",
    "text": "Using SSH keys for authentication is recommended for accessing servers and version control platforms such as GitLab, GitHub, or BitBucket. SSH keys provide a more convenient and secure method than passwords, allowing you to avoid typing a password every time you log in.\nThis tutorial guides you through configuring SSH keys, using GitHub as an example.\n\n\nAn SSH (Secure Shell) is a network protocol that allows you to execute commands on a remote server securely. SSH keys are cryptographic keys used for authenticating a user or device within the SSH protocol. They come in pairs: the “private” key and the “public” key.\n\nPublic Key: This key can be freely shared with anyone and is added to the ~/.ssh/authorized_keys file on the remote server you wish to access.\nPrivate Key: This key should remain on your local machine and be carefully protested, similar to a password. It should never be shared.\n\nThe holder of the private key can authenticate with any remote system that has the corresponding public key. For example, you can use a single public/private key pair to authenticate with the cluster at your research center and your GitHub account.\n\n\n\n\n\n\nHow SSH keys work\n\n\n\nThink of the public key as a complex number x, and the private key as its inverse 1/x. Encrypting a message m with your public key is like multiplying your message by x, making mx completely unreadable. Using the private key, you can decrypt m by multiplying mx by 1/x. This process is secure because you cannot derive 1/x from x. Therefore, you can safely distribute the public key to the machines you want to access while keeping the private key on your machine.\n\n\n\n\n\n\n\nOn your (Mac or Linux) workstation, create the public/private key pair. There are several types of keys and by default you will get an RSA pair. When prompted for a passphrase, just hit enter so that you won’t need a password when logging in.\nWhen prompted, press Enter to store the keys in .ssh/id_rsa (we recommend not to modify this). Press Enter twice again to skip setting a passphrase, which otherwise would require you to enter it each time you use git push or git pull. This process generates two files: the public key (~/.ssh/id_rsa.pub) and the private key (~/.ssh/id_rsa).\nssh-keygen\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/Users/abc123/.ssh/id_rsa): \nCreated directory '/Users/abc123/.ssh'.\nEnter passphrase (empty for no passphrase): \nEnter same passphrase again: \nYour identification has been saved in /Users/abc123/.ssh/id_rsa\nYour public key has been saved in /Users/abc123/.ssh/id_rsa.pub\nThe key fingerprint is:\nSHA256:w8JIQXafdq6scMiNZefNd8lVz9UmIzm0eiO+pvGOM6A abc123@c9b1d769db6e\nThe key's randomart image is:\n+---[RSA 3072]----+\n|   .+ .     .    |\n|   . o . . . o  .|\n|    .   + . = o =|\n|   . o o o . o =+|\n|    . = S + o   +|\n|   . *.= * o o o |\n|    =.o.= + . +  |\n|    Eo .o+.o .   |\n|      . o*+      |\n+----[SHA256]-----+\ncat `~/.ssh/id_rsa.pub\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCs3nqhRr7FyB8UKxqQ5mA3Gk60oty5lGdzcpVxjL08QmjLz15X6Odo2EotmJQuzCyYbzPw4Tq404N3NOmVqYywrn03i3kmCnZd5MYq8yi72Zsk9qFfgEt7pxpf5WJ+AWSDXwUkUQK2FFby2RWbi4gwTmRHfkB999j8F3F/d5EpB0llQfxmZWIIrX9ampgxfI8+yTIyeG8gpx5028h1oR5qukEFJk6nAoj5on+/vBqHdc0AVPd6jNoNCDZ6ur4xwUPam8iQ6A4NfbnCVUPa95Wqt+QWEF6Mn4UB1WIvly+blDzg5rVXvd9B685bZjQWfUsxSiRPkhtV6X2yImklt+KjV0ufB/Yl1O+x6S8r2+I41WSxwJNr5h5L588l3GH4ehT79uCtrxiu4zas+4s9OW02Ox3auCfLi1/jp/CJ5IsuLwM/jPBXrgzw+HREKM2gtm9d12gU2b9o9bmASZVhEKqeTb0aRPjcTYYdnjOWUl+pqkdVPyB7mJs8NAQemN/shvM= abc123@c9b1d769db6e\nThe public key can be safely displayed cat and shared with any achine where it will function as a personal lock that can be only opened with your private key.\n\n\n\n\n\n\nTip\n\n\n\nTip 1: If you provided a passphrase when generating your keys, you would need to enter this passphrase each time you use your private key. The passphrase adds an extra layer of security in case your private key is compromised. For simplicity, we not always recommend using a passphrase unless you are working in environments where people not part of the project might have access.\nTip 2: DSA keys should be avoided. An RSA key with at least 2048 bits should provide sufficient security. Finally, ecdsa and ed25519 keys are also fine (don’t worry about the number of bits for these keys).\n# Recommended\nssh-keygen -t rsa -b 4096\n# Alternative\nssh-keygen -t ed25519\n\n\nUsing an existing public/private key pair\nIf you already have an SSH key pair, you can use it without generating a new one. To check the details of your existing key, use the ssh-keygen command to display the key’s fingerprint.\nssh-keygen -l -f ~/.ssh/id_rsa\n2048 SHA256:7l0HauYJVRaQhuzmti8XEZImnRbzipu3NKGnE6tDFRk grg@t430s (RSA)\nAlternatively, check the content of the ssh directory: - Mac/Unix: ls -la ~/.ssh/ - Windows: ls ~/.ssh/\n\n\n\nFollow the steps below if you are using Mac or Linux.\n\n\n\n\n\n\nWarning\n\n\n\n\nWindows users: follow these instructions instead, the process is very similar.\nIf you are getting errors when running the commands below, please read carefully these guidelines as GitHUb provides instructions on what to do with common issues users run into for this specific step 2.\nGeneral GitHUbtroubleshooting\n\n\n\n# ONLY if you need to start the ssh agent. First, verify it is not already running: \n# pgrep -u $USER ssh-agent\n# If that command returns a process ID, your SSH agent is running! If not, run the command below: \n# eval \"$(ssh-agent -s)\"\n\n# Manually modify the `~/.ssh/config` file to automatically load keys into ssh-agent, like in the example below. For GitHub, the User is **git**, do not write your own. \n\nHost github.com\n  AddKeysToAgent yes\n  IdentityFile ~/.ssh/id_rsa\n\n# Note: if the file does not `touch ~/.ssh/config`to create the file. \n\n# Add the SSH private to the ssh-agent\nssh-add ~/.ssh/id_rsa\n\n# Optional, verify the keys are loaded\nssh-add -l\n\n\n\n\n\n\nOther tips\n\n\n\n\n~/.ssh/config: add several hosts (servers, GitLab, GitHUb, etc.).\n\nEdit the SSH config file Create and use a ~/.ssh/config file on your workstation. E.g., using a password-less ssh key for authentication and the following ~/.ssh/config, user abc123 can login with the ssh xyz command.\nHost xyz\n    Hostname ssh-myserver.dk\n    User abc123\n    ServerAliveInterval 60\n    ForwardX11 yes\n    ForwardX11Trusted yes\n\nUsing paraphrases: store paraphrases in your keychain if you are using one. Please, follow the instructions here.\n\n\n\nStep 3: Transferring the key\nA. Transfer the key to GitHub/GitLab\nCopy your SSH public key ~/.ssh/id_rsa.pub with your favorite editor, go to your GitHub profile (top left side) &gt; Settings &gt; Access &gt; SSH and GPG keys &gt; New/Add SSH key. You just need to fill up the title (e.g.: Personal laptop) and paste the copied key (e.g. ssh-rsa or ecdsa-xxx).\n# Mac/Linux: copy key \ncat ~/.ssh/id_rsa.pub | pbcopy\n# Windows\ncat ~/.ssh/id_rsa.pub | clip\n\n\n\n\n\n\nGitHub Guidelines Links\n\n\n\n\n\n\nGeneral\nLast step, key transfer\n\n\n\n\nB: Transfer the key to a remote server\nTransfer the public part of your key pair to the remote server. This can be done manually, but the easiest way is using ssh-copy-id:\nssh-copy-id -i ~/.ssh/id_rsa.pub &lt;userid&gt;@ssh-myserver.dk\nThis will prompt for a password to authenticate with the remote server in order to copy the file. Once successful though, future logins will use the key pair for authentication, and will not prompt for a password (assuming you login from your workstation, which has the private part of your key pair).\n\n\n\n\n\n\nNote\n\n\n\nssh-copy-id will blindly append the given key file to the remote ~/.ssh/authorized_keys file. If you made a mistake or copied the same key multiple times, you may want to edit this file! Each line in the file corresponds to one key.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nYou are set to start using your SSH key. If you are using UCloud, follow the next section; otherwise, you are done with this tutorial.\n\n\n\n\n\n\nSimilarly to what is explained above, copy your key (manually or using e.g. pbcopy ~/.ssh/id_rsa.pub) and go to Resources (navigation panel on the left) &gt; SSH keys.\n\nPaste the public key here and give a title to your key as in the image below:\n\nWhen you have added the public part of the SSH key pair to your UCloud account, you can access jobs from your local computer if SSH access is possible for the given app and enabled upon job submission. Open Visual Studio Code to test this yourself! You will need to click on “Enable SSH Server” when submitting the job as follows:\n\n\n\nSelect “Enable SSH server”\n\n\nWhen a job with SSH access enabled starts, the command for accessing the job from your local computer via SSH is displayed from the job’s progress view.\n\n\n\nCopy ssh command\n\n\nNow, open a terminal and run the command:\n# the 4 digits will change everytime you start a new job\nssh ucloud@ssh.cloud.sdu.dk -p 2465\nIf it is the first time you do this, this message will prompt, write yes\nThe authenticity of host '[ssh.cloud.sdu.dk]:2465 ([130.225.164.106]:2465)' can't be established.\nED25519 key fingerprint is SHA256:0Q5WMne+hzOwj5bEfssH/gQrxFDz2fvclCGsQbfLLb8.\nThis key is not known by any other names.\nAre you sure you want to continue connecting (yes/no/[fingerprint])?\nYour terminal will look like the image below which means you can starting interacting with the job from the console on your computer. \nThe connection can be closed using the command:\nexit",
    "crumbs": [
      "HPC launch",
      "Data management",
      "SSH keys"
    ]
  },
  {
    "objectID": "develop/ssh_keys.html#ssh-keys",
    "href": "develop/ssh_keys.html#ssh-keys",
    "title": "HPC Lab",
    "section": "",
    "text": "Using SSH keys for authentication is recommended for accessing servers and version control platforms such as GitLab, GitHub, or BitBucket. SSH keys provide a more convenient and secure method than passwords, allowing you to avoid typing a password every time you log in.\nThis tutorial guides you through configuring SSH keys, using GitHub as an example.\n\n\nAn SSH (Secure Shell) is a network protocol that allows you to execute commands on a remote server securely. SSH keys are cryptographic keys used for authenticating a user or device within the SSH protocol. They come in pairs: the “private” key and the “public” key.\n\nPublic Key: This key can be freely shared with anyone and is added to the ~/.ssh/authorized_keys file on the remote server you wish to access.\nPrivate Key: This key should remain on your local machine and be carefully protested, similar to a password. It should never be shared.\n\nThe holder of the private key can authenticate with any remote system that has the corresponding public key. For example, you can use a single public/private key pair to authenticate with the cluster at your research center and your GitHub account.\n\n\n\n\n\n\nHow SSH keys work\n\n\n\nThink of the public key as a complex number x, and the private key as its inverse 1/x. Encrypting a message m with your public key is like multiplying your message by x, making mx completely unreadable. Using the private key, you can decrypt m by multiplying mx by 1/x. This process is secure because you cannot derive 1/x from x. Therefore, you can safely distribute the public key to the machines you want to access while keeping the private key on your machine.\n\n\n\n\n\n\n\nOn your (Mac or Linux) workstation, create the public/private key pair. There are several types of keys and by default you will get an RSA pair. When prompted for a passphrase, just hit enter so that you won’t need a password when logging in.\nWhen prompted, press Enter to store the keys in .ssh/id_rsa (we recommend not to modify this). Press Enter twice again to skip setting a passphrase, which otherwise would require you to enter it each time you use git push or git pull. This process generates two files: the public key (~/.ssh/id_rsa.pub) and the private key (~/.ssh/id_rsa).\nssh-keygen\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/Users/abc123/.ssh/id_rsa): \nCreated directory '/Users/abc123/.ssh'.\nEnter passphrase (empty for no passphrase): \nEnter same passphrase again: \nYour identification has been saved in /Users/abc123/.ssh/id_rsa\nYour public key has been saved in /Users/abc123/.ssh/id_rsa.pub\nThe key fingerprint is:\nSHA256:w8JIQXafdq6scMiNZefNd8lVz9UmIzm0eiO+pvGOM6A abc123@c9b1d769db6e\nThe key's randomart image is:\n+---[RSA 3072]----+\n|   .+ .     .    |\n|   . o . . . o  .|\n|    .   + . = o =|\n|   . o o o . o =+|\n|    . = S + o   +|\n|   . *.= * o o o |\n|    =.o.= + . +  |\n|    Eo .o+.o .   |\n|      . o*+      |\n+----[SHA256]-----+\ncat `~/.ssh/id_rsa.pub\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCs3nqhRr7FyB8UKxqQ5mA3Gk60oty5lGdzcpVxjL08QmjLz15X6Odo2EotmJQuzCyYbzPw4Tq404N3NOmVqYywrn03i3kmCnZd5MYq8yi72Zsk9qFfgEt7pxpf5WJ+AWSDXwUkUQK2FFby2RWbi4gwTmRHfkB999j8F3F/d5EpB0llQfxmZWIIrX9ampgxfI8+yTIyeG8gpx5028h1oR5qukEFJk6nAoj5on+/vBqHdc0AVPd6jNoNCDZ6ur4xwUPam8iQ6A4NfbnCVUPa95Wqt+QWEF6Mn4UB1WIvly+blDzg5rVXvd9B685bZjQWfUsxSiRPkhtV6X2yImklt+KjV0ufB/Yl1O+x6S8r2+I41WSxwJNr5h5L588l3GH4ehT79uCtrxiu4zas+4s9OW02Ox3auCfLi1/jp/CJ5IsuLwM/jPBXrgzw+HREKM2gtm9d12gU2b9o9bmASZVhEKqeTb0aRPjcTYYdnjOWUl+pqkdVPyB7mJs8NAQemN/shvM= abc123@c9b1d769db6e\nThe public key can be safely displayed cat and shared with any achine where it will function as a personal lock that can be only opened with your private key.\n\n\n\n\n\n\nTip\n\n\n\nTip 1: If you provided a passphrase when generating your keys, you would need to enter this passphrase each time you use your private key. The passphrase adds an extra layer of security in case your private key is compromised. For simplicity, we not always recommend using a passphrase unless you are working in environments where people not part of the project might have access.\nTip 2: DSA keys should be avoided. An RSA key with at least 2048 bits should provide sufficient security. Finally, ecdsa and ed25519 keys are also fine (don’t worry about the number of bits for these keys).\n# Recommended\nssh-keygen -t rsa -b 4096\n# Alternative\nssh-keygen -t ed25519\n\n\nUsing an existing public/private key pair\nIf you already have an SSH key pair, you can use it without generating a new one. To check the details of your existing key, use the ssh-keygen command to display the key’s fingerprint.\nssh-keygen -l -f ~/.ssh/id_rsa\n2048 SHA256:7l0HauYJVRaQhuzmti8XEZImnRbzipu3NKGnE6tDFRk grg@t430s (RSA)\nAlternatively, check the content of the ssh directory: - Mac/Unix: ls -la ~/.ssh/ - Windows: ls ~/.ssh/\n\n\n\nFollow the steps below if you are using Mac or Linux.\n\n\n\n\n\n\nWarning\n\n\n\n\nWindows users: follow these instructions instead, the process is very similar.\nIf you are getting errors when running the commands below, please read carefully these guidelines as GitHUb provides instructions on what to do with common issues users run into for this specific step 2.\nGeneral GitHUbtroubleshooting\n\n\n\n# ONLY if you need to start the ssh agent. First, verify it is not already running: \n# pgrep -u $USER ssh-agent\n# If that command returns a process ID, your SSH agent is running! If not, run the command below: \n# eval \"$(ssh-agent -s)\"\n\n# Manually modify the `~/.ssh/config` file to automatically load keys into ssh-agent, like in the example below. For GitHub, the User is **git**, do not write your own. \n\nHost github.com\n  AddKeysToAgent yes\n  IdentityFile ~/.ssh/id_rsa\n\n# Note: if the file does not `touch ~/.ssh/config`to create the file. \n\n# Add the SSH private to the ssh-agent\nssh-add ~/.ssh/id_rsa\n\n# Optional, verify the keys are loaded\nssh-add -l\n\n\n\n\n\n\nOther tips\n\n\n\n\n~/.ssh/config: add several hosts (servers, GitLab, GitHUb, etc.).\n\nEdit the SSH config file Create and use a ~/.ssh/config file on your workstation. E.g., using a password-less ssh key for authentication and the following ~/.ssh/config, user abc123 can login with the ssh xyz command.\nHost xyz\n    Hostname ssh-myserver.dk\n    User abc123\n    ServerAliveInterval 60\n    ForwardX11 yes\n    ForwardX11Trusted yes\n\nUsing paraphrases: store paraphrases in your keychain if you are using one. Please, follow the instructions here.\n\n\n\nStep 3: Transferring the key\nA. Transfer the key to GitHub/GitLab\nCopy your SSH public key ~/.ssh/id_rsa.pub with your favorite editor, go to your GitHub profile (top left side) &gt; Settings &gt; Access &gt; SSH and GPG keys &gt; New/Add SSH key. You just need to fill up the title (e.g.: Personal laptop) and paste the copied key (e.g. ssh-rsa or ecdsa-xxx).\n# Mac/Linux: copy key \ncat ~/.ssh/id_rsa.pub | pbcopy\n# Windows\ncat ~/.ssh/id_rsa.pub | clip\n\n\n\n\n\n\nGitHub Guidelines Links\n\n\n\n\n\n\nGeneral\nLast step, key transfer\n\n\n\n\nB: Transfer the key to a remote server\nTransfer the public part of your key pair to the remote server. This can be done manually, but the easiest way is using ssh-copy-id:\nssh-copy-id -i ~/.ssh/id_rsa.pub &lt;userid&gt;@ssh-myserver.dk\nThis will prompt for a password to authenticate with the remote server in order to copy the file. Once successful though, future logins will use the key pair for authentication, and will not prompt for a password (assuming you login from your workstation, which has the private part of your key pair).\n\n\n\n\n\n\nNote\n\n\n\nssh-copy-id will blindly append the given key file to the remote ~/.ssh/authorized_keys file. If you made a mistake or copied the same key multiple times, you may want to edit this file! Each line in the file corresponds to one key.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nYou are set to start using your SSH key. If you are using UCloud, follow the next section; otherwise, you are done with this tutorial.\n\n\n\n\n\n\nSimilarly to what is explained above, copy your key (manually or using e.g. pbcopy ~/.ssh/id_rsa.pub) and go to Resources (navigation panel on the left) &gt; SSH keys.\n\nPaste the public key here and give a title to your key as in the image below:\n\nWhen you have added the public part of the SSH key pair to your UCloud account, you can access jobs from your local computer if SSH access is possible for the given app and enabled upon job submission. Open Visual Studio Code to test this yourself! You will need to click on “Enable SSH Server” when submitting the job as follows:\n\n\n\nSelect “Enable SSH server”\n\n\nWhen a job with SSH access enabled starts, the command for accessing the job from your local computer via SSH is displayed from the job’s progress view.\n\n\n\nCopy ssh command\n\n\nNow, open a terminal and run the command:\n# the 4 digits will change everytime you start a new job\nssh ucloud@ssh.cloud.sdu.dk -p 2465\nIf it is the first time you do this, this message will prompt, write yes\nThe authenticity of host '[ssh.cloud.sdu.dk]:2465 ([130.225.164.106]:2465)' can't be established.\nED25519 key fingerprint is SHA256:0Q5WMne+hzOwj5bEfssH/gQrxFDz2fvclCGsQbfLLb8.\nThis key is not known by any other names.\nAre you sure you want to continue connecting (yes/no/[fingerprint])?\nYour terminal will look like the image below which means you can starting interacting with the job from the console on your computer. \nThe connection can be closed using the command:\nexit",
    "crumbs": [
      "HPC launch",
      "Data management",
      "SSH keys"
    ]
  },
  {
    "objectID": "develop/cheat_sheet.html",
    "href": "develop/cheat_sheet.html",
    "title": "Cheat sheet",
    "section": "",
    "text": "Collection of useful commands for package and environment management.",
    "crumbs": [
      "HPC pipes",
      "Computational environments",
      "Cheat sheet"
    ]
  },
  {
    "objectID": "develop/cheat_sheet.html#environment-descriptors",
    "href": "develop/cheat_sheet.html#environment-descriptors",
    "title": "Cheat sheet",
    "section": "Environment descriptors",
    "text": "Environment descriptors\n\nGit: git log -1 and git status -u. In python, use the following command for a specific module version: .version.git_revision) or .__git_version__.\nR: sessionInfo() or devtools::session_info(): In R, these functions provide detailed information about the current session and list all dependencies.\nPython: the best way seems to import sys, and then, __version__, __file__ which will display package versions and their location without having to list all packages using pip. It’s essential to load the package first and then use the following code to print all its dependencies:",
    "crumbs": [
      "HPC pipes",
      "Computational environments",
      "Cheat sheet"
    ]
  },
  {
    "objectID": "develop/cheat_sheet.html#shell",
    "href": "develop/cheat_sheet.html#shell",
    "title": "Cheat sheet",
    "section": "SHELL",
    "text": "SHELL\n# finds the location of a command by searching through the PATH environment variable\nwhich &lt;program&gt;\n\n# lists all occurrences of a command found in the PATH\nwhich -a &lt;program&gt; \n\n# shows the shared libraries required by a specific program\nldd &lt;program&gt; \n\n# provides detailed system information, including machine name and kernel version\nuname -a \n\n# displays operating system identification data (such as Debian, Ubuntu, etc.)\ncat /etc/os-release \n\n# provides operating system identification data and is recommended if available. This command might not be installed by default but is part of the lsb-release package on Debian-based systems\nlsb_release -a \n\n# Environmental variables for locations \n$HOME # home directory\n$PYTHONPATH # empty by default\n$PYTHONHOME # python libraries\n$RHOME # R libraries\n$LD_LIBRARY_PATH # dynamic loader for shared libraries when running a program\nUnderstanding PYTHONPATH\n\nPYTHONPATH: you can set the $PYTHONPATH environment variable to include additional directories where Python will look for modules and packages. This allows you to extend the search path beyond the default locations and load packages that has been installed in a different directory. It is highly discouraged to mix different versions of libraries and interpreters! as some libraries are complex packages with dependencies.\n\nexport PYTHONPATH=/path/to/packages/\n# unset the variable \nunset PYTHONPATH",
    "crumbs": [
      "HPC pipes",
      "Computational environments",
      "Cheat sheet"
    ]
  },
  {
    "objectID": "develop/cheat_sheet.html#pip",
    "href": "develop/cheat_sheet.html#pip",
    "title": "Cheat sheet",
    "section": "PIP",
    "text": "PIP\npip3 show &lt;module&gt;\npip3 install &lt;module&gt;      # install the latest version (and dependencies)\npip3 uninstall &lt;module&gt;    # remove\npip3 freeze                # output installed packages in requirements.txt format (similar to pip3 list) which can conveniently be used with: pip3 install -r requirements.txt\nWe highly recommend to avoid using pip and start using Python virtualenv management tools, pipenv.\n\nAdvantages: it will generates and checks file hashes for locked dependencies when installing from Pipfile.lock and it creates a virtualenv in a standard customizable location.",
    "crumbs": [
      "HPC pipes",
      "Computational environments",
      "Cheat sheet"
    ]
  },
  {
    "objectID": "develop/cheat_sheet.html#python",
    "href": "develop/cheat_sheet.html#python",
    "title": "Cheat sheet",
    "section": "PYTHON",
    "text": "PYTHON\n# version info on installed packages\n\ndef print_imported_modules():\n  import sys\n  for name, val in sorted(sys.modules.items()):\n      if(hasattr(val, '__version__')): \n          print(val.__name__, val.__version__)\n      else:\n          print(val.__name__, \"(unknown version)\")\n\nprint(\"==== Package list after loading pandas ====\");\nimport &lt;module&gt;\nprint_imported_modules()",
    "crumbs": [
      "HPC pipes",
      "Computational environments",
      "Cheat sheet"
    ]
  },
  {
    "objectID": "develop/cheat_sheet.html#r",
    "href": "develop/cheat_sheet.html#r",
    "title": "Cheat sheet",
    "section": "R",
    "text": "R\ninstall.packages()     # install a given package\nlibrary()              # loads a given package\nremove.packages()      # install a given package\nWe highly recommend using renv.",
    "crumbs": [
      "HPC pipes",
      "Computational environments",
      "Cheat sheet"
    ]
  },
  {
    "objectID": "develop/cheat_sheet.html#conda",
    "href": "develop/cheat_sheet.html#conda",
    "title": "Cheat sheet",
    "section": "CONDA",
    "text": "CONDA\nconda -V\nconda list \nconda env list",
    "crumbs": [
      "HPC pipes",
      "Computational environments",
      "Cheat sheet"
    ]
  },
  {
    "objectID": "develop/cheat_sheet.html#dpkg",
    "href": "develop/cheat_sheet.html#dpkg",
    "title": "Cheat sheet",
    "section": "DPKG",
    "text": "DPKG\nLow-level tool for package management on Debian-based systems\ndpkg -S package_name # Seach   \ndpkg -I package.deb  # --info\ndpkg -L package_name # --list files installed by a package\ndpkg -i package.deb  # --install   [requires sudo]\ndpkg -r   # --remove:  Remove debian_package        [requires sudo]\ndpkg --get-selections    # List all the packages known by dpkg and whether they are installed or not\ndpkg --set-selections    # Set which package should be installed     [requires sudo]\ndpkg-query -W -f='${Package} == ${Version}\\n' # package version",
    "crumbs": [
      "HPC pipes",
      "Computational environments",
      "Cheat sheet"
    ]
  },
  {
    "objectID": "develop/cheat_sheet.html#apt",
    "href": "develop/cheat_sheet.html#apt",
    "title": "Cheat sheet",
    "section": "APT",
    "text": "APT\nHigh-level tool for package management on Debian-based systems. It also handles package dependencies and repositories.\napt\napt update                      # Update the package list                  [sudo]\napt search &lt;package_name&gt;\napt show &lt;package_name&gt;\napt install &lt;package_name&gt;      # Install a debian package                 [sudo]\napt upgrade                     # Upgrade all your packages                [sudo]\napt clean                       # Delete all the .deb you've downloaded (save some space)\napt remove --purge &lt;package_name&gt; # Remove a debian package                  [sudo]\napt-cache search keyword # Search for packages containing a keyword.",
    "crumbs": [
      "HPC pipes",
      "Computational environments",
      "Cheat sheet"
    ]
  },
  {
    "objectID": "develop/singularity.html",
    "href": "develop/singularity.html",
    "title": "Singularity",
    "section": "",
    "text": "Requirements\n\n\n\nInstall singularity\n\n\nThe equivalent to a Dockerfile for singularity is Singularity definition file where the instructions for the image are specified.\n\n\n\nCopyrightCC-BY-SA 4.0 license"
  },
  {
    "objectID": "develop/exercise_pipes.html",
    "href": "develop/exercise_pipes.html",
    "title": "Exercises",
    "section": "",
    "text": "Put your learning to the test with what you’ve covered so far.",
    "crumbs": [
      "HPC pipes",
      "Pipeline languages",
      "Exercises"
    ]
  },
  {
    "objectID": "develop/exercise_pipes.html#general-hpc-pipes",
    "href": "develop/exercise_pipes.html#general-hpc-pipes",
    "title": "Exercises",
    "section": "General HPC pipes",
    "text": "General HPC pipes\n1. What role does a workflow manager play in computational research??\n\n Automating the execution of complex computational processes To manually execute each step of a computation Offering real-time and intuitive data analysis To minimize the reliance on specific software environments\n\n2.What is the primary drawback of using shell scripts for automating computations?\n\n Limited error handling and debugging capabilities Difficulty in integrating with notebooks Not compatible with all operating systems They re-run all the steps every time Insufficient support for parallel processing Complex and extensive coding for simple tasks\n\n3. What are the key features of workflow manager in computational research? (Several possible solutions)\n\n Executing tasks only when required Managing task dependencies Overseeing storage and resource allocation Providing intuitive graphical interfaces\n\n4. Workflow managers can run tasks (different) concurrently if there are no dependencies (True or False) TRUEFALSE\n5. A workflow manager can execute a single parallelized task on multiple nodes in a computing cluster (True or False) TRUEFALSE",
    "crumbs": [
      "HPC pipes",
      "Pipeline languages",
      "Exercises"
    ]
  },
  {
    "objectID": "develop/exercise_pipes.html#snakemake",
    "href": "develop/exercise_pipes.html#snakemake",
    "title": "Exercises",
    "section": "Snakemake",
    "text": "Snakemake\n\n\n\n\n\n\nExercise 1S: Exploring Rule Invocation in Snakemake\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this exercise, we will explore how rules are invoked in a Snakemake workflow. Download the Snakefile and data required for this exercise using the links below.\n Download data input   Download Snakefile \nNow follow these steps and answer the questions:\n\nOpen the snakefile, named process_1kgp.smk and try to understand every single line. If you request Snakemake to generate the file results/all_female.txt, what commands will be executed and in what sequence?\nDry run the workflow: Check the number of jobs that will be executed.\n6. How many jobs will Snakemake run? \nRun the workflow: Use the name flag --snakefile | -s follow by the name of the file.\nVerify output: Ensure that the output files are in your working directory.\nClean Up: remove all files starting with EUR in your results folder.\nRerun the workflow: Execute the Snakefile again.\n7. How many jobs did Snakemake run in this last execution? \nRemove lines 4-6 in the process_1kgp.smk. How else can you run the workflow but to generate instead all_male.txt using only the command-line?\nrule all:\n   input:\n      expand(\"results/all_{gender}.txt\", gender=[\"female\"])\n8. Tip: what is missing at the end of the command ( e.g. what should be added to ensure all_male.txt is generated)? snakemake -s process_1kgp.smk -c1 \n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\n# dry run \nsnakemake -s process_1kgp.smk -n \n# run the workflow \nsnakemake -s process_1kgp.smk-c1 &lt;name_rule|name_output&gt;\n# verify output \nls &lt;name_output&gt;\n# remove file belonging to european individuals \nrm results/EUR.tsv results/all_female.txt\n# rerun again \nsnakemake -s process_1kgp.smk -c1 &lt;name_rule|name_output&gt;",
    "crumbs": [
      "HPC pipes",
      "Pipeline languages",
      "Exercises"
    ]
  },
  {
    "objectID": "develop/data_transfer.html",
    "href": "develop/data_transfer.html",
    "title": "HPC Lab",
    "section": "",
    "text": "To ensure efficient file transfers on HPC systems, we will go through:\n\nOverview rsync and scp\nBasic commands\n\nWhen transferring files between servers, it’s important to ensure that the files are consistent. Differences in file size can occur due to variations in filesystems, but large differences might indicate an issue. Manually checking file sizes in a terminal (ls -lh or du -sh) to determine if a transfer was successful might not be ideal, as it doesn’t guarantee file integrity. Using checksums provides a more reliable verification method.\nYou can use md5sum to verify that the file contents are identical on both servers. Run the following command on each server:\nmd5sum /path/to/file\nUsing rsync for Efficient File Transfers rsync is a powerful alternative to scp for transferring files. It only sends data if the file has changed, making it more efficient.\n# Transferring files between local machine-server\nrsync -avz local/path/to/file user@server:/remote/path/to/file\n# Transferring files between servers\nrsync -azv server1:/path/to/my_folder server2:/path/to/destination_folder\n\n\n\n\n\n\nNote\n\n\n\nTo transfer files directly between two servers from your local workstation, ensure your SSH setup (configuration, keys, etc.) allows access to both servers. Check this section if you need help setting up your keys (generating, configuring and managing).\n\n\n\n\nrsync is an efficient protocol to compare and copy files between directories or server. It can resume interrupted transfers and compress files on the fly.\n\nChecksum Verification: rsync checks the hashsums of files and only transfers data if the hashes differ. This ensures that only the changed parts of the files are sent (so you can rsync a whole folder, and only the changes files will be send).\nTimestamp Preservation: Using the -a flag with rsync preserves the modified timestamps of files, which is particularly useful for tools like Snakemake.\n\n\n\n\n\n-a: Archive mode, preserves file attributes like timestamps and permissions (important if you are using snakemake).\n-v: Verbose mode, provides detailed information during transfer.\n-z: Compresses data during transfer, reducing the amount of data sent over the network.\n-c: Enables checksum checking, ensuring that files are identical by comparing their contents rather than just their size and modification time.\n\n\n\n\nFor users who prefer a graphical interface, tools like Cyberduck and FileZilla can also be used for transferring files between servers.",
    "crumbs": [
      "HPC launch",
      "Data management",
      "Data transfer"
    ]
  },
  {
    "objectID": "develop/data_transfer.html#data-transfer",
    "href": "develop/data_transfer.html#data-transfer",
    "title": "HPC Lab",
    "section": "",
    "text": "To ensure efficient file transfers on HPC systems, we will go through:\n\nOverview rsync and scp\nBasic commands\n\nWhen transferring files between servers, it’s important to ensure that the files are consistent. Differences in file size can occur due to variations in filesystems, but large differences might indicate an issue. Manually checking file sizes in a terminal (ls -lh or du -sh) to determine if a transfer was successful might not be ideal, as it doesn’t guarantee file integrity. Using checksums provides a more reliable verification method.\nYou can use md5sum to verify that the file contents are identical on both servers. Run the following command on each server:\nmd5sum /path/to/file\nUsing rsync for Efficient File Transfers rsync is a powerful alternative to scp for transferring files. It only sends data if the file has changed, making it more efficient.\n# Transferring files between local machine-server\nrsync -avz local/path/to/file user@server:/remote/path/to/file\n# Transferring files between servers\nrsync -azv server1:/path/to/my_folder server2:/path/to/destination_folder\n\n\n\n\n\n\nNote\n\n\n\nTo transfer files directly between two servers from your local workstation, ensure your SSH setup (configuration, keys, etc.) allows access to both servers. Check this section if you need help setting up your keys (generating, configuring and managing).\n\n\n\n\nrsync is an efficient protocol to compare and copy files between directories or server. It can resume interrupted transfers and compress files on the fly.\n\nChecksum Verification: rsync checks the hashsums of files and only transfers data if the hashes differ. This ensures that only the changed parts of the files are sent (so you can rsync a whole folder, and only the changes files will be send).\nTimestamp Preservation: Using the -a flag with rsync preserves the modified timestamps of files, which is particularly useful for tools like Snakemake.\n\n\n\n\n\n-a: Archive mode, preserves file attributes like timestamps and permissions (important if you are using snakemake).\n-v: Verbose mode, provides detailed information during transfer.\n-z: Compresses data during transfer, reducing the amount of data sent over the network.\n-c: Enables checksum checking, ensuring that files are identical by comparing their contents rather than just their size and modification time.\n\n\n\n\nFor users who prefer a graphical interface, tools like Cyberduck and FileZilla can also be used for transferring files between servers.",
    "crumbs": [
      "HPC launch",
      "Data management",
      "Data transfer"
    ]
  },
  {
    "objectID": "develop/docker.html",
    "href": "develop/docker.html",
    "title": "Docker",
    "section": "",
    "text": "Requirements\n\n\n\nRegister on Docker Hub at docker.com and follow the Docker Desktop guide to get started. We recommend going through this tutorial before diving in.\nDocker helps developers build, share, run, and verify applications anywhere — without tedious environment configuration or management. Before diving into hands-on activities, make sure these three main concepts are clear:",
    "crumbs": [
      "HPC pipes",
      "Computational environments",
      "Docker"
    ]
  },
  {
    "objectID": "develop/docker.html#using-docker-images",
    "href": "develop/docker.html#using-docker-images",
    "title": "Docker",
    "section": "Using Docker images",
    "text": "Using Docker images\nFirst, we’ll explore some basic commands using existing Docker images available on Docker Hub. This will help you become familiar with Docker’s functionality before we dive into creating our own custom images. People commonly use shorthand Docker commands like docker pull and docker run, though the full versions (e.g., docker image pull, docker container run) can clarify the underlying concepts and their specific functions.\nDocker Hub\n\nSearch for images docker search &lt;name&gt; (e.g. docker search debian)\nDownload an image docker pull &lt;image-name&gt;\n\nLocal Docker Daemon\n\nDisplay all docker images currently stored on your local Docker daemon docker images (alias for docker image ls)\nInspect docker image docker inspect &lt;image_name&gt; (alias for docker image inspect)\nRun a command (cmd) in a container docker run &lt;image_name&gt; cmd (alias for docker container run  &lt;image_name&gt; cmd)\nStart an interactive bash shell docker run -it &lt;image_name&gt; bash. Add other flags like:\n\n-p 8888:8888 to access your interactive shell through ‘localhost:8888’ on your host.\n-rm to automatically delete the container once it stops, keeping your system clean (including its filesystem changes, logs and metadata). If you don’t run this flag, a container will automatically be created and information about tje processes will be kept. Check all containers in your Docker daemon docker container ls -a\n--user=$(id -u):$(id -g) useful if you are using sharing volumes and need appropriate permissions on the host to manipulate files.\n\nShare the current directory with the container docker run -it --volume=$(pwd):/directory/in/container image_name bash, the output of pwd will be mounted to the /directory/in/container (e.g. data, shared, etc.)\nManage your containers using pause or stop\n\nTag images with a new name docker image tag image_name:tag new_name:tag\ndocker logs &lt;container_id&gt;\nRemove images and clean up your hard drive docker rmi &lt;image_name&gt;\nRemove containers docker container rm &lt;container_name&gt;. Alternatively, remove all dead containers: docker container prune\n\nAll Docker containers have a digest which is thesha256 hash of the image. It allows to uniquely identify a docker image and it is great for reproducibility.\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nWe use a Debian stable image (sha256-540ebf19fb0bbc243e1314edac26b9fe7445e9c203357f27968711a45ea9f1d4) as an example for pulling and inspecting because it provides a reliable, minimal base with essential tools. This image includes fundamental utilities like bash for shell scripting and apt for package management. It can be an ideal starting point for developing and testing pipelines or configuring server environments, offering a stable and consistent foundation that can be customized with additional packages as needed.\n1. Get a container image\n\nPull docker image (using tag stable otherwise, latest will be pulled by default)\ndocker pull debian:stable@sha256:2171c87301e5ea528cee5c2308a4589f8f2ac819d4d434b679b7cb1dc8de6912\n2. Run commands in a container\nList the content of the container\ndocker run -it -rm debian@sha256:2171c87301e5ea528cee5c2308a4589f8f2ac819d4d434b679b7cb1dc8de6912 ls\nCheck is there is python or perl in this container:\ndocker run -rm debian@sha256:2171c87301e5ea528cee5c2308a4589f8f2ac819d4d434b679b7cb1dc8de6912 which -a python perl\n\n3. Use docker interactively\n\nEnter the container interactively with a Bash shell\ndocker run -it -rm debian@sha256:2171c87301e5ea528cee5c2308a4589f8f2ac819d4d434b679b7cb1dc8de6912 /bin/bash\nNow, collect info about the packages installed in the environment.\n\n\nInteractive Docker container\n\nhostname\nwhoami\nls -la ~/\npython  \necho \"Hello world!\" &gt; ~/myfile.txt\nls -la ~/\n\n4. Exit and check the container\nExit the container\nexit\n\nNow, rerun commands from step 3 under “Interactive Docker container”. Does the output look any different?\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nYou will notice that you are the root but the name of the machine has now changed and the file that you had created has disappeared.\n\n\n\n\n\n5. Inspect the docker image\ndocker image inspect debian:stable@sha256:2171c87301e5ea528cee5c2308a4589f8f2ac819d4d434b679b7cb1dc8de6912\n6. Inspect Docker Image Details Identify the date of creation, the name of the field with the digest of the image and command run by default when entering this container.\n7. Remove container\ndocker image docker rmi debian:stable@sha256:2171c87301e5ea528cee5c2308a4589f8f2ac819d4d434b679b7cb1dc8de6912\n\n\n\n\n\nWhen you exit a container, any changes made are lost because each new container starts in a clean state. Containers are isolated environments that don’t retain memory of past interactions, allowing you to experiment without affecting your system. This immutability ensures that your program will run in the same conditions in the future, even months later. So, how can you retrieve the results of computations performed inside a container?\nImportantly, when running the Docker container using the command for mounting, it is recommended to execute it from the temporary directory (e.g. /tmp/ ) or a project-specific directory rather than your home directory. This way you would keep a clean and isolated environment within the container.\n\nNamed volumes\n# -v: mounting only your project-specific dir \ndocker run -it --volumes &lt;project_dir&gt;\n# Named volumes: are managed by Docker and are isolated from your host file system\ndocker volume create &lt;my_project_volume&gt;\ndocker run -it --volumes &lt;my_project_volume&gt;\nNow, we’re using named volumes or shared directories. When you make changes in these directories within the container, the updates are instantly reflected in the corresponding directory on the host machine. This allows seamless synchronization between the container and the host file system.\n\n\n\n\n\n\nWhy not mounting your home directory\n\n\n\nThis is because the command mounts the current working directory (${PWD}) into the container at /home/rstudio. If you run this command from your home directory, any local packages or configurations (such as R or Python packages installed with install.packages or pip) would be accessible inside the container. This could compromise the isolation benefits of using the container, as it would inadvertently include your personal setup and packages.\n\n\nIt is important to store container images in a shared storage area (e.g., with git or git annex).\n\n\nTransfer and backup Docker images\nSaving a Docker image as a tar file is useful because it enables you to transfer the image to another system or operating system without needing access to the original Docker registry. The tar file contains several key components:\n\nmetadata: JSON files essential to reconstruct the image\n\nlayer information with each layer associated with metadata that includes which commands are used to create the later.\n\n\nDockerfile\n\nFROM ubuntu:20.04 # Layer 1 - base image \nRUN apt-get update && apt-get install -y python3 # Layer 2 - the result of running the command \nCOPY . /app # Layer 3 - add application files to the images\n\ntags pointers to specific image digests or versions.\nhistory of the image and instructions from the DOckerfile that were used to build the image.\nmanifest which ties together the layers and the overall image structure.\n\nFilesystem: the actual content, files and directories that make up the image.\n\n# save to tar file \ndocker image save --output=image.tar image_name \n# load tar file\ndocker image load --input=image.tar  \nIn some cases, you might only need to access the content of the filesystem of a Docker image (debugging, backup, repurposing. In such cases, you will need to create the container and export the root’s filesystem to a tar file. Similarly, you can create a new image from the tar file.\ndocker container create --name=temp_container image_name\ndocker container export --output=image.tar temp_container\ndocker container rm temp_container\n# new image importing tar file \ndocker image import --input image.tar image_name",
    "crumbs": [
      "HPC pipes",
      "Computational environments",
      "Docker"
    ]
  },
  {
    "objectID": "develop/docker.html#building-docker-images",
    "href": "develop/docker.html#building-docker-images",
    "title": "Docker",
    "section": "Building Docker images",
    "text": "Building Docker images\nWe’re now ready to build custom Docker images. We’ll start with a minimal pre-built image, add the necessary software, and then upload the final image to Docker Hub.\nA common practice is to start with a minimal or pre-configured image from Docker Hub. You then use a Dockerfile to modify this base image, specifying instructions for software installation and configuration. he Dockerfile acts as a recipe, providing a list of commands to build the image as needed.\n\n\nDockerfile\n\n# deploy docker container\nFROM node\n\n# Info and rights of the app\nLABEL software=\"App_name - sandbox\" \\\n    maintainer=\"&lt;author.address@sund.ku.dk&gt;\"\n\n# root: needed to install and modify the container \nUSER 0 \n\n# run bash commands (eg. installing packages or softwares)\nRUN mkdir -p\n\n# \nRUN apt update \\\n    && apt install -y jupyter-notebook python3-matplotlib python3-pandas python3-numpy  \\\n    && rm -fr node_modules # removes dependencies\n\n# set working directory (this directory should act as the main application directory)\nWORKDIR /app\n\n# copy files to-from directories\nCOPY /from/path/... /to/path/...\n\n# set environment variables for Conda\nENV\n\n# change up user permissions \nUSER 11042\n\nIn this example, as indicated with the RUN command, it will update the package list and install jupyter and everything else required. It is a common practice to remove dependencies with the command in the example above.\n\n\n\n\n\n\nLabel\n\n\n\nKey-value pair syntax, where should add at least these labels to the container:\n\nsoftware = name of the app\nauthor = maintainer or author of the app\nversion = version of the app\nlicense = app licence, e.g., “MIT”\ndescription = very short description of the app\n\n\n\nAfter preparing your Dockerfile, run docker build to create a new image based on these instructions. Docker’s isolation ensures that any changes made within a container are lost when you exit. Nevertheless, you can use docker commit to save these changes by creating a new image from the updated container. This new image can then be used to create containers with your modifications.\ndocker build -t \n-p: port to use, usually 8787:8787 works for R studio - R studio: 8787:8787 - JupyterLab: 8888:8888",
    "crumbs": [
      "HPC pipes",
      "Computational environments",
      "Docker"
    ]
  }
]
---
title: Day 1 - Part 2 
format: 
  html: default
summary: intro to workflows
css: ./web.css
---

```{r, echo = FALSE, results='asis'}
library(webexercises)

knitr::opts_chunk$set(echo = FALSE)

# Uncomment to change widget colours:
style_widgets(incorrect = "red", correct = "green", highlight = "firebrick")

```

Put your learning to the test with what you’ve covered so far.

## A. General knowledge

::: {.webex-check .callout-exercise}
# I - General knowledge 

```{r, results='asis', echo = FALSE}

opts_1 <- c(
   answer="Automating the execution of complex computational processes",
   "To manually execute each step of a computation",
   "Offering real-time and intuitive data analysis",
   "To minimize the reliance on specific software environments"
)

cat("**G.1. What role does a workflow manager play in computational research?**", longmcq(opts_1))
```


```{r, results='asis', echo = FALSE}

opts_2 <- c(
   answer="Limited error handling and debugging capabilities",
   "Difficulty in integrating with notebooks",
   "Not compatible with all operating systems",
   answer="They re-run all the steps every time",
   "Insufficient support for parallel processing",
   "Complex and extensive coding for simple tasks"
)

cat("**G.2.What is the primary drawback of using shell scripts for automating computations?**", longmcq(opts_2))

```

```{r, results='asis', echo = FALSE}
#| echo: false
opts_3 <- c(
   answer="Executing tasks only when required",
   answer = "Managing task dependencies",
   answer = "Overseeing storage and resource allocation",
   "Providing intuitive graphical interfaces"
)

cat("**G.3. What are the key features of workflow manager in computational research?**
(Several possible solutions)",  longmcq(opts_3))
```

**G.4. Workflow managers can run tasks (different) concurrently if there are no dependencies** (True or False) `r torf(TRUE)`

**G.5. A workflow manager can execute a single parallelized task on multiple nodes in a computing cluster** (True or False) `r torf(TRUE)`
:::

## B. Snakemake

:::{.callout-warning}
# How to run snakemake on UCloud

First, mount the following two drives, use the `setup.sh` initialization file and ask for 2 CPUs so we can run things in parallel: 

- `YourNameSurname#xxxx`: save your results/files here.  
- `hpclab-workshop`: this contains input files and scripts. You can read-only access from this directory (no write permissions). 

Next, activate `snakemake` environment. 

```{.bash}
conda deactivate 
# make sure no env is active!
conda activate snakemake 
```
:::

:::{.callout-warning}
# Are you running the exercises locally?
Download the `Snakefile` and data required for this exercise using the links below if you are running things locally. 

<div style="display: flex; gap: 10px;">
  <a href="../develop/scripts/samples_1kgp_test.tsv" download="samples_1kgp_test.tsv">
    <button class="btn small-button">Download data input</button>
  </a>

  <a href="../develop/scripts/process_1kgp.smk" download="process_1kgp.smk">
    <button class="btn small-button">Download Snakefile</button>
  </a>
</div>

:::

We strongly recommend keeping the [Snakemake documentation](https://snakemake.readthedocs.io/en/stable/) open for reference and guidance.


::: {.webex-check .callout-exercise}
# II - Exploring rule invocation in Snakemake

In this exercise, we will explore how rules are invoked in a Snakemake workflow. The snakemake file is located at: `/work/HPCLab_workshop/rules/process_1kgp.smk`. Now follow these steps and answer the questions: 

1. Open the snakefile, named `process_1kgp.smk` and try to understand every single line. If you request Snakemake to generate the file `results/all_female.txt`, **what commands will be executed and in what sequence?**
2. Open a terminal and navigate to your personal drive `cd /work/YourNameSurname#xxxx`. Create a project directory called, for example, `hpclab` and make it your working directory. You should save all results here!
3. **Dry-run** the workflow: Check the number of jobs that will be executed.

   **S.1. How many jobs will Snakemake run?** `r fitb(8)`

4. Run the workflow from the directory `hpclab` (the one you just created on your personal drive). Use the name flag `--snakefile <snakefile>.smk --jobs 1`, or the abbreviated format `-s <snakefile>.smk -j 1`. 
5. Please verify that the output has been successfully generated and saved in your working directory.

   **S.2. Has Snakemake created a subdirectory that didn't previously exist? What is its name?** `r fitb("results")`

   **S.3. How many files with the extension `*.tsv` can you find in that subdirectory?** `r fitb(6)`
6. **Dry-run** the workflow again (from `hpclab`). 

   ```{r, results='asis', echo = FALSE}
   #| echo: false
   opts_2 <- c(
      answer="**No**. There is nothing to be done. All requested files are present and up to date",
      "**Yes**. Reasons: input files updated by another job (all, combine, split_by_superpop) and output files have to be generated (combine, preprocess, split_by_superpop)"
   )

   cat("**S.4. Would Snakemake run any jobs based on the results of the dry-run?**",  longmcq(opts_2))
   ```

7. Remove files starting with `E` in your `results` folder ("EAS.tsv" and "EUR.tsv") and `all_female.txt`. Then, dry-run once more. **S.1. How many jobs will Snakemake run?** `r fitb(4)`

8. Under your working directory, create a folder named `rules` and copy the snakefile (`process_1kgp.smk`) to that folder so you can edit it! Then, open the file and remove lines 13-15. How else can you run the workflow but to generate instead `all_male.txt` using only the command-line? 

   ```{.bash filename="process_1kgp.smk"} 
   13 rule all:
   14    input:
   15       expand("results/all_{gender}.txt", gender=["female"])
   ```
   
   **S.5. Tip: what is missing at the end of the command ( e.g. what should be added to ensure `all_male.txt` is generated)? `snakemake -s process_1kgp.smk -c1`** `r fitb("results/all_male.txt")`

9. Let's add a new rules that concatenates the two files you have generated (`all_female.txt` and `all_male.txt`) and save it into `concatenated.txt`. Remember, all files should be saved into the `results` subdir. Hint: `cat file1.txt file2.txt > output.txt`

:::{.callout-hint}
# Solution 

1. Tasks will be executed in this order preprocess (1), split_by_superpop (5) and combine (1). 

```{.bash}
# 2. Create subdir 
cd /work/AlbaRefoyoMartínez#0753/
mkdir hpclab
cd hpclab
# 3. dry run 
snakemake -s /work/HPCLab_workshop/rules/process_1kgp.smk -n 
# 4. run the workflow 
snakemake -s HPCLab_workshop/rules/process_1kgp.smk -c2 
# 5. verify output 
ls results/*
# 6. dry run 
snakemake -s /work/HPCLab_workshop/rules/process_1kgp.smk -n 
# 7. remove file starting with E  and the all_female.txt
rm results/E*.tsv results/all_female.txt
# 8. make a copy of the snakefile and remove the lines 
mkdir rules 
cp /work/HPCLab_workshop/rules/process_1kgp.smk rules/
# 8. S5. rerun again with the <name_output>
snakemake -s process_1kgp.smk -c1 results/all_male.txt 
# 9. create rule 
rule concat: 
   input: 
      "results/all_female.txt",
      "results/all_male.txt"
   output:
      "results/concatenated.txt"
   shell:
      "cat {input} > {output}"
```
:::

:::